var relearn_searchindex = [
  {
    "breadcrumb": "/",
    "content": "Learn by doing",
    "description": "Learn by doing",
    "tags": [],
    "title": "Kubernetes documentation",
    "uri": "/k8s/index.html"
  },
  {
    "breadcrumb": "/",
    "content": "Topics Kubernetes Overview Kubernetes Architecture Kubernetes Components Kubernetes Installation Kubernetes SSL Certificates Kubernetes Static Pods Kubernetes Configuration files Kubernetes Upgrade Etcd backup and restore Kubernetes Node related commands CIS Benchmark Kubernetes Overview Kubernetes or k8s is an open-source orchestration and cluster management for container-based applications maintained by the Cloud Native Computing Foundation. Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications. It groups containers that make up an application into logical units for easy management and discovery\nKubernetes Architecture \u0026 Components Managed Kubernetes Cluster (We do not have on control Plane) e.g AKS on Azure Cloud EKS on AWS cloud GKE on Google Cloud Self Managed kubernetes Cluster (Installed on VMs) Kubernetes has two type of nodes Master Nodes (Control Plane) Worker Nodes Control Plane The control plane is responsible for container orchestration and maintaining the desired state of the cluster. It has the following components.\nkube-apiserver etcd kube-scheduler kube-controller-manager cloud-controller-manager Worker Node The Worker nodes are responsible for running containerized applications. The worker Node has the following components.\nkubelet kube-proxy Container runtime Control Plane Components: kube-apiserver The kube-api server is the central hub of the Kubernetes cluster that exposes the Kubernetes API. End users, and other cluster components, talk to the cluster via the API server. So when you use kubectl to manage the cluster, at the backend you are actually communicating with the API server through HTTP REST APIs. The communication between the API server and other components in the cluster happens over TLS to prevent unauthorized access to the cluster. It is the only component that communicates with etcd. etcd etcd is an open-source strongly consistent, distributed key-value store. etcd is designed to run on multiple nodes as a cluster without sacrificing consistency. etcd stores all configurations, states, and metadata of Kubernetes objects (pods, secrets, daemonsets, deployments, configmaps, statefulsets, etc). etcd it is the only Statefulset component in the control plane. Only kube-apiserver can talk to etcd. kube-scheduler: The kube-scheduler is responsible for scheduling Kubernetes pods on worker nodes. It is a controller that listens to pod creation events in the API server. The scheduler has two phases. Scheduling cycle and the Binding cycle. Together it is called the scheduling context. The scheduling cycle selects a worker node and the binding cycle applies that change to the cluster. Kube Controller Manager Controllers are programs that run infinite control loops. Meaning it runs continuously and watches the actual and desired state of objects. Kube controller manager is a component that manages all the Kubernetes controllers. Kubernetes resources/objects like pods, namespaces, jobs, replicaset are managed by respective controllers. the Kube scheduler is also a controller managed by the Kube controller manager. built-in Kubernetes controllers. Deployment controller Replicaset controller DaemonSet controller Job Controller (Kubernetes Jobs) CronJob Controller endpoints controller namespace controller service accounts controller. Node controller Cloud Controller Manager (CCM) When kubernetes is deployed in cloud environments, the cloud controller manager acts as a bridge between Cloud Platform APIs and the Kubernetes cluster.\nCloud controller integration allows Kubernetes cluster to provision cloud resources like instances (for nodes), Load Balancers (for services), and Storage Volumes (for persistent volumes).\n3 main controllers that are part of the cloud controller manager.\nNode controller: This controller updates node-related information by talking to the cloud provider API. For example, node labeling \u0026 annotation, getting hostname, CPU \u0026 memory availability, nodes health, etc.\nRoute controller: It is responsible for configuring networking routes on a cloud platform. So that pods in different nodes can talk to each other.\nService controller: It takes care of deploying load balancers for kubernetes services, assigning IP addresses, etc. Kubernetes Worker Node Components Kubelet Kubelet is an agent component that runs on every node in the cluster. It does not run as a container instead runs as a daemon, managed by systemd. It is responsible for registering worker nodes with the API server. It then brings the podSpec to the desired state by creating containers. Creating, modifying, and deleting containers for the pod. Responsible for handling liveliness, readiness, and startup probes. Responsible for Mounting volumes by reading pod configuration and creating respective directories on the host for the volume mount. Kubelet is also a controller that watches for pod changes and utilizes the node’s container runtime to pull images, run containers, etc. Static pods are controlled by kubelet, not the API servers. Static pods from podSpecs located at /etc/kubernetes/manifests Kube proxy Kube-Proxy is a network proxy that runs on each node in a Kubernetes cluster. It is responsible for maintaining network connectivity between services and pods. Kube-Proxy does this by translating service definitions into actionable networking rules. Kube-proxy uses iptables as a default mode. Container Runtime Container runtime runs on all the nodes in the Kubernetes cluster. It is responsible for pulling images from container registries, running containers, allocating and isolating resources for containers, and managing the entire lifecycle of a container on a host.\nKubernetes supports multiple container runtimes (CRI-O, Docker Engine, containerd, etc) that are compliant with Container Runtime Interface (CRI). This means, all these container runtimes implement the CRI interface and expose gRPC CRI APIs (runtime and image service endpoints). When there is a new request for a pod from the API server, the kubelet talks to CRI-O daemon to launch the required containers via Kubernetes Container Runtime Interface.\nCRI-O checks and pulls the required container image from the configured container registry using containers/image library.\nCRI-O then generates OCI runtime specification (JSON) for a container.\nCRI-O then launches an OCI-compatible runtime (runc) to start the container process as per the runtime specification.\nKubernetes Cluster Addon Components CNI Plugin (Container Network Interface) CoreDNS (For DNS server): CoreDNS acts as a DNS server within the Kubernetes cluster. By enabling this addon, you can enable DNS-based service discovery. Metrics Server (For Resource Metrics): This addon helps you collect performance data and resource usage of Nodes and pods in the cluster. Web UI (Kubernetes Dashboard): This addon enables the Kubernetes dashboard to manage the object via web UI. Virual Machine setup for Kubernetes Installation. Prerequisites Virtualbox installed on Windows vm. Internet connectivity has to be there. Vscode should be installed Gitbash should also be available on Windows machine Create two VMS using vagrant on virtualbox Create a folder in Windows OS Open folder in vscode Open Terminal in vscode and run the below commands git clone https://gitlab.com/container-and-kubernetes/kubernetes-2024.git cd kubernetes-2024/two-vms vagrant up\rCheck your virtualbox and see if you have two vms (kmaster \u0026 kworker1) with below ips kmaster: 172.16.16.100 kworker1: 172.16.16.101\rTry to login to each vms using the command below usinf vscode terminal ssh vagrant@172.16.16.100 ssh vagrant@172.16.16.101\rKubernetes Cluster Setup Documentation Documentation Link: https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/\nInstallation of kubeadm ,kubelet etc will be done on master and worker nodes. Setup containerd Prerequisites A compatible Linux host.\n2 GB or more of RAM per machine (any less will leave little room for your apps).\n2 CPUs or more.\nFull network connectivity between all machines in the cluster (public or private network is fine).\nUnique hostname, MAC address, and product_uuid for every node. See here for more details.\nCertain ports are open on your machines. See here for more details.\nSwap configuration. The default behavior of a kubelet was to fail to start if swap memory was detected on a node.\nVerify the MAC address and product_uuid are unique for every node\nip link ip addr show ifconfig -a sudo cat /sys/class/dmi/id/product_uuid\rCheck Required Ports nc 127.0.0.1 6443\rDisable swap sudo swapoff -a sudo sed -i '/ swap / s/^\\(.*\\)$/#\\1/g' /etc/fstab\rInstalling a container runtime To run containers in Pods, Kubernetes uses a container runtime.\nBy default, Kubernetes uses the Container Runtime Interface (CRI) to interface with your chosen container runtime.\nIf you don’t specify a runtime, kubeadm automatically tries to detect an installed container runtime by scanning through a list of known endpoints.\nIf multiple or no container runtimes are detected kubeadm will throw an error and will request that you specify which one you want to use.\nThe tables below include the known endpoints for supported operating systems:\nRuntime\tPath to Unix domain socket\nContainer Runtime Socket containerd unix:///var/run/containerd/containerd.sock CRI-O unix:///var/run/crio/crio.sock Docker Engine (cri-dockerd) unix:///var/run/cri-dockerd.sock Install containerd apt update -y apt-get install -y containerd mkdir -p /etc/containerd containerd config default \u003e /etc/containerd/config.toml\rUpdate SystemdCgroup Settings sed -i 's/SystemdCgroup \\= false/SystemdCgroup \\= true/g' /etc/containerd/config.toml\rRestart containerd systemctl restart containerd\rKernel Parameter Configuration Forwarding IPv4 and letting iptables see bridged traffic cat \u003c\u003cEOF | sudo tee /etc/modules-load.d/k8s.conf overlay br_netfilter EOF sudo modprobe overlay sudo modprobe br_netfilter\rsysctl params required by setup, params persist across reboots cat \u003c\u003cEOF | sudo tee /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-iptables = 1 net.bridge.bridge-nf-call-ip6tables = 1 net.ipv4.ip_forward = 1 EOF\rApply sysctl params without reboot sudo sysctl --system\rInstalling kubeadm, kubelet and kubectl sudo apt-get update sudo apt-get install -y apt-transport-https ca-certificates curl sudo curl -fsSL https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-archive-keyring.gpg echo \"deb [signed-by=/etc/apt/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main\" | sudo tee /etc/apt/sources.list.d/kubernetes.list sudo apt-get update apt-cache madison kubeadm sudo apt-get install -y kubelet=1.24.0-00 kubeadm=1.24.0-00 kubectl=1.24.0-00 cri-tools=1.24.2-00 sudo apt-mark hold kubelet kubeadm kubectl\rInitialize Cluster with kubeadm (Only master node) kubeadm init --pod-network-cidr=10.244.0.0/16 --kubernetes-version=1.24.0 --apiserver-advertise-address=172.16.16.100\rExecute the below command to run kubectl commands as you will require kubeconfig file to connect to kubernetes cluster mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config\rClick Here to know more about kubeconfig\nCheck nodes status kubectl get nodes\rInstall Network Addon (flannel) (master node) kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml\rConnect Worker Node (Only worker node) kubeadm join 172.16.16.100:6443 --token qmw5dj.ljdh8r74ce3y85ad \\ --discovery-token-ca-cert-hash sha256:83374ec05088fa7efe9c31cce63326ae7037210ab049048ef08f8c961a048ddf\rVerification check node status kubectl get nodes\rrun a pod kubectl run nginx --image=nginx\rcheck running pod kubectl get pods -o wide\rKubernetes Cluster Configurations Static Pod Manifests: /etc/kubernetes/manifests /etc/kubernetes/manifests ├── etcd.yaml ├── kube-apiserver.yaml ├── kube-controller-manager.yaml └── kube-scheduler.yaml\rTLS Certificates All the components talk to each other over mTLS. Under the PKI folder, you will find all the TLS certificates and keys Kubeconfig Files\nAny components that need to authenticate to the API server need the kubeconfig file.\nAll the cluster Kubeconfig files are present in the /etc/kubernetes folder (.conf files). You will find the following files.\nadmin.conf controller-manager.conf kubelet.conf scheduler.conf It contains the API server endpoint, cluster CA certificate, cluster client certificate, and other information.\nThe admin.conf, file, which is the admin kubeconfig file used by end users to access the API server to manage the clusters.\nThe Kubeconfig for the Controller manager controller-manager.conf, scheduler, and Kubelet is used for API server authentication and authorization.\nKubelet Configurations kubelet kubeconfig file: /etc/kubernetes/kubelet.conf kubelet config file: /var/lib/kubelet/config.yaml EnvironmentFile=-/var/lib/kubelet/kubeadm-flags.env\rThe /var/lib/kubelet/config.yaml contains all the kubelet-related configurations. The static pod manifest location is added as part of the staticPodPath parameter. /var/lib/kubelet/kubeadm-flags.env file contains the container runtime environment Linux socket and the infra container (pause container) image. CoreDNS Configurations kubectl get configmap --namespace=kube-system\rNode Related commands Check all nodes which are the part of kubernetes cluster kubectl get nodes\rCordon a node (prevent new pods from being scheduled) kubectl cordon \u003cnode-name\u003e\rUncordon a node (allow new pods to be scheduled): kubectl uncordon \u003cnode-name\u003e\rDrain a node (evacuate all pods, reschedule them elsewhere) kubectl drain \u003cnode-name\u003e\rSort Nodes by CPU Capacity kubectl get nodes --sort-by=.status.capacity.cpu\rSort by memory kubectl get nodes --sort-by=.status.capacity.memory\rHow to label a node kubectl label node \u003cnodename\u003e key=value\rcheck nodes with label kubectl get nodes -l key=value\rcheck the nodes which does not label key=value kubectl get nodes key!=value\rCheck detailed information about a node kubectl get nodes -o wide\rUpgrade Kubeadm Cluster Before upgrade kubernetes cluster Do not forgot to study kubernetes release\nCheck the release notes before you upgrade your kubernetes cluster\nAlso drain a node and uncordon after the upgrade is successful\nCheck the upgrade plan using below commands\nkubeadm upgrade plan kubectl -n kube-system get cm kubeadm-config -o yaml\rCheck the kubeadm version available apt-cache madison kubeadm\rUnhold the packages to be upgraded apt-mark unhold kubelet kubectl kubeadm\rUpgrade the available packages apt-get update \u0026\u0026 apt-get install -y kubelet=1.25.0-00 kubectl=1.25.0-00 kubeadm=1.25.0-00 Lock again the below packages apt-mark hold kubelet kubectl\rCheck new kubeadm version kubeadm version\rUpgrade the kubernetes cluster kubeadm upgrade apply 1.25.0\rRestart the services systemctl daemon-reload systemctl restart kubelet\rCheck the kubernetes version kubectl version\rKubens \u0026 Kubectx Login to master node and run the command there Install Kubens to switch from one namespace to another wget https://raw.githubusercontent.com/ahmetb/kubectx/master/kubens chmod +x kubens cp kubens /bin\rInstall kubectx to switch from one Kubernetes cluster to another wget https://raw.githubusercontent.com/ahmetb/kubectx/master/kubectx chmod +x kubectx cp kubectx /bin\rKubernetes Initial commands for cluster verification How to check nodes in cluster kubectl get nodes\rCluster api info kubectl api-resources\rCluster info kubect cluster-info\rKubernetes nodes information kubectl describe nodes \u003cnodename\u003e\rUpdate worker node role as a worker kubectl label node \u003cnode-name\u003e kubernetes.io/role=worker\rMake a test after creating one pod kubectl run mypod --image=nginx\rCheck newly created pod kubectl get pods\rInstall kubernetes Cluster Automatically git clone https://gitlab.com/container-and-kubernetes/kubernetes-2024.git cd kubernetes-2024/ cd kubernetes-with-vagrant/ vagrant up\rCommon Kubeadm Commands kubeadm init (initialize kubernetes) kubeadm init --pod-network-cidr=192.168.0.0/16\rkubeadm join (To join a worker node) kubeadm join \u003cmaster-node-ip\u003e:\u003cmaster-node-port\u003e --token \u003ctoken\u003e --discovery-token-ca-cert-hash \u003chash\u003e\rkubeadm reset( removing all installed Kubernetes components and returning it to its pre-Kubernetes state. ) kubeadm reset --force\rkubeadm upgrade (check available upgrades) kubeadm upgrade plan\rkubeadm token (Token management) kubeadm token create\rPrint Token along with command kubeadm token create --print-join-command\rkubeadm token list (Check all token along with expiration date) kubeadm token list\rkubeadm token delete kubeadm token delete \u003ctoken_value\u003e\rkubeadm config migrate kubeadm config migrate --old-config kubeadm.conf --new-config kubeadm.yaml\rkubeadm config (Print default config files) kubeadm config print init-defaults\rkubeadm config images(List all required images) kubeadm config images list\rkubeadm certs(Check certificate status) kubeadm certs check-expiration kubeadm certs certificate-key\rkubeadm upgrade node(upgrades the kubelet and kube-proxy on a worker node to match the control plane’s version.) kubeadm upgrade node\rWhat is a Kubeconfig file? A Kubeconfig is a YAML file with all the Kubernetes cluster details, certificates, and secret tokens to authenticate the cluster. You might get this config file directly from the cluster administrator or from a cloud platform if you are using a managed Kubernetes cluster. When you use kubectl, it uses the information in the kubeconfig file to connect to the kubernetes cluster API. The default location of the Kubeconfig file is $HOME/.kube/config Example Kubeconfig File certificate-authority-data: Cluster CA server: Cluster endpoint (IP/DNS of the master node) name: Cluster name user: name of the user/service account. token: Secret token of the user/service account. apiVersion: v1 clusters: - cluster: certificate-authority-data: \u003cca-data-here\u003e server: https://your-k8s-cluster.com name: \u003ccluster-name\u003e contexts: - context: cluster: \u003ccluster-name\u003e user: \u003ccluster-name-user\u003e name: \u003ccluster-name\u003e current-context: \u003ccluster-name\u003e kind: Config preferences: {} users: - name: \u003ccluster-name-user\u003e user: token: \u003csecret-token-here\u003e\rList all cluster contexts kubectl config get-contexts -o=name\rSet the current context kubectl config use-context \u003ccluster-name\u003e Using Environment variable (KUBECONFIG) export KUBECONFIG=\u003ckubeconfig filename\u003e\rUsing Kubeconfig File With Kubectl kubectl get nodes --kubeconfig=\u003cfilepath\u003e\rHow to merge multiple kubeconfig into one KUBECONFIG=config:dev_config:test_config kubectl config view --merge --flatten \u003e config.new",
    "description": "Topics Kubernetes Overview Kubernetes Architecture Kubernetes Components Kubernetes Installation Kubernetes SSL Certificates Kubernetes Static Pods Kubernetes Configuration files Kubernetes Upgrade Etcd backup and restore Kubernetes Node related commands CIS Benchmark Kubernetes Overview Kubernetes or k8s is an open-source orchestration and cluster management for container-based applications maintained by the Cloud Native Computing Foundation. Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications. It groups containers that make up an application into logical units for easy management and discovery",
    "tags": [],
    "title": "Part 1",
    "uri": "/part01/index.html"
  },
  {
    "breadcrumb": "/",
    "content": "Topics: NameSpace Create a namespace Switch from one ns to another Resource Quota Resource Limits POD Single Container pod MultiContainer pod Login to a pod Copy to a pod and from a pod(container) How to check logs from a container Environment Variables of pod initContainer Command and Argument of a pod pullSecret of a pod pod restart policy imagePull policy How to delete a pod Pod priority Pod Resources Pod Quality of Services(QOS) Advance Scheduling of pod Scheduler Nodename nodeSelector Node Affinity Taints and Tolerations Pod affinity and anti affinity Priority and PriorityClass Preemption Disruption Budget Topology and Constraints Descheduler Namespace Namespaces are a way to divide cluster resources between multiple users (via resource quota). namespaces provides a mechanism for isolating groups of resources within a single cluster Names of resources need to be unique within a namespace, but not across namespaces. To achieve multitenancy with Networkpolicy To configure RBAC Check all namespaces kubectl get ns\rCreate a new namespace kubectl create ns \u003cname\u003e\rSwitch from one ns to another kubens kubens \u003cns name\u003e\rTo see which Kubernetes resources are and aren’t in a namespace: In a namespace kubectl api-resources --namespaced=true\rNot in a namespace kubectl api-resources --namespaced=false\rLearn About Resource Quota and limits Configure Memory and CPU Quotas for a Namespace\nkubectl apply -f - \u003c\u003cEOF apiVersion: v1 kind: ResourceQuota metadata: name: mem-cpu-demo spec: hard: requests.cpu: \"1\" requests.memory: 1Gi limits.cpu: \"2\" limits.memory: 2Gi EOF\rAs per above Example: The ResourceQuota places these requirements on the quota-mem-cpu-example namespace:\nFor every Pod in the namespace, each container must have a memory request, memory limit, cpu request, and cpu limit.\nThe memory request total for all Pods in that namespace must not exceed 1 GiB.\nThe memory limit total for all Pods in that namespace must not exceed 2 GiB.\nThe CPU request total for all Pods in that namespace must not exceed 1 cpu.\nThe CPU limit total for all Pods in that namespace must not exceed 2 cpu.\nCheck applied quota\nkubectl get quota Create namespace using a yaml file kubectl apply -f - \u003c\u003cEOF kind: Namespace apiVersion: v1 metadata: name: test labels: name: test EOF\rResource Limits A Kubernetes cluster can be divided into namespaces. Once you have a namespace that has a default memory limit, and you then try to create a Pod with a container that does not specify its own memory limit, then the control plane assigns the default memory limit to that container.\nCreate a LimitRange kubectl apply -f - \u003c\u003cEOF apiVersion: v1 kind: LimitRange metadata: name: mem-limit-range spec: limits: - default: memory: 512Mi defaultRequest: memory: 256Mi type: Container EOF\rNow create one pod with no resource and check if the default resource has been applied kubectl describe pod \u003cpodname\u003e\rPOD A pod is the smallest deployable unit in Kubernetes that represents a single instance of an application. For example, if you want to run the Nginx application, you run it in a pod. A container is a single unit. However, a pod can contain more than one container. You can think of pods as a box that can hold one or more containers together. the pod gets a single unique IP address and containers running inside the pod use localhost to connect to each other on different ports. Each pod gets a unique IP address. Pods communicate with each other using the IP address. Containers inside a pod connect using localhost on different ports. Containers running inside a pod should have different port numbers to avoid port clashes. You can set CPU and memory resources for each container running inside the pod. Containers inside a pod share the same volume mount. All the containers inside a pod are scheduled on the same node; It cannot span multiple nodes. If there is more than one container, during the pod startup all the main containers start in parallel. Whereas the init containers inside the pod run in sequence. Create a pod kubectl run pod --image=nginx\rCheck newly created pod kubectl get pods\rCheck more info of a pod like where the pod is scheduled and ip address kubectl get pods -o wide\rcheck pods details like events and resources kubectl describe pod \u003cpodname\u003e\rCheck the name of all pods kubectl get pods -o name\rCreate a pod using yaml file kubectl apply -f - \u003c\u003cEOF apiVersion: v1 kind: Pod metadata: name: example-pod labels: app: example spec: containers: - name: my-container image: nginx:latest EOF\rCheck Pod Logs kubectl logs \u003cpod-name\u003e\rFollow Pod Logs in Real-time kubectl logs -f \u003cpod-name\u003e\rHow to check logs for a container from multiContainer pod kubectl logs -c \u003ccontainername\u003e \u003cpodname\u003e\rCheck the logs for all containers kubectl logs \u003cpodname\u003e --all-containers=true - Execute Command in a Pod: ```t kubectl exec -it \u003cpod-name\u003e -- \u003ccommand\u003e\rCopy Files to/from Pod: kubectl cp \u003clocal-path\u003e \u003cpod-name\u003e:\u003cpod-path\u003e kubectl cp \u003cpod-name\u003e:\u003cpod-path\u003e \u003clocal-path\u003e\rDelete a Pod: kubectl delete pod \u003cpod-name\u003e\rHow to delete a pod forcefully kubectl delete pod --force --grace-period=0\rPort Forwarding to Pod: kubectl port-forward \u003cpod-name\u003e \u003clocal-port\u003e:\u003cpod-port\u003e\rPort forwarding on ip address not on the localhost kubectl port-forward --address 0.0.0.0 pod/mypod 8888:5000\rGet YAML Definition of a Running Pod: kubectl get pod \u003cpod-name\u003e -o yaml\rSome useful command in realtime(Production) Find out all the images of all the pods kubectl get pods --all-namespaces -o jsonpath='{range .items[*]}{.spec.containers[*].image}{\"\\n\"}{end}'\rGet All containers name kubectl get pods --all-namespaces -o jsonpath='{range .items[*]}{.metadata.namespace} {.metadata.name}: {.spec.containers[*].name}{\"\\n\"}{end}'\rDefine Environment Variables for a Container A Kubernetes environment variable is a dynamic value that configures some aspect of the environment in which a Kubernetes-based application runs.\nenv: - name: SERVICE_PORT value: \"8080\" - name: SERVICE_IP value: \"192.168.100.1\"\rProblem Statement if the pod is failing becuase of environment variable Deploy one mysql pod and see if the pod is failing kubectl run mysql --image=mysql:5.6\rCheck the logs and fix the issue Environment Variables Apply required variables using below yaml file kubectl apply -f - \u003c\u003cEOF apiVersion: v1 kind: Pod metadata: name: pod-with-env spec: containers: - name: my-container image: mysql:5.6 env: - name: MYSQL_ROOT_PASSWORD value: \"\" EOF\rCommand and Arguments with Kubernetes Pod Here’s a table summarizing the field names used by Docker and Kubernetes:\nDescription Docker field name Kubernetes field name The command run by the container Entrypoint command The arguments passed to the command Cmd args When you override the default Entrypoint and Cmd, these rules apply:\nIf you do not supply command or args for a Container, the defaults defined in the Docker image are used. If you supply a command but no args for a Container, only the supplied command is used. The default EntryPoint and the default Cmd defined in the Docker image are ignored. If you supply only args for a Container, the default Entrypoint defined in the Docker image is run with the args that you supplied. If you supply a command and args, the default Entrypoint and the default Cmd defined in the Docker image are ignored. Your command is run with your args. Example 1: Command Override kubectl apply -f - \u003c\u003cEOF apiVersion: v1 kind: Pod metadata: name: pod-with-command spec: containers: - name: my-container image: nginx:latest command: [\"echo\"] args: [\"Hello, Kubernetes!\"] EOF\rExample 2: Command and Arguments kubectl apply -f - \u003c\u003cEOF apiVersion: v1 kind: Pod metadata: name: pod-with-command-args spec: containers: - name: my-container image: busybox:latest command: [\"sh\", \"-c\"] args: [\"echo Hello from Kubernetes! \u0026\u0026 sleep 3600\"] EOF\rExample 3: Passing Environment Variables to Commands kubectl apply -f - \u003c\u003cEOF apiVersion: v1 kind: Pod metadata: name: pod-with-env-vars spec: containers: - name: my-container image: alpine:latest command: [\"/bin/sh\", \"-c\"] args: [\"echo \\$GREETING\"] env: - name: GREETING value: \"Hello, Kubernetes!\" EOF\rExample 4: Passing Arguments to Docker Entrypoint kubectl apply -f - \u003c\u003cEOF apiVersion: v1 kind: Pod metadata: name: pod-with-entrypoint-args spec: containers: - name: my-container image: ubuntu:latest command: [\"/bin/echo\"] args: [\"Hello\", \"Kubernetes!\"] EOF\rMultiContainer pod Use Cases for Multi-Container Pods Pods that run multiple containers that need to work together.\nA Pod can encapsulate an application composed of multiple co-located containers that are tightly coupled and need to share resources.\nThese co-located containers form a single cohesive unit. Here is an example for multiple container.\nkubectl apply -f - \u003c\u003cEOF apiVersion: v1 kind: Pod metadata: name: nginx-redis-pod spec: containers: - name: nginx-container image: nginx:latest ports: - containerPort: 80 - name: redis-container image: redis:latest ports: - containerPort: 6379 EOF\rInitContainer A Pod can have multiple containers running apps within it, but it can also have one or more init containers, which are run before the app containers are started. Init containers are exactly like regular containers, except:\nInit containers always run to completion.\nEach init container must complete successfully before the next one starts.\nIf a Pod’s init container fails, the kubelet repeatedly restarts that init container until it succeeds.\nRegular init containers (in other words: excluding sidecar containers) do not support the lifecycle, livenessProbe, readinessProbe, or startupProbe fields.\nInit containers must run to completion before the Pod can be ready\nIf you specify multiple init containers for a Pod, kubelet runs each init container sequentially\nEach init container must succeed before the next can run. Here is an example for initContainer:\nkubectl apply -f - \u003c\u003cEOF apiVersion: v1 kind: Pod metadata: name: init-container-pod spec: containers: - name: main-container image: nginx:latest ports: - containerPort: 80 initContainers: - name: init-wait-redis image: busybox:latest command: [\"sh\", \"-c\", \"until nc -zv nginx 80; do echo 'Waiting for Redis to be ready'; sleep 1; done\"] EOF\rMake a test by creating a pod and service kubectl run nginx --image=nginx kubectl expose pod/nginx --port 80\rNow check the pod status initContainer should be successful Also check the logs for initContainer Sidecar container A Sidecar container extends and enhances the functionality of a preexisting container without changing it. This pattern is one of the fundamental container patterns that allows single-purpose containers to cooperate closely together. Adapter Container The Adapter pattern takes a heterogeneous containerized system and makes it conform to a consistent, unified interface with a standardized and normalized format that can be consumed by the outside world. The Adapter pattern inherits all its characteristics from the Sidecar, Resources definition in pod Burstable QOS Kubernetes assigns the Burstable class to a Pod when a container in the pod has more resource limit than the request value.\nA pod in this category will have the following characteristics:\nThe Pod has not met the criteria for Guaranteed QoS class. A container in the Pod has an unequal memory or CPU request or limit An example is given below resources: limits: memory: \"300Mi\" cpu: \"800m\" requests: memory: \"100Mi\" cpu: \"600m\"\rkubectl apply -f - \u003c\u003cEOF apiVersion: v1 kind: Pod metadata: name: pod-with-resources spec: containers: - name: my-container image: nginx:latest resources: requests: memory: \"64Mi\" cpu: \"250m\" limits: memory: \"128Mi\" cpu: \"500m\" EOF\rCheck the pods if this is running\ncheck the QOS of the pod\nkubectl describe pod | grep -i qos\rGuranteed QOS Kubernetes considers Pods classified as Guaranteed as a top priority. It won’t evict them until they exceed their limits.\nA Pod with a Guaranteed class has the following characteristics:\nAll containers in the Pod have a memory limit and request. All containers in the Pod have a memory limit equal to the memory request. All containers in the Pod have a CPU limit and a CPU request. All containers in the Pod have a CPU limit equal to the CPU request. kubectl apply -f - \u003c\u003cEOF apiVersion: v1 kind: Pod metadata: name: pod-with-resources spec: containers: - name: my-container image: nginx:latest resources: requests: memory: \"64Mi\" cpu: \"250m\" limits: memory: \"64Mi\" cpu: \"250m\" EOF\rBest Efforts Kubernetes assigns the Burstable class to a Pod when a container in the pod has more resource limit than the request value. A pod in this category will have the following characteristics: The Pod has not met the criteria for Guaranteed QoS class. A container in the Pod has an unequal memory or CPU request or limit Pod without resourcs section are considered best efforts kubectl apply -f - \u003c\u003cEOF apiVersion: v1 kind: Pod metadata: name: pod-with-resources spec: containers: - name: my-container image: nginx:latest EOF\rimagepullSecret Pod that uses a Secret to pull an image from a private container image registry or repository. There are many private registries in use. This task uses Docker Hub as an example registry. Steps to Use a Pull Secret in a Pod YAML: Create Docker Config JSON File: Create a Docker configuration file (~/.docker/config.json) with the credentials for your private registry. You can use the docker login command to generate this file. docker login\rCreate a Kubernetes Secret: kubectl create secret generic my-pull-secret --from-file=.dockerconfigjson=$HOME/.docker/config.json --type=kubernetes.io/dockerconfigjson\ruse the yaml to create pod with private registry kubectl apply -f - \u003c\u003cEOF apiVersion: v1 kind: Pod metadata: name: pod-with-pull-secret spec: containers: - name: my-container image: myregistry.example.com/my-image:latest imagePullSecrets: - name: my-pull-secret EOF\rGive permission to serviceAccount to pull image so that you will not have to mention in pod Spec kubectl patch serviceaccount default -p '{\"imagePullSecrets\": [{\"name\": \"mySecret\"}]}'\rLearn about ServiceAccount\nimagePullPolicy Examples 1. IfNotPresent Description: Pull the image only if it does not already exist locally. Usage: Use this policy when you want to minimize image pulls and rely on locally cached images. Example: spec: containers: - name: my-container image: my-image:latest imagePullPolicy: IfNotPresent\r1. IfNotPresent Description: Pull the image only if it does not already exist locally. Usage: Use this policy when you want to minimize image pulls and rely on locally cached images. Example: spec: containers: - name: my-container image: my-image:latest imagePullPolicy: IfNotPresent\r2. Always Description : Always pull the latest version of the image, even if it already exists locally. Usage: Use this policy when you want to ensure that the container runs the latest version of the image. Example: spec: containers: - name: my-container image: my-image:latest imagePullPolicy: Always\r3. Never Description: Never pull the image, and only use the locally cached version if available.\nUsage : Use this policy when you want to prevent Kubernetes from pulling the image, even if it does not exist locally.\nExample:\nspec: containers: - name: my-container image: my-image:latest imagePullPolicy: Never\r4. IfPresent Description: Pull the image only if it already exists locally. If the image does not exist locally, do not pull it. Usage: Use this policy when you want to pull the image only if it is already available locally, otherwise use a cached version. Example spec: containers: - name: my-container image: my-image:latest imagePullPolicy: IfPresent\r5. Default (IfNotPresent) Description: Kubernetes default behavior if imagePullPolicy is not explicitly set. Pull the image only if it does not already exist locally. Usage: This is the default behavior, and it is suitable for many use cases where you want to minimize image pulls. Example : spec: containers: - name: my-container image: my-image:latest\rContainer RestartPolicy in Pod block 1. Always Description: Always restart the container regardless of the exit status or reason. Usage: Use this policy for critical services that should always be running. Example: spec: restartPolicy: Always\r2. OnFailure Description: Restart the container only if it exits with a non-zero status. Usage: Use this policy for jobs or batch processes that should be retried on failure. Example:\nspec: restartPolicy: OnFailure\r3. Never Description: Never restart the container, regardless of the exit status or reason. Usage: Use this policy for containers that are expected to run to completion and not be restarted. Example:\nspec: restartPolicy: Never\r4. Default (Always) Description: Kubernetes default behavior if restartPolicy is not explicitly set. Always restart the container. Usage: This is the default behavior and is suitable for many long-running services. Example\nspec: restartPolicy: Always (default behavior)\rAdvance Scheduling of pod Using nodeName kubectl apply -f - \u003c\u003cEOF apiVersion: v1 kind: Pod metadata: name: pod-scheduled-node spec: nodeName: \u003cmentionhere nodename\u003e containers: - name: my-container image: nginx:latest EOF\rSteps to Use nodeSelector: Label the node kubectl label nodes specific-node-name diskType=ssd\rUse this label to schedule the pod\napiVersion: v1 kind: Pod metadata: name: pod-with-node-selector spec: containers: - name: my-container image: nginx:latest nodeSelector: diskType: ssd\rCheck where this pod has been schduled\nkubectl get pods -o wide\rTaint and Toleration A taint allows a node to refuse pod to be scheduled unless that pod has a matching toleration. You apply taints to a node through the node specification (NodeSpec) and apply tolerations to a pod through the pod specification (PodSpec). A taint on a node instructs the node to repel all pods that do not tolerate the taint. Taints and tolerations consist of a key, value, and effect. An operator allows you to leave one of these parameters empty. Taint and Toleration key points\nParameter Description key Any string, up to 253 characters. Must begin with a letter or number, and may contain letters, numbers, hyphens, dots, and underscores. value Any string, up to 63 characters. Must begin with a letter or number, and may contain letters, numbers, hyphens, dots, and underscores. effect One of: NoSchedule, PreferNoSchedule, NoExecute operator One of: Equal, Exists How to apply taint on a node kubectl taint nodes \u003cnode-name\u003e \u003ckey\u003e=\u003cvalue\u003e:\u003ceffect\u003e kubectl taint nodes specific-node-name disktype=ssd:NoSchedule\rNow use this in pod yaml kubectl apply -f - \u003c\u003cEOF apiVersion: v1 kind: Pod metadata: name: toleration-pod spec: containers: - name: my-container image: nginx:latest tolerations: - key: disktype operator: Equal value: ssd effect: NoSchedule EOF\rExample : NotEqual Operator kubectl apply -f - \u003c\u003cEOF apiVersion: v1 kind: Pod metadata: name: toleration-not-equal-pod spec: containers: - name: my-container image: nginx:latest tolerations: - key: disktype operator: NotEqual value: ssd effect: NoSchedule EOF\rExample Exists Operator with Key kubectl apply -f - \u003c\u003cEOF apiVersion: v1 kind: Pod metadata: name: toleration-exists-key-pod spec: containers: - name: my-container image: nginx:latest tolerations: - key: disktype operator: Exists effect: NoSchedule EOF\rCheck the pods where are they schedules kubectl get pod -o wide\rNode Affinity Label Nodes kubectl label nodes kworker1 example-label=value1 kubectl label nodes kworker2 example-label=value2\rYaml for pod creation cat \u003c\u003cEOF \u003e pod-with-node-affinity.yaml apiVersion: v1 kind: Pod metadata: name: pod-with-node-affinity spec: containers: - name: nginx-container image: nginx affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: example-label operator: In values: - value1 - value2 EOF\rExample 2 with operator notin cat \u003c\u003cEOF \u003e pod-with-node-affinity.yaml apiVersion: v1 kind: Pod metadata: name: pod-with-node-affinity spec: containers: - name: nginx-container image: nginx affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: example-label operator: NotIn values: - value3 - value4 EOF\rExample Using Exists Operator cat \u003c\u003cEOF \u003e pod-with-node-affinity.yaml apiVersion: v1 kind: Pod metadata: name: pod-with-node-affinity spec: containers: - name: nginx-container image: nginx affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: example-label operator: Exists EOF\rCheck the pods status Example with Preferred Affinity cat \u003c\u003cEOF \u003e pod-with-preferred-node-affinity.yaml apiVersion: v1 kind: Pod metadata: name: pod-with-preferred-node-affinity spec: containers: - name: nginx-container image: nginx affinity: nodeAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 1 preference: matchExpressions: - key: example-label operator: Exists EOF\rPod Affinity and Anti Affinity kubectl apply -f - \u003c\u003cEOF apiVersion: v1 kind: Pod metadata: name: pod-with-pod-affinity labels: app: web spec: containers: - name: nginx-container image: nginx --- apiVersion: v1 kind: Pod metadata: name: pod-with-pod-affinity-rule spec: affinity: podAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - web topologyKey: kubernetes.io/hostname containers: - name: nginx-container image: nginx EOF\rAnother Example with weight kubectl apply -f - \u003c\u003cEOF apiVersion: v1 kind: Pod metadata: name: pod-with-preferred-pod-affinity labels: app: database spec: containers: - name: postgres-container image: postgres --- apiVersion: v1 kind: Pod metadata: name: pod-with-preferred-pod-affinity-rule spec: affinity: podAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: app operator: In values: - database topologyKey: kubernetes.io/hostname containers: - name: nginx-container image: nginx EOF\rPod antiAffinity kubectl apply -f - \u003c\u003cEOF apiVersion: v1 kind: Pod metadata: name: pod-with-anti-affinity labels: app: web spec: containers: - name: nginx-container image: nginx --- apiVersion: v1 kind: Pod metadata: name: pod-with-anti-affinity-rule spec: affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - web topologyKey: kubernetes.io/hostname containers: - name: nginx-container image: nginx EOF\rSoft AntiAffinity kubectl apply -f - \u003c\u003cEOF apiVersion: v1 kind: Pod metadata: name: pod-with-preferred-anti-affinity labels: app: database spec: containers: - name: postgres-container image: postgres --- apiVersion: v1 kind: Pod metadata: name: pod-with-preferred-anti-affinity-rule spec: affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: app operator: In values: - database topologyKey: kubernetes.io/hostname containers: - name: nginx-container image: nginx EOF\rPod Priority Pod priority classes You can assign pods a priority class, which is a non-namespaced object that defines a mapping from a name to the integer value of the priority. The higher the value, the higher the priority.\nA priority class object can take any 32-bit integer value smaller than or equal to 1000000000 (one billion). Reserve numbers larger than one billion for critical pods that should not be preempted or evicted. By default, OpenShift Container Platform has two reserved priority classes for critical system pods to have guaranteed scheduling.\nSystem-node-critical - This priority class has a value of 2000001000 and is used for all pods that should never be evicted from a node. Examples of pods that have this priority class are sdn-ovs, sdn, and so forth.\nSystem-cluster-critical - This priority class has a value of 2000000000 (two billion) and is used with pods that are important for the cluster. Pods with this priority class can be evicted from a node in certain circumstances. For example, pods configured with the system-node-critical priority class can take priority. However, this priority class does ensure guaranteed scheduling. Examples of pods that can have this priority class are fluentd, add-on components like descheduler, and so forth.\nSample priority class object\nkubectl apply -f - \u003c\u003cEOF apiVersion: scheduling.k8s.io/v1 kind: PriorityClass metadata: name: high-priority value: 1000000 globalDefault: false description: \"This priority class should be used for XYZ service pods only.\" EOF\rSample pod specification with priority class name kubectl apply -f - \u003c\u003cEOF apiVersion: v1 kind: Pod metadata: name: nginx labels: env: test spec: containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent priorityClassName: high-priority EOF Pod Disruptions Budget: Learn more about Disruptions Budget kubectl apply -f - \u003c\u003cEOF apiVersion: policy/v1 kind: PodDisruptionBudget metadata: name: example-pdb spec: minAvailable: 2 selector: matchLabels: app: example-app EOF\rUse of pdb with Deployment kubectl apply -f - \u003c\u003cEOF apiVersion: apps/v1 kind: Deployment metadata: name: example-deployment spec: replicas: 3 selector: matchLabels: app: example-app template: metadata: labels: app: example-app spec: containers: - name: nginx-container image: nginx:latest --- apiVersion: policy/v1 kind: PodDisruptionBudget metadata: name: example-pdb spec: minAvailable: 2 selector: matchLabels: app: example-app EOF\rPod Preemption and Pod Distruption Budget Pod preemption and other scheduler settings If you enable pod priority and preemption, consider your other scheduler settings: Pod priority and pod disruption budget A pod disruption budget specifies the minimum number or percentage of replicas that must be up at a time. If you specify pod disruption budgets, OpenShift Container Platform respects them when preempting pods at a best effort level. The scheduler attempts to preempt pods without violating the pod disruption budget. If no such pods are found, lower-priority pods might be preempted despite their pod disruption budget requirements. Pod priority and pod affinity Pod affinity requires a new pod to be scheduled on the same node as other pods with the same label.\nPointer Regarding Pod Scheduling Preemption removes existing Pods from a cluster under resource pressure to make room for higher priority pending Pods\nThe default priority for all pods is zero ( 0 )\nSupported Operators for Affinity\nThe operator represents the relationship between the label on the node and the set of values in the matchExpression parameters in the pod specification.\nThis value can be below:\nIn, NotIn, Exists DoesNotExist Lt Gt Specify a weight for the node, 1-100. The node that with highest weight is preferred.\nA taint on a node instructs the node to repel all pods that do not tolerate the taint.\nTaint Effects are given below:\nThe effect is one of the following: NoSchedule New pods that do not match the taint are not scheduled onto that node. Existing pods on the node remain.\nPreferNoSchedule New pods that do not match the taint might be scheduled onto that node, but the scheduler tries not to.\nExisting pods on the node remain.\nNoExecute New pods that do not match the taint cannot be scheduled onto that node.\nExisting pods on the node that do not have a matching toleration are removed.\noperator Equal The key/value/effect parameters must match. This is the default.\nExists The key/effect parameters must match. You must leave a blank value parameter, which matches any.\nDescheduler",
    "description": "Topics: NameSpace Create a namespace Switch from one ns to another Resource Quota Resource Limits POD Single Container pod MultiContainer pod Login to a pod Copy to a pod and from a pod(container) How to check logs from a container Environment Variables of pod initContainer Command and Argument of a pod pullSecret of a pod pod restart policy imagePull policy How to delete a pod Pod priority Pod Resources Pod Quality of Services(QOS) Advance Scheduling of pod Scheduler Nodename nodeSelector Node Affinity Taints and Tolerations Pod affinity and anti affinity Priority and PriorityClass Preemption Disruption Budget Topology and Constraints Descheduler Namespace Namespaces are a way to divide cluster resources between multiple users (via resource quota). namespaces provides a mechanism for isolating groups of resources within a single cluster Names of resources need to be unique within a namespace, but not across namespaces. To achieve multitenancy with Networkpolicy To configure RBAC Check all namespaces kubectl get ns\rCreate a new namespace kubectl create ns \u003cname\u003e\rSwitch from one ns to another kubens kubens \u003cns name\u003e\rTo see which Kubernetes resources are and aren’t in a namespace: In a namespace kubectl api-resources --namespaced=true\rNot in a namespace kubectl api-resources --namespaced=false\rLearn About Resource Quota and limits",
    "tags": [],
    "title": "Part 2",
    "uri": "/part02/index.html"
  },
  {
    "breadcrumb": "/",
    "content": "Topics Service CLusterIP NodePort LoadBalancer Headless Service METALLB DNS Troubleshooting Deployment ReplicaSet Daemonset Labels and Selector Type of Selector Equality based Set based selector Job CronJob Metric Server HPA(pod Autoscaling) Helm Overview Service A Service is a method for exposing a network application that is running as one or more Pods in your cluster.\nA key aim of Services in Kubernetes is that you don’t need to modify your existing application to use an unfamiliar service discovery mechanism. A Kubernetes service is a logical abstraction for a deployed group of pods in a cluster What are the types of Kubernetes services? ClusterIP: Exposes a service which is only accessible from within the cluster. NodePort: Exposes a service via a static port on each node’s IP. NodePorts are in the 30000-32767 range by default LoadBalancer: Exposes the service via the cloud provider’s load balancer. ExternalName Maps a service to a predefined externalName field by returning a value for the CNAME record. Create a service of Type(ClusterIp) kubectl apply -f - \u003c\u003cEOF apiVersion: v1 kind: Service metadata: name: myapp-service spec: selector: app: myapp ports: - protocol: TCP port: 80 targetPort: 8080 EOF\rCreate a service of Type (NodePort) kubectl expose pod/\u003cname\u003e --port 80 --target-port=80 --name \u003cSvc\u003e\rCreate a type of service (LoadBalancer) kubectl expose pod/name --port=80 --type=LoadBalancer\rCheck the endpoint kubectl get ep\rAchieve the blue green Delete a service Exchange the service selector Metallb A LoadBalancer Solution for OnPrem Kubernetes Deploying Metallb Steps: git clone https://gitlab.com/container-and-kubernetes/kubernetes-2024.git cd kubernetes-2024/ cd metallb kubectl apply -f 01 01_metallb.yaml kubectl apply -f 02_metallb-config.yaml\rMake a test after creating a service kubectl apply -f 03_test-load-balancer.yaml\rCheck the service kubectl get svc\rMetric Server Metric Server Github Link\nSteps git clone https://gitlab.com/container-and-kubernetes/kubernetes-2024.git cd kubernetes-2024 cd metricserver kubectl apply -f .\rwait for few mins and check if the metric server pods are up kubectl top nodes kubectl top pods\rHPA (Pod Autoscaling) Steps Prerequisites Resources should be defined for a pod Metric Api has to be available . git clone https://gitlab.com/container-and-kubernetes/kubernetes-2024.git cd kubernetes-2024 cd hpa kubectl apply -f deployment.yaml kubectl apply -f service.yaml kubectl apply hpa.yaml\rcheck hpa kubectl get hpa\rDeployment Create Deployment Update secret for a deployment Set resource block for a deployment Set Environment Variables in Deployment Perform Rollout Perform Rollback Rolled back to another revision Check the max rs for a deployment Deployment strategy Create a deployment kubectl create deployment myapp-deployment --image=nginx\rDeployment with Yaml file kubectl apply -f - \u003c\u003cEOF apiVersion: apps/v1 kind: Deployment metadata: name: myapp-deployment spec: replicas: 3 # Number of desired replicas selector: matchLabels: app: myapp template: metadata: labels: app: myapp spec: containers: - name: nginx-container image: nginx:latest ports: - containerPort: 80 EOF\rPerform Rollout kubectl set image deployment/myapp-deployment nginx-container=nginx:1.21.4\rCheck rollout status kubectl rollout status deployment myapp-deployment\rCheck the rollout history kubectl rollout history deployment myapp-deployment\rPerform Rollback kubectl rollout undo deployment myapp-deployment\rPerform scale up/down kubectl scale deployment myapp-deployment --replicas=5\rSet Environment Variables in Deployment kubectl set env deployment/myapp-deployment KEY=VALUE\rSet Resources kubectl set resources deployment/myapp-deployment --limits=cpu=200m,memory=512Mi --requests=cpu=100m,memory=256Mi\rUpdate SA for a deployment kubectl set serviceaccount deployment/myapp-deployment my-service-account\rChange the revision history spec: revisionHistoryLimit: 20 replicas: 3 selector: matchLabels: app: your-app template: metadata: labels: app: your-app spec: containers: - name: your-container image: your-image\rHow to check if the change has been accepted kubectl get deployment \u003cdeployment-name\u003e -o=jsonpath='{.spec.revisionHistoryLimit}\rHow to use record option with rollback depoloyment kubectl rollout undo deployment/\u003cdeployment-name\u003e --to-revision=\u003crevision-number\u003e --record\rjob Create a job Create a cronjob Clean up finished jobs automatically ttlSecondsAfterFinished: 100 check different option in cronjob as job backoffLimit Max pod restart in case of failure\rcompletions How many containers of the job are created one after another\rparallelism: it defines how many pods will be created at once Job Creation yaml kubectl apply -f - \u003c\u003cEOF apiVersion: batch/v1 kind: Job metadata: name: pi spec: template: spec: containers: - name: pi image: busybox command: [\"/bin/echo\", \"Hello World\"] restartPolicy: Never backoffLimit: 4 EOF\rcronjob Create a cronjob kubectl apply -f - \u003c\u003cEOF apiVersion: batch/v1beta1 kind: CronJob metadata: name: simple-cronjob spec: schedule: \"*/1 * * * *\" # Run every minute jobTemplate: spec: template: spec: containers: - name: simple-cronjob-container image: busy EOF\rExample 2 kubectl apply -f - \u003c\u003cEOF apiVersion: batch/v1beta1 kind: CronJob metadata: name: simple-cronjob spec: schedule: \"*/1 * * * *\" # Run every minute jobTemplate: spec: template: spec: containers: - name: simple-cronjob-container image: busybox command: [\"echo\", \"Hello, Kubernetes!\"] EOF\rExample 3 kubectl apply -f - \u003c\u003cEOF apiVersion: batch/v1beta1 kind: CronJob metadata: name: parallel-cronjob spec: schedule: \"0 */6 * * *\" # Run every 6 hours jobTemplate: spec: completions: 2 parallelism: 1 template: spec: containers: - name: parallel-cronjob-container image: busybox command: [\"echo\", \"Running parallel cronjob\"] EOF\rExample 4 kubectl apply -f - \u003c\u003cEOF apiVersion: batch/v1beta1 kind: CronJob metadata: name: cronjob-volume-mounts spec: schedule: \"*/5 * * * *\" # Run every 5 minutes jobTemplate: spec: template: spec: containers: - name: cronjob-volume-mounts-container image: busybox command: [\"/bin/sh\", \"-c\"] args: [\"echo Hello \u003e /data/hello.txt\"] volumeMounts: - name: data-volume mountPath: /data volumes: - name: data-volume emptyDir: {} EOF\rCronjob/job with imperative way kubectl create job myjob --image=busybox --command -- echo \"Hello, Kubernetes!\" kubectl create cronjob mycronjob --image=busybox --schedule=\"*/5 * * * *\" --command -- echo \"Scheduled Job\"\rSuspend an active Job: kubectl patch job/myjob --type=strategic --patch '{\"spec\":{\"suspend\":true}}'\rResume a suspended Job: kubectl patch job/myjob --type=strategic --patch '{\"spec\":{\"suspend\":false}}'\rReplicaset Create a replicaset Scale Replicaset Check the image used for replicaset Change the image for a rs Create a service for rs Forward the port of a rs Test if the app is opening ReplicasSet Yaml example kubectl apply -f - \u003c\u003cEOF apiVersion: apps/v1 kind: ReplicaSet metadata: name: my-replicaset spec: replicas: 3 selector: matchLabels: app: myapp template: metadata: labels: app: myapp spec: containers: - name: myapp-container image: nginx EOF\rView Replicaset kubectl get replicasets kubectl get rs\rScale up/down a replicaset kubectl scale replicaset my-replicaset --replicas=5\rCreate a service for Replicaset kubectl apply -f - \u003c\u003cEOF apiVersion: v1 kind: Service metadata: name: my-service spec: selector: app: myapp ports: - protocol: TCP port: 80 targetPort: 8080 type: LoadBalancer EOF\rconfigure port forward for replicaset kubectl port-forward replicaset/my-replicaset 8080:80\rDaemonset A DaemonSet ensures that all (or some) Nodes run a copy of a Pod. As nodes are added to the cluster, Pods are added to them. As nodes are removed from the cluster, those Pods are garbage collected. Deleting a DaemonSet will clean up the Pods it created.\nSome typical uses of a DaemonSet are: running a cluster storage daemon on every node running a logs collection daemon on every node running a node monitoring daemon on every node Create a daemonset kubectl apply -f - \u003c\u003cEOF apiVersion: apps/v1 kind: DaemonSet metadata: name: fluentd-elasticsearch namespace: kube-system labels: k8s-app: fluentd-logging spec: selector: matchLabels: name: fluentd-elasticsearch template: metadata: labels: name: fluentd-elasticsearch spec: tolerations: # these tolerations are to have the daemonset runnable on control plane nodes # remove them if your control plane nodes should not run pods - key: node-role.kubernetes.io/control-plane operator: Exists effect: NoSchedule - key: node-role.kubernetes.io/master operator: Exists effect: NoSchedule containers: - name: fluentd-elasticsearch image: quay.io/fluentd_elasticsearch/fluentd:v2.5.2 resources: limits: memory: 200Mi requests: cpu: 100m memory: 200Mi volumeMounts: - name: varlog mountPath: /var/log # it may be desirable to set a high priority class to ensure that a DaemonSet Pod # preempts running Pods # priorityClassName: important terminationGracePeriodSeconds: 30 volumes: - name: varlog hostPath: path: /var/log EOF\rCheck the daemonset kubectl get ds\rtest if the daemonset is as expected. kubectl get pods -n kube-system -o wide |grep -i fluentd-elasticsearch\rcheck the pod if they are creating on all nodes DNS Troubleshooting: Follow this for DNS Troubleshooting\nCreate a simple Pod to use as a test environment kubectl apply -f - \u003c\u003cEOF apiVersion: v1 kind: Pod metadata: name: dnsutils namespace: default spec: containers: - name: dnsutils image: registry.k8s.io/e2e-test-images/jessie-dnsutils:1.3 command: - sleep - \"infinity\" imagePullPolicy: IfNotPresent restartPolicy: Always EOF\rRun nslookup for kuberntes cluster kubectl exec -i -t dnsutils -- nslookup kubernetes.default\rCheck the local DNS configuration first kubectl exec -ti dnsutils -- cat /etc/resolv.conf\rif you get some error Check if the DNS pod is running kubectl get pods --namespace=kube-system -l k8s-app=kube-dns\rCheck for errors in the DNS pod kubectl logs --namespace=kube-system -l k8s-app=kube-dns\rIs DNS service up? kubectl get svc --namespace=kube-system\rAre DNS endpoints exposed? kubectl get endpoints kube-dns --namespace=kube-system\rDNS for Services and Pods Kubernetes creates DNS records for Services and Pods. You can contact Services with consistent DNS names instead of IP addresses. Services defined in the cluster are assigned DNS names. By default, a client Pod’s DNS search list includes the Pod’s own namespace and the cluster’s default domain.",
    "description": "Topics Service CLusterIP NodePort LoadBalancer Headless Service METALLB DNS Troubleshooting Deployment ReplicaSet Daemonset Labels and Selector Type of Selector Equality based Set based selector Job CronJob Metric Server HPA(pod Autoscaling) Helm Overview Service A Service is a method for exposing a network application that is running as one or more Pods in your cluster.\nA key aim of Services in Kubernetes is that you don’t need to modify your existing application to use an unfamiliar service discovery mechanism. A Kubernetes service is a logical abstraction for a deployed group of pods in a cluster What are the types of Kubernetes services? ClusterIP: Exposes a service which is only accessible from within the cluster. NodePort: Exposes a service via a static port on each node’s IP. NodePorts are in the 30000-32767 range by default LoadBalancer: Exposes the service via the cloud provider’s load balancer. ExternalName Maps a service to a predefined externalName field by returning a value for the CNAME record. Create a service of Type(ClusterIp) kubectl apply -f - \u003c\u003cEOF apiVersion: v1 kind: Service metadata: name: myapp-service spec: selector: app: myapp ports: - protocol: TCP port: 80 targetPort: 8080 EOF\rCreate a service of Type (NodePort) kubectl expose pod/\u003cname\u003e --port 80 --target-port=80 --name \u003cSvc\u003e\rCreate a type of service (LoadBalancer) kubectl expose pod/name --port=80 --type=LoadBalancer\rCheck the endpoint kubectl get ep\rAchieve the blue green Delete a service Exchange the service selector Metallb A LoadBalancer Solution for OnPrem Kubernetes Deploying Metallb Steps: git clone https://gitlab.com/container-and-kubernetes/kubernetes-2024.git cd kubernetes-2024/ cd metallb kubectl apply -f 01 01_metallb.yaml kubectl apply -f 02_metallb-config.yaml\rMake a test after creating a service kubectl apply -f 03_test-load-balancer.yaml\rCheck the service kubectl get svc\rMetric Server Metric Server Github Link",
    "tags": [],
    "title": "Part 3",
    "uri": "/part03/index.html"
  },
  {
    "breadcrumb": "/",
    "content": "Topics Kubernetes Probes Kubernetes Storage Secret ConfigMap emptyDIr hostPath StatefulSet Kubernetes IngressController Kubectl Set Kubectl Patch Probes The kubelet uses liveness probes to know when to restart a container. Liveness probes can be a powerful way to recover from application failures, but they should be used with caution.\nLiveness Probe It checks whether the application running inside the container is still alive and functioning properly. Readiness Probe: It determines whether the application inside the container is ready to accept traffic or requests. When a readiness probe fails, Kubernetes stops sending traffic to the container until it passes the probe. This is useful during application startup or when the application needs some time to initialize before serving traffic.\nStartup Probe (introduced in Kubernetes 1.16): It is similar to a readiness probe, but it only runs during the initial startup of a container. The purpose of the startup probe is to differentiate between a container that’s starting up and one that’s in a crashed state. Once the startup probe succeeds, it is disabled, and the readiness probe takes over.\nCreate a pod with startup probe Make a test and monitor the pods and logs as well Create a pod with liveness prode Create a pod with readiness probe Probes have a number of fields that you can use to more precisely control the behavior of startup, liveness and readiness checks:\ninitialDelaySeconds: Number of seconds after the container has started before startup, liveness or readiness probes are initiated. Defaults to 0 seconds. Minimum value is 0. periodSeconds: How often (in seconds) to perform the probe. Default to 10 seconds. The minimum value is 1. timeoutSeconds: Number of seconds after which the probe times out. Defaults to 1 second. Minimum value is 1. successThreshold: Minimum consecutive successes for the probe to be considered successful after having failed. Defaults to 1. Must be 1 for liveness and startup Probes. Minimum value is 1. failureThreshold: After a probe fails failureThreshold times in a row, terminationGracePeriodSeconds: configure a grace period for the kubelet to wait between triggering a shut down of the failed container, and then forcing the container runtime to stop that container. The default is to inherit the Pod-level value for terminationGracePeriodSeconds (30 seconds if not specified), and the minimum value is 1. See probe-level terminationGracePeriodSeconds for more detail. A pod with Startup Probe kubectl apply -f - \u003c\u003cEOF apiVersion: v1 kind: Pod metadata: name: pod-with-startup-probe spec: containers: - name: nginx-container image: nginx:latest startupProbe: httpGet: path: / port: 80 initialDelaySeconds: 20 periodSeconds: 5 EOF\rA Pod with Liveness probe kubectl apply -f - \u003c\u003cEOF apiVersion: v1 kind: Pod metadata: name: pod-with-liveness-probe spec: containers: - name: nginx-container image: nginx:latest livenessProbe: httpGet: path: / port: 80 initialDelaySeconds: 10 periodSeconds: 5 EOF\rA Pod with Readiness probe kubectl apply -f - \u003c\u003cEOF apiVersion: v1 kind: Pod metadata: name: pod-with-readiness-probe spec: containers: - name: nginx-container image: nginx:latest readinessProbe: httpGet: path: / port: 80 initialDelaySeconds: 15 periodSeconds: 5 EOF\rKey Points about kubernetes Probes initialDelaySeconds 60 seconds specifies the delay before the probe starts executing after the container starts. periodSeconds 10 seconds means that the probe will be executed every 10 seconds. failureThreshold 3 indicates that the container will be marked as unhealthy after three consecutive failed probes. Kubernetes Storage Set up NFS storage git clone https://gitlab.com/container-and-kubernetes/kubernetes-2024.git cd kubernetes-2024/nfs-subdir-external-provisioner\rRun the script on all nodes #!/usr/bin/env bash export DEBIAN_FRONTEND=noninteractive readonly NFS_SHARE=\"/srv/nfs/kubedata\" echo \"[TASK 1] apt update\" sudo apt-get update -qq \u003e/dev/null if [[ $HOSTNAME == \"kmaster\" ]]; then echo \"[TASK 2] install nfs server\" sudo -E apt-get install -y -qq nfs-kernel-server \u003e/dev/null echo \"[TASK 3] creating nfs exports\" sudo mkdir -p $NFS_SHARE sudo chown nobody:nogroup $NFS_SHARE echo \"$NFS_SHARE *(rw,sync,no_subtree_check)\" | sudo tee /etc/exports \u003e/dev/null sudo systemctl restart nfs-kernel-server else echo \"[TASK 2] install nfs common\" sudo -E apt-get install -y -qq nfs-common \u003e/dev/null fi\rNow apply nfs on kubernetes kubectl apply -f 01-setup-nfs-provisioner.yaml\rCreate a pvc using 02-test-claim.yaml kubectl apply -f 02-test-claim.yaml\rCheck newly created pvc kubectl get pvc\rStorage Classes A StorageClass provides a way for administrators to describe the classes of storage they offer. Different classes might map to quality-of-service levels, or to backup policies, or to arbitrary policies determined by the cluster administrators. You can define default storageclass You can also decide whether the pvc can be expanded. Example of storage class\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: managed-nfs-storage annotations: storageclass.kubernetes.io/is-default-class: \"true\" provisioner: k8s-sigs.io/nfs-subdir-external-provisioner parameters: archiveOnDelete: \"false\"\rCheck available storageClass kubectl get sc\rEnable Expansion of pvc query='{.items[?(@.metadata.annotations.storageclass\\.kubernetes\\.io/is-default-class==\"true\")].metadata.name}' default_sc=$(kubectl get sc -o=jsonpath=\"$query\") echo patching storage class \"[$default_sc]\" kubectl patch storageclass $default_sc -p '{\"allowVolumeExpansion\": true}'\rA PersistentVolume A PersistentVolume (PV) is a piece of storage in the cluster that has been provisioned by an administrator or dynamically provisioned using Storage Classes. It is a resource in the cluster just like a node is a cluster resource. A PersistentVolumeClaim (PVC) A PersistentVolumeClaim (PVC) is a request for storage by a user. It is similar to a Pod. Pods consume node resources and PVCs consume PV resources. Lifecycle of a volume and claim PVs are resources in the cluster. PVCs are requests for those resources and also act as claim checks to the resource. The interaction between PVs and PVCs follows this lifecycle: Provisioning There are two ways PVs may be provisioned: statically or dynamically.\nStatic A cluster administrator creates a number of PVs. They carry the details of the real storage, which is available for use by cluster users. They exist in the Kubernetes API and are available for consumption.\nDynamic When none of the static PVs the administrator created match a user’s PersistentVolumeClaim, the cluster may try to dynamically provision a volume specially for the PVC. This provisioning is based on StorageClasses: the PVC must request a storage class The administrator must have created and configured that class for dynamic provisioning to occur Access Modes The access modes are: ReadWriteOnce the volume can be mounted as read-write by a single node. ReadWriteOnce access mode still can allow multiple pods to access the volume when the pods are running on the same node. For single pod access, please see ReadWriteOncePod.\nReadOnlyMany the volume can be mounted as read-only by many nodes.\nReadWriteMany the volume can be mounted as read-write by many nodes.\nReadWriteOncePod the volume can be mounted as read-write by a single Pod. Use ReadWriteOncePod access mode if you want to ensure that only one pod across the whole cluster can read that PVC or write to it.\nIn the CLI, the access modes are abbreviated to: RWO - ReadWriteOnce\nROX - ReadOnlyMany\nRWX - ReadWriteMany\nRWOP - ReadWriteOncePod\nExample of a pvc\nkubectl apply -f - \u003c\u003cEOF apiVersion: v1 kind: PersistentVolumeClaim metadata: name: task-pv-claim spec: storageClassName: manual accessModes: - ReadWriteOnce resources: requests: storage: 3Gi EOF\rCreate a pod and use the pvc there apiVersion: v1 kind: Pod metadata: name: task-pv-pod spec: volumes: - name: task-pv-storage persistentVolumeClaim: claimName: task-pv-claim containers: - name: task-pv-container image: nginx ports: - containerPort: 80 name: \"http-server\" volumeMounts: - mountPath: \"/usr/share/nginx/html\" name: task-pv-storage\rTest pod and pvc Also check the storage class kubectl get sc\remptyDir emptyDir are volumes that get created empty when a Pod is created. While a Pod is running its emptyDir exists. If a container in a Pod crashes the emptyDir content is unaffected. Deleting a Pod deletes all its emptyDirs. An emptyDir volume is first created when a Pod is assigned to a Node and initially its empty A Volume of type emptyDir that lasts for the life of the Pod, even if the Container terminates and restarts. If a container in a Pod crashes the emptyDir content is unaffected. All containers in a Pod share use of the emptyDir volume . Each container can independently mount the emptyDir at the same / or different path. Using emptyDir, The Kubelet will create the directory in the container, but not mount any storage. Containers in the Pod can all read/write the same files in the emptyDir volume, though that volume can be mounted at the same or different paths in each Container. When a Pod is removed from a node for any reason, the data in the emptyDir is deleted forever along with the container. A Container crashing does NOT remove a Pod from a node, so the data in an emptyDir volume is safe across Container crashes. By default, emptyDir volumes are stored on whatever medium is backing the node – that might be disk or SSD or network storage. You can set the emptyDir.medium field to “Memory” to tell Kubernetes to mount a tmpfs (RAM-backed filesystem) for you instead. The location should of emptyDir should be in /var/lib/kubelet/pods/{podid}/volumes/kubernetes.io~empty-dir/ on the given node where your pod is running. Example: apiVersion: v1 kind: Pod metadata: name: test-pd spec: containers: - image: nginx name: test-container volumeMounts: - mountPath: /cache name: cache-volume - image: redis name: redis volumeMounts: - mountPath: /data name: cache-volume volumes: - name: cache-volume emptyDir: {}\rhostPath: The hostPath volume mounts a resource from the host node filesystem. the resources could be directory, file socket, character, or block device. These resources mu A hostPath volume mounts a file or directory from the host node’s filesystem into your pod. A hostPath PersistentVolume must be used only in a single-node cluster. Kubernetes does not support hostPath on a multi-node cluster currently. The directories created on the underlying hosts are only writable by root. You either need to run your process as root in a privileged container or modify the file permissions on the host to be able to write to a hostPath volume Example: apiVersion: v1 kind: Pod metadata: name: test-pd spec: containers: - image: k8s.gcr.io/test-webserver name: test-container volumeMounts: - mountPath: /test-pd name: test-volume volumes: - name: test-volume hostPath: # directory location on host path: /data # this field is optional type: DirectoryOrCreate\rPersistentVolumeClaim https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/\nKubernetes Objects Secret Creating a secret Checking the value of a secret Use jsonpath to fetch the value Use secret in pod an environment value Use secret as a file in pod Creating a secret of type tls Configmap Create a config map use cm as a file in pod Secret To create a secret, you can use the kubectl create secret command. kubectl create secret generic my-secret --from-literal=MYSQL_ROOT_PASSWORD=root To create secret of type TLS kubectl create secret tls my-tls-secret --cert=path/to/tls.crt --key=path/to/tls.key\rCheck secret kubectl get secrets\ruse the secret with pod as an environment variable apiVersion: v1 kind: Pod metadata: name: mypod spec: containers: - name: mycontainer image: mysql env: - name: MYSQL_ROOT_PASSWORD valueFrom: secretKeyRef: name: my-secret key: MYSQL_ROOT_PASSWORD Use secret as a file apiVersion: v1 kind: Pod metadata: name: mypod spec: containers: - name: mycontainer image: myimage volumeMounts: - name: secret-volume mountPath: /etc/secrets volumes: - name: secret-volume secret: secretName: my-secret\rHow to extract the value from a secret kubectl get secret my-secret -o jsonpath='{.data.username}' | base64 --decode \u003e username.txt kubectl get secret my-secret -o jsonpath='{.data.password}' | base64 --decode \u003e password.txt\rConfigmap Create a configmap kubectl create configmap my-configmap --from-literal=key1=value1\rCreate cm with yaml apiVersion: v1 kind: ConfigMap metadata: name: my-configmap data: key1: value1 key2: value2\rConfigmap for a config file kubectl create configmap my-configmap --from-file=config-file.txt\ruse configmap as a file in a pod apiVersion: v1 kind: Pod metadata: name: mypod spec: containers: - name: mycontainer image: myimage envFrom: - configMapRef: name: my-configmap --- apiVersion: v1 kind: Pod metadata: name: mypod spec: containers: - name: mycontainer image: myimage volumeMounts: - name: config-volume mountPath: /etc/config volumes: - name: config-volume configMap: name: my-configmap\rIngressController Documentation Referred: https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/\nhttps://kubernetes.github.io/ingress-nginx/deploy/\nStep 1: Install Nginx Ingress Controller: helm upgrade --install ingress-nginx ingress-nginx --repo https://kubernetes.github.io/ingress-nginx --namespace ingress-nginx --create-namespace\rStep 2: Verify the Ingress Controller Resource helm list --all-namespaces kubectl get ingressclass kubectl get service -n ingress-nginx\rcheck if the IngressClass is available kubectl get ingressClassName\rAdd the Application dns to your hosts in case you are not using DNS service\nnano /etc/hosts for windows it is under C:\\Windows\\System32\\drivers\\etc\r\u003cADD-LOAD-BALANCER-IP\u003e website01.example.internal website02.example.internal\rStep 1: Create two Pods\nkubectl run service-pod-1 --image=nginx kubectl run service-pod-2 --image=nginx\rStep 2: Create Service for above created pods\nkubectl expose pod service-pod-1 --name service1 --port=80 --target-port=80 kubectl expose pod service-pod-2 --name service2 --port=80 --target-port=80 kubectl get services\rStep 3: Verify Service to POD connectivity\nkubectl run frontend-pod --image=ubuntu --command -- sleep 36000 kubectl exec -it frontend-pod -- bash apt-get update \u0026\u0026 apt-get -y install curl nano curl \u003cSERVER-1-IP\u003e curl \u003cSERVER-1-IP\u003e\rCheck if the application is reachable\ncurl website01.example.internal curl website02.example.internal\rStep 5: Change the Default Nginx Page for Each Service\nkubectl exec -it service-pod-1 -- bash cd /usr/share/nginx/html echo \"This is Website 1\" \u003e index.html\rkubectl exec -it service-pod-2 -- bash cd /usr/share/nginx/html echo \"This is Website 2\" \u003e index.html\rStep 6: Verification\nkubectl exec -it frontend-pod -- bash curl website01.example.internal curl website02.example.internal\rDocumentation Referred: Lean About name based virtual Hosting\nStep 7: Create Ingress Resource\nkubectl apply -f - \u003c\u003cEOF apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: name-virtual-host-ingress spec: ingressClassName: nginx rules: - host: website01.example.internal http: paths: - pathType: Prefix path: \"/\" backend: service: name: service1 port: number: 80 - host: website02.example.internal http: paths: - pathType: Prefix path: \"/\" backend: service: name: service2 port: number: 80 EOF\rCheck newlly created ingress rules\nkubectl get ingress\rCheck more information for ingress\nkubectl describe ingress name-virtual-host-ingress created\rNow check if the application is opening in browser\nKubernetes Patch command Description Briefly describe the purpose of the issue.\nExamples using kubectl patch Patch a Deployment (Update Replicas) kubectl patch deployment \u003cdeployment-name\u003e -p '{\"spec\": {\"replicas\": 3}}'\rPatch a Pod (Update Container Image) kubectl patch pod \u003cpod-name\u003e -p '{\"spec\": {\"containers\": [{\"name\": \"container-name\", \"image\": \"new-image:tag\"}]}}'\rUpdate configmap kubectl patch configmap \u003cconfigmap-name\u003e -p '{\"data\": {\"new-key\": \"new-value\"}}'\rPatch a Service (Update Service Type) kubectl patch service \u003cservice-name\u003e -p '{\"spec\": {\"type\": \"LoadBalancer\"}}'\rPatch a PVC (Update Storage Size) kubectl patch pvc \u003cpvc-name\u003e -p '{\"spec\": {\"resources\": {\"requests\": {\"storage\": \"5Gi\"}}}}'\rStatefulSets StatefulSet is the workload API object used to manage stateful applications. Like a Deployment, a StatefulSet manages Pods that are based on an identical container spec. Unlike a Deployment, a StatefulSet maintains a sticky identity for each of its Pods. Using StatefulSets StatefulSets are valuable for applications that require one or more of the following. Stable, unique network identifiers. Stable, persistent storage. Ordered, graceful deployment and scaling. Ordered, automated rolling updates. Example of statefulset kubectl apply -f - \u003c\u003cEOF apiVersion: v1 kind: Service metadata: name: nginx labels: app: nginx spec: ports: - port: 80 name: web clusterIP: None selector: app: nginx --- apiVersion: apps/v1 kind: StatefulSet metadata: name: web spec: selector: matchLabels: app: nginx # has to match .spec.template.metadata.labels serviceName: \"nginx\" replicas: 3 # by default is 1 minReadySeconds: 10 # by default is 0 template: metadata: labels: app: nginx # has to match .spec.selector.matchLabels spec: terminationGracePeriodSeconds: 10 containers: - name: nginx image: registry.k8s.io/nginx-slim:0.8 ports: - containerPort: 80 name: web volumeMounts: - name: www mountPath: /usr/share/nginx/html volumeClaimTemplates: - metadata: name: www spec: accessModes: [ \"ReadWriteOnce\" ] storageClassName: \"my-storage-class\" \u003cChange this as per your sc\u003e resources: requests: storage: 1Gi EOF\rcheck statefulset kubectl get sts\rAlso check for service type kubectl get svc\rCheck for pvc kubectl get pvc\rCheck pods kubectl get pods\rTest after deleting a pod if the pod is taking the same name again",
    "description": "Topics Kubernetes Probes Kubernetes Storage Secret ConfigMap emptyDIr hostPath StatefulSet Kubernetes IngressController Kubectl Set Kubectl Patch Probes The kubelet uses liveness probes to know when to restart a container. Liveness probes can be a powerful way to recover from application failures, but they should be used with caution.\nLiveness Probe It checks whether the application running inside the container is still alive and functioning properly. Readiness Probe: It determines whether the application inside the container is ready to accept traffic or requests. When a readiness probe fails, Kubernetes stops sending traffic to the container until it passes the probe. This is useful during application startup or when the application needs some time to initialize before serving traffic.",
    "tags": [],
    "title": "Part 4",
    "uri": "/part04/index.html"
  },
  {
    "breadcrumb": "/",
    "content": "Topics Rancher Network Policies Metric Server Configure HPA Logging with Grafana Loki Monitoring with Prometheus Enable Audit logs Rancher (Kubernetes Management Tool) Rancher Installation Steps\nRancher is a Kubernetes management tool to deploy and run clusters anywhere and on any provider.\nRancher can provision Kubernetes from a hosted provider, provision compute nodes and then install Kubernetes onto them, or import existing Kubernetes clusters running anywhere.\nRancher adds significant value on top of Kubernetes, first by centralizing authentication and role-based access control (RBAC) for all of the clusters, giving global admins the ability to control cluster access from one location.\nIt then enables detailed monitoring and alerting for clusters and their resources, ships logs to external providers, and integrates directly with Helm via the Application Catalog. If you have an external CI/CD system, you can plug it into Rancher, but if you don’t, Rancher even includes Fleet to help you automatically deploy and upgrade workloads.\nRancher is a complete container management platform for Kubernetes, giving you the tools to successfully run Kubernetes anywhere.\nRancher Installation on docker\nsudo docker run --privileged -d --restart=unless-stopped -p 80:80 -p 443:443 rancher/rancher\rLogin to rancher Wait for atleast 4 to 5 mins Monitoring all node pods Try to deploy some pod/app Get started with Kubernetes network policy Kubernetes network policy lets administrators and developers enforce which network traffic is allowed using rules. Kubernetes network policy lets developers secure access to and from their applications using the same simple language they use to deploy them. Developers can focus on their applications without understanding low-level networking concepts. Enabling developers to easily secure their applications using network policies supports a shift left DevOps environment. The Kubernetes Network Policy API supports the following features: Policies are namespace scoped Policies are applied to pods using label selectors Policy rules can specify the traffic that is allowed to/from pods, namespaces, or CIDRs Policy rules can specify protocols (TCP, UDP, SCTP), named ports or port numbers Ingress and egress ingress is incoming traffic to the pod, and egress is outgoing traffic from the pod. In Kubernetes network policy, you create ingress and egress “allow” rules independently (egress, ingress, or both). Default deny/allow behavior Default allow means all traffic is allowed by default, unless otherwise specified. Default deny means all traffic is denied by default, unless explicitly allowed. Network Policy Create base setup (Default behaviour is all traffic is allowd) Create a namespace kubectl create ns external\rTest the default behaviour by creating pod and try to ping kubectl run pod-1 --image=praqma/network-multitool kubectl run pod-2 --image=praqma/network-multitool kubectl run pod-3 --image=praqma/network-multitool -n external\rGet Ip address kubectl get pods -o wide kubectl get pods -o wide -n external\rCheck connectivity from POD-1 to POD2 and POD-3 and Internet kubectl exec -it pod-1 -- ping [pod-2-ip] kubectl exec -it pod-1 -- ping [pod-3-ip] kubectl exec -it pod-1 -- ping google.com\rRule 1: Deny all Ingress traffic kubectl apply -f - \u003c\u003cEOF --- apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: default-deny-ingress spec: podSelector: {} policyTypes: - Ingress EOF\rCheck newly applied network policy kubectl get netpol\rTest incoming traffic should not be allowed kubectl exec -it pod-1 -- ping [pod2-ip] kubectl exec -it pod-1 -- ping [pod3-ip] kubectl exec -it pod-1 -- ping google.com kubectl exec -it pod-3 -- ping [pod1-ip] kubectl exec -it pod-3 -- ping [pod2-ip]\rRule 2: Allow Ingress kubectl apply -f - \u003c\u003cEOF --- apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: allow-all-ingress spec: podSelector: {} ingress: - {} policyTypes: - Ingress EOF\rTest again kubectl exec -it pod-3 -- ping [pod1-ip] kubectl exec -it pod-3 -- ping [pod2-ip]\rRule 3: Deny All Egress: kubectl apply -f - \u003c\u003cEOF apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: default-deny-egress spec: podSelector: {} policyTypes: - Egress EOF\rMake a test kubectl exec -it pod-2 -- ping google.com kubectl exec -n external -it pod-3 -- ping [pod1] kubectl exec -n external -it pod-3 -- ping [pod2]\rRule 4 - PodSelector: kubectl apply -f - \u003c\u003cEOF apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: podselector-suspicous spec: podSelector: matchLabels: role: suspicious policyTypes: - Ingress - Egress EOF\rRule 5 - Ingress From: kubectl label pod pod-1 role=secure\rkubectl apply -f - \u003c\u003cEOF apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: ingress-from-ips spec: podSelector: matchLabels: role: secure ingress: - from: - ipBlock: cidr: 192.168.0.0/16 except: - 192.168.137.70/32 policyTypes: - Ingress EOF\rkubectl exec -n external -it pod-3 -- ping [pod-1] kubectl exec -it pod-2 -- ping [pod-1]\rkubectl label pod pod-1 role- kubectl delete -f netpol.yaml\rRule 6 - Egress To: kubectl label pod pod-1 role=secure\rkubectl apply -f - \u003c\u003cEOF apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: egress-to-ips spec: podSelector: matchLabels: role: secure egress: - to: - ipBlock: cidr: 192.168.137.70/32 policyTypes: - Egress EOF\rkubectl exec -it pod-1 -- ping [pod-2] kubectl exec -it pod-1 -- ping google.com\rkubectl delete -f netpol.yaml\rRule 7 - Namespace Selector: kubectl apply -f - \u003c\u003cEOF apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: namespace-selector spec: podSelector: matchLabels: role: secure ingress: - from: - namespaceSelector: matchLabels: role: app podSelector: matchLabels: role: reconcile policyTypes: - Ingress EOF\rCreate ingress policies with pod selector color=blue on port 80 kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: name: allow-same-namespace namespace: default spec: podSelector: matchLabels: color: blue ingress: - from: - podSelector: matchLabels: color: red ports: - port: 80\rAllow ingress traffic from pods in a different namespace using namespaceSelector kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: name: allow-different-namespace namespace: default spec: podSelector: matchLabels: color: blue ingress: - from: - podSelector: matchLabels: color: red namespaceSelector: matchLabels: shape: square ports: - port: 80\rHere’s an example of a Network Policy in Kubernetes that allows incoming traffic only from a pod with label color=red in a namespace with label shape=square, on port 80:\nCreate egress policies In the following example, outbound traffic is allowed only if they go to a pod with label color=red, on port 80.\nkind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: name: allow-egress-same-namespace namespace: default spec: podSelector: matchLabels: color: blue egress: - to: - podSelector: matchLabels: color: red ports: - port: 80\rAllow a CIDR range The following policy allows egress traffic to pods in CIDR, 172.18.0.0/24. kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: name: allow-egress-external namespace: default spec: podSelector: matchLabels: color: red egress: - to: - ipBlock: cidr: 172.18.0.0/24\rCreate deny-all default ingress and egress network policy kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: name: default-deny namespace: policy-demo spec: podSelector: matchLabels: {} policyTypes: - Ingress - Egress\rKubernetes Monitoring What is Prometheus ? Prometheus is a open source Linux Server Monitoring tool mainly used for metrics monitoring, event monitoring, alert management, etc. Prometheus has changed the way of monitoring systems and that is why it has become the Top-Level project of Cloud Native Computing Foundation (CNCF). Prometheus uses a powerful query language i.e. “PromQL”. In Prometheus tabs are on and handles hundreds of services and microservices. Prometheus use multiple modes used for graphing and dashboarding support. Prometheus Architecture Promethe port: 9090 Node Exporter: 9100\rPrometheus Components Prometheus Server\nPrometheus server is a first component of Prometheus architecture. Prometheus server is a core of Prometheus architecture which is divided into several parts like Storage, PromQL, HTTP server, etc. In Prometheus server data is scraped from the target nodes and then stored int the database.\n1.1 Storage\nStorage in Prometheus server has a local on disk storge. Prometheus has many interfaces that allow integrating with remote storage systems.\n1.2. PromQL\nPrometheus uses its own query language i.e. PromQL which is very powerful querying language. PromQL allows the user to select and aggregate the data.\nService Discovery\nNext and very important component of Prometheus Server is the Service Discovery. With the help of Service discovery the services are identified which are need to scraped. To Pull metrics, identification of services and finding the targets are compulsory needed. Through Service discovery we monitor the entities and can also locate its targets.\nScrape Target\nOnce the services are identified and the targets are ready then we can pull metrics from it and can scrape the target. We can export the data of end point using node exporters. Once the metrics or other data is pulled, Prometheus stores it in a local storage.\nAlert Manager\nAlert Manager handles the alerts which may occurs during the session. Alert manager handles all the alerts which are sent by Prometheus server. Alert manager is one of the very useful component of Prometheus tool. If in case any big error or any issue occurs, alert manager manage those alerts and contact with human via E-mail, Text Messages, On-call, or any other chat application service.\nUser Interface\nUser interface is also a important component as it builds a bridge between the user and the system. In Prometheus, user interface are note that much user friendly and can be used till graph queries. For good exclusive dashboards Prometheus works together with Grafana (visualization tool). Using Grafana over Prometheus to visualize properly we can use custom dashboards. Grafana dashboards displays via pie charts, line charts, tables, good data graphs of CPU usage, RAM utilization, network load, etc with indicators. Grafana supports and run with Prometheus by querying language i.e. PromQL. To fetch data from Prometheus and to display the results on Grafana dashboards PromQL is used.\nWhat is Grafana ? Grafana is a free and open source visualization tool mostly used with Prometheus to which monitor metrics. Grafana provides various dashboards, charts, graphs, alerts for the particular data source. Grafana allows us to query, visualize, explore metrics and set alerts for the data source which can be a system, server, nodes, cluster, etc. We can also create our own dynamic dashboard for visualization and monitoring. We can save the dashboard and can even share with our team members which is one of the main advantage of Grafana.\nWhat is Node Exporter ? Node exporter is one of the Prometheus exporters which is used to expose servers or system OS metrics.\nWith the help of Node exporter we can expose various resources of the system like RAM, CPU utilization, Memory Utilization, disk space.\nNode exporter runs as a system service which gathers the metrics of your system and that gathered metrics is displayed with the help of Grafana visualization tool.\nPrerequisites Access to Kubernetes Cluster Helm cli installed Add Prometheus Helm Repo helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo update\rSearch for Prometheus helm chart helm search repo prometheus-community\rInstall Prometheus Helm Chart on Kubernetes Cluster helm install prometheus prometheus-community/prometheus\rCheck all services pods,deployment kubectl get svc,deployment,pods,secret\rExposing the prometheus-server Kubernetes Service kubectl expose service prometheus-server --type=LoadBalancer --target-port=9090 --name=prometheus-server-ext\rAccess Prometheus from UI LoadBalancer Ip\rCheck targets, Rules etc Install Grafana Search for Grafana Chart helm search hub grafana\rAdd and update Grafana repo helm repo add grafana https://grafana.github.io/helm-charts helm repo update\rInstall Grafana Helm Chart on Kubernetes Cluster helm install grafana grafana/grafana\rCheck for Services and other details kubectl get service\rExposing the grafana Kubernetes Service kubectl expose service grafana --type=LoadBalancer --target-port=3000 --name=grafana-ext\rTake the password for Grafana user(Admin) kubectl get secret --namespace default grafana -o jsonpath=\"{.data.admin-password}\" | base64 --decode ; echo\rLogin to Grafana and add datasource Select Prometheus as the data source: Click on “Save \u0026 test” to save your changes.\nGrafana Dashboard To import a Grafana Dashboard, follow these steps: Click here\nKuberntes Monitoring with Grafana Loki Kubernetes Monitoring source https://artifacthub.io/packages/helm/grafana/loki-stack Grafana Loki Loki-Stack Helm Chart Get Repo Info helm repo add grafana https://grafana.github.io/helm-charts helm repo update\rDeploy Loki and Promtail to your cluster helm upgrade --install loki grafana/loki-stack \\ --set fluent-bit.enabled=false,promtail.enabled=true,grafana.enabled=true\rTo get the admin password for the Grafana pod, run the following command: kubectl get secret --namespace \u003cYOUR-NAMESPACE\u003e loki-grafana -o jsonpath=\"{.data.admin-password}\" | base64 --decode ; echo\rTo access Grafana you can use port forwarding or Service LoadBalancer kubectl port-forward --namespace \u003cYOUR-NAMESPACE\u003e service/loki-grafana 3000:80\rEtcd Backup and Restore on Kubernetes Cluster Login to control plane ssh vagrant@172.16.16.100\rChange user to root sudo -s\rInstall etcd cli sudo apt install etcd-client\rWe need to pass the following three pieces of information to etcdctl to take an etcd snapshot. etcd endpoint (–endpoints)\nca certificate (–cacert)\nserver certificate (–cert)\nserver key (–key)\nCheck the file for above information\ncat /etc/kubernetes/manifests/etcd.yaml\rTake an etcd snapshot backup using the following command. ETCDCTL_API=3 etcdctl \\ --endpoints=https://127.0.0.1:2379 \\ --cacert=\u003cca-file\u003e \\ --cert=\u003ccert-file\u003e \\ --key=\u003ckey-file\u003e \\ snapshot save \u003cbackup-file-location\u003e ETCDCTL_API=3 etcdctl \\ --endpoints=https://127.0.0.1:2379 \\ --cacert=/etc/kubernetes/pki/etcd/ca.crt \\ --cert=/etc/kubernetes/pki/etcd/server.crt \\ --key=/etc/kubernetes/pki/etcd/server.key \\ snapshot save /opt/backup/etcd.db\ryou can verify the snapshot using the following command. ETCDCTL_API=3 etcdctl --write-out=table snapshot status /opt/backup/etcd.db\rKubernetes etcd Restore Using Snapshot Backup\nUse the below command\nETCDCTL_API=3 etcdctl snapshot restore \u003cbackup-file-location\u003e ETCDCTL_API=3 etcdctl snapshot restore /opt/backup/etcd.db ETCDCTL_API=3 etcdctl --data-dir /opt/etcd snapshot restore /opt/backup/etcd.db\rKubernetes Api related commands All namespaced resources kubectl api-resources --namespaced=true kubectl api-resources --namespaced=false kubectl api-resources -o name kubectl api-resources -o wide kubectl api-resources --verbs=list,get kubectl api-resources --api-group=extensions",
    "description": "Topics Rancher Network Policies Metric Server Configure HPA Logging with Grafana Loki Monitoring with Prometheus Enable Audit logs Rancher (Kubernetes Management Tool) Rancher Installation Steps\nRancher is a Kubernetes management tool to deploy and run clusters anywhere and on any provider.\nRancher can provision Kubernetes from a hosted provider, provision compute nodes and then install Kubernetes onto them, or import existing Kubernetes clusters running anywhere.\nRancher adds significant value on top of Kubernetes, first by centralizing authentication and role-based access control (RBAC) for all of the clusters, giving global admins the ability to control cluster access from one location.",
    "tags": [],
    "title": "Part 5",
    "uri": "/part05/index.html"
  },
  {
    "breadcrumb": "/",
    "content": "Topics Create a virtual machine git clone https://gitlab.com/container-and-kubernetes/kubernetes-2024.git cd kubernetes-2024/ vagrant up\rYou will have a vm ready within 5 mins of time Login to Linux vm ssh vagrant@172.16.16.105 Enter password: vagrant\rSwitch from vagrant user of root user sudo -s\rDocker Basics Lab How to install docker # Add Docker's official GPG key: sudo apt-get update sudo apt-get install ca-certificates curl sudo install -m 0755 -d /etc/apt/keyrings sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc sudo chmod a+r /etc/apt/keyrings/docker.asc # Add the repository to Apt sources: echo \\ \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \\ $(. /etc/os-release \u0026\u0026 echo \"$VERSION_CODENAME\") stable\" | \\ sudo tee /etc/apt/sources.list.d/docker.list \u003e /dev/null sudo apt-get update\rRun the below command\nsudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin\rCheck docker version\ndocker version\rcheck all docker images\ndocker images\rcheck all running containers\ndocker ps Check all containers\ndocker ps -a\rDownload an image\ndocker pull \u003cimagename\u003e docker pull nginx\rHow to create a container\ndocker run container id (this will run a process in foreground)\rRun a container in backgroud\ndocker run -d container id\rLogin to a container\ndocker exec container id sh/bash\rDelete a container\ndocker rm containerid\rDelete container forcefully(if the container is running)\ndocker rm containerid --force\rCheck the id of all containers\ndocker ps -aq\rDelete all containers\ndocker rm $(docker ps -aq) -f\rDelete all images\ndocker rmi $(docker images -aq)\rDocker Networking Expose a docker container port to access from host ip docker run -d -p \u003chostport\u003e:\u003ccontainerport\u003e \u003cimage name\u003e\rNOTE: You can not map a host port which is already in use Docker storage How to map a directory to container dir docker run -d -v \u003cdir at host\u003e:\u003cdir at container\u003e \u003cimagename\u003e\rDocker Images How to login to docker hub\ndocker login\rHow to convert a running container into an image\ndocker commit containerid newname\rHow to save docker image into a tar\ndocker save \u003cimagename\u003e \u003e example.tar\rHow to import from a tar file\ndocker load \u003c example.tar\rHow to rename an image\ndocker tag \u003coldname\u003e \u003cnewname\u003e\rLogin to docker hub/any other container registry\ndocker login docker login \u003cregistry url if any\u003e\rTag the image before you push.\ndocker tag \u003cold image name\u003e \u003cnew image name \u003e\rHow to push image to dockerhub\ndocker push imagename\rCreating dockerimage from Docker file Dockerfile Documentation\nDestroy the vm once the Lab is completed from the directory of your Laptop where you have vagrantfile vagrant destroy Upcoming Docker Capabilities Docker Compose Dockerfile Docker containers are very similar to LXC containers, and they have similar security features. When you start a container with docker run, behind the scenes Docker creates a set of namespaces and control groups for the container.\nDocker makes heavy use of Linux kernel features. One of the fundamental aspects that containers make use of from Linux Kernel are namespaces and cgroups.\nNamespace Namespaces provide the first and most straightforward form of isolation. Processes running within a container cannot see, and even less affect, processes running in another container, or in the host system. Namespaces are a feature of the Linux kernel that partitions kernel resources such that one set of processes sees one set of resources and another set of processes sees a different set of resources.\nTypes of Namespaces Within the Linux kernel, there are different types of namespaces. Each namespace has its own unique properties:\nA user namespace has its own set of user IDs and group IDs for assignment to processes. In particular, this means that a process can have root privilege within its user namespace without having it in other user namespaces. A process ID (PID) namespace assigns a set of PIDs to processes that are independent from the set of PIDs in other namespaces. The first process created in a new namespace has PID 1 and child processes are assigned subsequent PIDs. If a child process is created with its own PID namespace, it has PID 1 in that namespace as well as its PID in the parent process’ namespace. See below for an example. A network namespace has an independent network stack: its own private routing table, set of IP addresses, socket listing, connection tracking table, firewall, and other network‑related resources. A mount namespace has an independent list of mount points seen by the processes in the namespace. This means that you can mount and unmount filesystems in a mount namespace without affecting the host filesystem. An interprocess communication (IPC) namespace has its own IPC resources, for example POSIX message queues. A UNIX Time‑Sharing (UTS) namespace allows a single system to appear to have different host and domain names to different processes. Create two container and check if the they are sharing the namespaces defined above docker run -d --name c1 ubuntu sleep 1d docker run -d --name c2 ubuntu sleep 2d\rLogin to container and check docker exec c2 ps aux\rNow check on host vm ps -aux\rNow create a container with the namespace docker run -d c3 --pid=container:c1 ununtu sleep 3d\rNow check if the pid is shared for c1 container and c3 container What Are cgroups? A control group (cgroup) is a Linux kernel feature that limits, accounts for, and isolates the resource usage (CPU, memory, disk I/O, network, and so on) of a collection of processes.\nCgroups provide the following features:\nResource limits – You can configure a cgroup to limit how much of a particular resource (memory or CPU, for example) a process can use. Prioritization – You can control how much of a resource (CPU, disk, or network) a process can use compared to processes in another cgroup when there is resource contention. Accounting – Resource limits are monitored and reported at the cgroup level. Control – You can change the status (frozen, stopped, or restarted) of all processes in a cgroup with a single command. Check the resouce stats docker stats\rIntroduction to capabilities The Linux kernel is able to break down the privileges of the root user into distinct units referred to as capabilities. For example, the CAP_CHOWN capability is what allows the root use to make arbitrary changes to file UIDs and GIDs. The CAP_DAC_OVERRIDE capability allows the root user to bypass kernel permission checks on file read, write and execute operations. Almost all of the special powers associated with the Linux root user are broken down into individual capabilities.\nThis breaking down of root privileges into granular capabilities allows you to:\nRemove individual capabilities from the root user account, making it less powerful/dangerous. Add privileges to non-root users at a very granular level.\nCapabilities apply to both files and threads. File capabilities allow users to execute programs with higher privileges. This is similar to the way the setuid bit works. Thread capabilities keep track of the current state of capabilities in running programs.\nCapabilities Definition CHOWN Make arbitrary changes to file UIDs and GIDs DAC_OVERRIDE Discretionary access control (DAC) - Bypass file read, write, and execute permission checks FSETID Don’t clear set-user-ID and set-group-ID mode bits when a file is modified; set the set-group-ID bit for a file whose GID does not match the file system or any of the supplementary GIDs of the calling process. FOWNER Bypass permission checks on operations that normally require the file system UID of the process to match the UID of the file, excluding those operations covered by CAP_DAC_OVERRIDE and CAP_DAC_READ_SEARCH. MKNOD MKNOD - Create special files using mknod(2) NET_RAW Use RAW and PACKET sockets; bind to any address for transparent proxying. SETGID Make arbitrary manipulations of process GIDs and supplementary GID list; forge GID when passing socket credentials via UNIX domain sockets; write a group ID mapping in a user namespace. SETUID Make arbitrary manipulations of process UIDs; forge UID when passing socket credentials via UNIX domain sockets; write a user ID mapping in a user namespace. SETFCAP Set file capabilities. SETPCAP If file capabilities are not supported: grant or remove any capability in the caller’s permitted capability set to or from any other process. NET_BIND_SERVICE Bind a socket to Internet domain privileged ports (port numbers less than 1024). SYS_CHROOT Use chroot(2) to change to a different root directory. KILL Bypass permission checks for sending signals. This includes use of the ioctl(2) KDSIGACCEPT operation. AUDIT_WRITE Write records to kernel auditing log. To drop capabilities from the root account of a container. sudo docker run --rm -it --cap-drop $CAP alpine sh\rTo add capabilities to the root account of a container. sudo docker run --rm -it --cap-add $CAP alpine sh\rTo drop all capabilities and then explicitly add individual capabilities to the root account of a container. sudo docker run --rm -it --cap-drop ALL --cap-add $CAP alpine sh\rTesting Docker capabilities docker container run --rm -it alpine chown nobody / docker container run --rm -it --cap-drop ALL --cap-add CHOWN alpine chown nobody / docker container run --rm -it --cap-drop CHOWN alpine chown nobody / docker container run --rm -it --cap-add chown -u nobody alpine chown nobody / ocker run --rm -it --cap-drop=NET_RAW alpine sh ping 127.0.0.1 -c 2\rcheck all capabilities capsh --print\rdocker run --rm -it --privileged=true 71aa5f3f90dc bash capsh --print\rAdd Capabiliies for a pod apiVersion: v1 kind: Pod metadata: name: security-context-demo-4 spec: containers: - name: sec-ctx-4 image: gcr.io/google-samples/node-hello:1.0 securityContext: capabilities: add: [\"NET_ADMIN\", \"SYS_TIME\"]\rSelinux securityContext: seLinuxOptions: level: \"s0:c123,c456\"\rTopics Docker Course TOC Day 1: Introduction to Containerization Containerization History of Containers Namespaces and Cgroups Containers vs Virtual Machines Types of Containers Introduction to Docker Docker Architecture Container Lifecycle Docker CE vs Docker EE Day 2: The Docker Engine Docker Engine Configuring Logging Drivers Docker Terminology Port Binding Detached vs Foreground Mode Docker CLI Docker Exec Restart Policy Hands-On Setting up Docker Engine Upgrading Docker Engine Setting up logging drivers in Docker Port Binding Starting Containers in different modes Docker CLI Commands Docker Exec Commands Restart Policy in Docker Removing Containers Day 3: Image Management and Registry Dockerfile Dockerfile Instructions Build Context Docker Image Docker Registry Write a Dockerfile to create an Image Docker Image Tags Setting up Docker Hub Configuring Local Registry Removing Images from the Registry Day 4: Storage in Docker Docker Storage Types of Persistent Storage Volumes Bind Mounts tmpfs Mount Storage Drivers Device Mapper Docker Clean Up Hands-On Deploy Docker Volumes Deploy Bind Mounts Use tmpfs mounts Configure Device Mapper Docker Clean Up Day 5: Orchestration in Docker Docker Compose Docker Swarm Docker Service Service Placement Rolling Update and Rollback Docker Stack Hands-On Deploy a Multi-container Application using Compose Running Docker in Swarm mode Deploying a Service in Swarm Scale Services Service Placement Rolling Updates and Rollbacks Docker Stack Day 6: Networking and Security Docker Networking Network Drivers Bridge Network Overlay Network Host and Macvlan Docker Security Docker Content Trust Securing the Docker Daemon Hands-On Create and use a User-defined Bridge Network Create and use a Overlay Network Use Host and Macvlan Network Configure Docker to use External DNS Signing images using DCT Securing the Docker Daemon Day 7: Docker EE and Monitoring Docker Enterprise Universal Control Plane (UCP) UCP Architecture Access Control in UCP Docker Trusted Registry (DTR) Monitoring using Prometheus Set up Docker Enterprise Edition",
    "description": "Topics Create a virtual machine git clone https://gitlab.com/container-and-kubernetes/kubernetes-2024.git cd kubernetes-2024/ vagrant up\rYou will have a vm ready within 5 mins of time Login to Linux vm ssh vagrant@172.16.16.105 Enter password: vagrant\rSwitch from vagrant user of root user sudo -s\rDocker Basics Lab How to install docker # Add Docker's official GPG key: sudo apt-get update sudo apt-get install ca-certificates curl sudo install -m 0755 -d /etc/apt/keyrings sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc sudo chmod a+r /etc/apt/keyrings/docker.asc # Add the repository to Apt sources: echo \\ \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \\ $(. /etc/os-release \u0026\u0026 echo \"$VERSION_CODENAME\") stable\" | \\ sudo tee /etc/apt/sources.list.d/docker.list \u003e /dev/null sudo apt-get update\rRun the below command",
    "tags": [],
    "title": "Container Basics",
    "uri": "/containerbasics/index.html"
  },
  {
    "breadcrumb": "/",
    "content": "Topics What is Helm ? What is Helm chart ? What is Helm release ? What is Helm repo ? What is Helm ? Helm is a packager for Kubernetes that bundles related manifest files and packages them into a single logical deployment unit: Chart. Simplified, for many engineers Helm makes it easy to start using Kubernetes with real applications.\nWhat is Helm Chart ? A Helm chart is a package that contains all the necessary resources to deploy an application to a Kubernetes cluster. This includes YAML configuration files for deployments, services, secrets, and config maps that define the desired state of your application.\nWhat is Helm release ? A Release is an instance of a chart running in a Kubernetes cluster. One chart can often be installed many times into the same cluster.\nWhat is Helm repo ? At a high level, a chart repository is where packaged charts can be stored and shared.\nHelm Installation Download and Installation of Helm from the link\nInstall using script\ncurl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 chmod 700 get_helm.sh ./get_helm.sh\rCheck helm version helm version\rClick here to check the helm version and k8s version compatiblity Prerequisites The following prerequisites are required for a successful and properly secured use of Helm. A Kubernetes cluster Installed and configured Helm. Initialize a Helm Chart Repository helm repo add bitnami https://charts.bitnami.com/bitnami\rCheck newly added repo. helm repo ls How to search package in repo helm search repo bitnami\rUpdate helm repo helm repo update Install an Example Chart helm install bitnami/mysql --generate-name\rCheck all the release installed helm list\rUninstall a Release helm uninstall \u003cmysql-1612624192\u003e\rCheck the history if the flag –keep-history is provided helm status mysql-1612624192\rCustomizing the Chart Before Installing helm show values bitnami/wordpress\rmake the changes in values.yaml file and use below command to install helm install -f values.yaml bitnami/wordpress --generate-name\rHow to get the value file helm get values happy-panda\rHow to rollback to previous version helm rollback happy-panda 1\rHow to check packages in all projects helm list --all\rCheck all repositories helm repo list\rCreating Your Own Charts Use below command helm create deis-workflow\rValidate your helm chart helm lint .\rHow to package your helm chart helm package deis-workflow\rHow to install charts helm install deis-workflow ./deis-workflow-0.1.0.tgz\rHow to download the charts to your local vm helm pull [chart]\rDisplaya list of a chart’s dependencies helm dependency list [chart]",
    "description": "Topics What is Helm ? What is Helm chart ? What is Helm release ? What is Helm repo ? What is Helm ? Helm is a packager for Kubernetes that bundles related manifest files and packages them into a single logical deployment unit: Chart. Simplified, for many engineers Helm makes it easy to start using Kubernetes with real applications.\nWhat is Helm Chart ? A Helm chart is a package that contains all the necessary resources to deploy an application to a Kubernetes cluster. This includes YAML configuration files for deployments, services, secrets, and config maps that define the desired state of your application.",
    "tags": [],
    "title": "Helm Overview",
    "uri": "/helm-overview/index.html"
  },
  {
    "breadcrumb": "/",
    "content": "Topics How to increase disk size if the vm is created with vagrant First set the environment variable C:\\Program Files\\Oracle\\VirtualBox this is default path for VBoxManage command Run the below command VBoxManage modifymedium disk \"E:\\VirtualBox VMs\\Prometheus_default_1725120844484_62520\\packer-rocky-84-1624559295-disk001.vmdk\" --resize 100000\rVerify the size VBoxManage showmediuminfo \"E:\\VirtualBox VMs\\Prometheus_default_1725120844484_62520\\packer-rocky-84-1624559295-disk001.vmdk\"",
    "description": "Topics How to increase disk size if the vm is created with vagrant First set the environment variable C:\\Program Files\\Oracle\\VirtualBox this is default path for VBoxManage command Run the below command VBoxManage modifymedium disk \"E:\\VirtualBox VMs\\Prometheus_default_1725120844484_62520\\packer-rocky-84-1624559295-disk001.vmdk\" --resize 100000\rVerify the size VBoxManage showmediuminfo \"E:\\VirtualBox VMs\\Prometheus_default_1725120844484_62520\\packer-rocky-84-1624559295-disk001.vmdk\"",
    "tags": [],
    "title": "Imp Commands ",
    "uri": "/imp/index.html"
  },
  {
    "breadcrumb": "/",
    "content": "Topics Set up a Highly Available Kubernetes Cluster using kubeadm and external etcd Follow this documentation to set up a highly available Kubernetes cluster using Ubuntu 24.04 LTS.\nQuorum For N nodes in a cluster Quorum = floor(N/2 + 1)\rVagrant Environment Role FQDN IP OS RAM CPU Master etcd1.example.com 172.16.16.221 Ubuntu 24.04 2G 1 Master etcd2.example.com 172.16.16.222 Ubuntu 24.04 2G 1 Worker etcd3.example.com 172.16.16.223 Ubuntu 24.04 2G 1 Password for the root account on all these virtual machines is kubeadmin Perform all the commands as root user unless otherwise specified Pre-requisites If you want to try this in a virtualized environment on your workstation\nVirtualbox installed Vagrant installed Host machine has atleast 8 cores Host machine has atleast 8G memory Bring up all the virtual machines vagrant up\rDowmload and Insall etcd on all nodes Login to all nodes ssh vagrant@172.16.16.22{1..3}\rMake yourself root sudo -s\rUpdate OS apt update \u0026\u0026 apt install -y haproxy\rRun the below command On all etcd nodes\nETCD_VER=v3.5.1 wget -q --show-progress \"https://github.com/etcd-io/etcd/releases/download/${ETCD_VER}/etcd-${ETCD_VER}-linux-amd64.tar.gz\" tar zxf etcd-v3.5.1-linux-amd64.tar.gz mv etcd-v3.5.1-linux-amd64/etcd* /usr/local/bin/ rm -rf etcd*\rCreate systemd unit file for etcd service NODE_IP=$(ip addr show eth1 | grep inet | awk '{print $2}' | cut -d'/' -f1 |head -1) ETCD_NAME=$(hostname -s) ETCD1_IP=\"172.16.16.221\" ETCD2_IP=\"172.16.16.222\" ETCD3_IP=\"172.16.16.223\" cat \u003c\u003cEOF \u003e/etc/systemd/system/etcd.service [Unit] Description=etcd [Service] Type=exec ExecStart=/usr/local/bin/etcd \\\\ --name ${ETCD_NAME} \\\\ --initial-advertise-peer-urls http://${NODE_IP}:2380 \\\\ --listen-peer-urls http://${NODE_IP}:2380 \\\\ --advertise-client-urls http://${NODE_IP}:2379 \\\\ --listen-client-urls http://${NODE_IP}:2379,http://127.0.0.1:2379 \\\\ --initial-cluster-token etcd-cluster-1 \\\\ --initial-cluster etcd1=http://${ETCD1_IP}:2380,etcd2=http://${ETCD2_IP}:2380,etcd3=http://${ETCD3_IP}:2380 \\\\ --initial-cluster-state new Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target EOF\rEnable and Start etcd service systemctl daemon-reload systemctl enable --now etcd\rVerify Etcd cluster status ETCDCTL_API=3 etcdctl --endpoints=http://127.0.0.1:2379 member list\rETCD with tls On your local workstation (Linux) Generate TLS certificates Download required binaries wget -q --show-progress --https-only --timestamping \\ https://github.com/cloudflare/cfssl/releases/download/v1.6.3/cfssl_1.6.3_linux_amd64 \\ https://github.com/cloudflare/cfssl/releases/download/v1.6.3/cfssljson_1.6.3_linux_amd64 chmod +x cfssl_1.6.3_linux_amd64 cfssljson_1.6.3_linux_amd64 sudo mv cfssl_1.6.3_linux_amd64 /usr/local/bin/cfssl sudo mv cfssljson_1.6.3_linux_amd64 /usr/local/bin/cfssljson\rCreate a Certificate Authority (CA) We then use this CA to create other TLS certificates cat \u003e ca-config.json \u003c\u003cEOF { \"signing\": { \"default\": { \"expiry\": \"8760h\" }, \"profiles\": { \"etcd\": { \"expiry\": \"8760h\", \"usages\": [\"signing\",\"key encipherment\",\"server auth\",\"client auth\"] } } } } EOF cat \u003e ca-csr.json \u003c\u003cEOF { \"CN\": \"etcd cluster\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"GB\", \"L\": \"England\", \"O\": \"Kubernetes\", \"OU\": \"ETCD-CA\", \"ST\": \"Cambridge\" } ] } EOF cfssl gencert -initca ca-csr.json | cfssljson -bare ca\rCreate TLS certificates ETCD1_IP=\"172.16.16.221\" ETCD2_IP=\"172.16.16.222\" ETCD3_IP=\"172.16.16.223\" cat \u003e etcd-csr.json \u003c\u003cEOF { \"CN\": \"etcd\", \"hosts\": [ \"localhost\", \"127.0.0.1\", \"${ETCD1_IP}\", \"${ETCD2_IP}\", \"${ETCD3_IP}\" ], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"GB\", \"L\": \"England\", \"O\": \"Kubernetes\", \"OU\": \"etcd\", \"ST\": \"Cambridge\" } ] } EOF cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=etcd etcd-csr.json | cfssljson -bare etcd\rCopy the certificates to etcd nodes declare -a NODES=(172.16.16.221 172.16.16.222 172.16.16.223) for node in ${NODES[@]}; do scp ca.pem etcd.pem etcd-key.pem root@$node: done\rCopy the certificates to a standard location on all etcd nodes mkdir -p /etc/etcd/pki mv ca.pem etcd.pem etcd-key.pem /etc/etcd/pki/\rCreate systemd unit file for etcd service Set NODE_IP to the correct IP of the machine where you are running this\nNODE_IP=$(ip addr show eth1 | grep inet | awk '{print $2}' | cut -d'/' -f1 |head -1) ETCD_NAME=$(hostname -s) ETCD1_IP=\"172.16.16.221\" ETCD2_IP=\"172.16.16.222\" ETCD3_IP=\"172.16.16.223\" cat \u003c\u003cEOF \u003e/etc/systemd/system/etcd.service [Unit] Description=etcd [Service] Type=notify ExecStart=/usr/local/bin/etcd \\\\ --name ${ETCD_NAME} \\\\ --cert-file=/etc/etcd/pki/etcd.pem \\\\ --key-file=/etc/etcd/pki/etcd-key.pem \\\\ --peer-cert-file=/etc/etcd/pki/etcd.pem \\\\ --peer-key-file=/etc/etcd/pki/etcd-key.pem \\\\ --trusted-ca-file=/etc/etcd/pki/ca.pem \\\\ --peer-trusted-ca-file=/etc/etcd/pki/ca.pem \\\\ --peer-client-cert-auth \\\\ --client-cert-auth \\\\ --initial-advertise-peer-urls https://${NODE_IP}:2380 \\\\ --listen-peer-urls https://${NODE_IP}:2380 \\\\ --advertise-client-urls https://${NODE_IP}:2379 \\\\ --listen-client-urls https://${NODE_IP}:2379,https://127.0.0.1:2379 \\\\ --initial-cluster-token etcd-cluster-1 \\\\ --initial-cluster etcd1=https://${ETCD1_IP}:2380,etcd2=https://${ETCD2_IP}:2380,etcd3=https://${ETCD3_IP}:2380 \\\\ --initial-cluster-state new Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target EOF\rEnable and Start etcd service systemctl daemon-reload systemctl enable --now etcd\rVerify Etcd cluster status In any one of the etcd nodes\nETCDCTL_API=3 etcdctl \\ --endpoints=https://127.0.0.1:2379 \\ --cacert=/etc/etcd/pki/ca.pem \\ --cert=/etc/etcd/pki/etcd.pem \\ --key=/etc/etcd/pki/etcd-key.pem \\ member list\rYou can alos user below export ETCDCTL_API=3 export ETCDCTL_ENDPOINTS=https://172.16.16.221:2379,https://172.16.16.222:2379,https://172.16.16.223:2379 export ETCDCTL_CACERT=/etc/etcd/pki/ca.pem export ETCDCTL_CERT=/etc/etcd/pki/etcd.pem export ETCDCTL_KEY=/etc/etcd/pki/etcd-key.pem\rAnd now its a lot easier\netcdctl member list etcdctl endpoint status etcdctl endpoint health\rProvision Kubernetes cluster with kubeadm using external Etcd cluster Copy TLS certificates to kubernetes master node scp /etc/etcd/pki/ca.pem /etc/etcd/pki/etcd.pem /etc/etcd/pki/etcd-key.pem root@172.16.16.100:\rCopy etcd certificates to a standard location mkdir -p /etc/kubernetes/pki/etcd mv ca.pem etcd.pem etcd-key.pem /etc/kubernetes/pki/etcd/\rCreate Cluster Configuration ETCD1_IP=\"172.16.16.221\" ETCD2_IP=\"172.16.16.222\" ETCD3_IP=\"172.16.16.223\" cat \u003c\u003cEOF \u003e kubeadm-config.yaml apiVersion: kubeadm.k8s.io/v1beta3 kind: ClusterConfiguration networking: podSubnet: \"192.168.0.0/16\" etcd: external: endpoints: - https://172.16.16.221:2379 - https://172.16.16.222:2379 - https://172.16.16.223:2379 caFile: /etc/kubernetes/pki/etcd/ca.pem certFile: /etc/kubernetes/pki/etcd/etcd.pem keyFile: /etc/kubernetes/pki/etcd/etcd-key.pem --- apiVersion: kubeadm.k8s.io/v1beta3 kind: InitConfiguration localAPIEndpoint: advertiseAddress: \"172.16.16.100\" EOF\rInitialize Kubernetes Cluster kubeadm init --config kubeadm-config.yaml --ignore-preflight-errors=all\rDeploy Calico network kubectl --kubeconfig=/etc/kubernetes/admin.conf create -f https://docs.projectcalico.org/v3.14/manifests/calico.yaml\rCluster join command kubeadm token create --print-join-command\rUse default admin kubernetes file mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config\rHow to write and get values to and from etcd etcdctl --endpoints=127.0.0.1:2379 put foo \"Hello World!\" etcdctl --endpoints=127.0.0.1:2379 get foo",
    "description": "Topics Set up a Highly Available Kubernetes Cluster using kubeadm and external etcd Follow this documentation to set up a highly available Kubernetes cluster using Ubuntu 24.04 LTS.\nQuorum For N nodes in a cluster Quorum = floor(N/2 + 1)\rVagrant Environment Role FQDN IP OS RAM CPU Master etcd1.example.com 172.16.16.221 Ubuntu 24.04 2G 1 Master etcd2.example.com 172.16.16.222 Ubuntu 24.04 2G 1 Worker etcd3.example.com 172.16.16.223 Ubuntu 24.04 2G 1 Password for the root account on all these virtual machines is kubeadmin Perform all the commands as root user unless otherwise specified Pre-requisites If you want to try this in a virtualized environment on your workstation",
    "tags": [],
    "title": "kubeadm-external-etcd",
    "uri": "/external-etcd/index.html"
  },
  {
    "breadcrumb": "/",
    "content": "RBAC Kubernetes RBAC is a powerful security feature that allows administrators to control who can access the Kubernetes API and what actions they can perform. You can use it to implement the principle of “least privilege,” which means that users should have the minimum levels of access necessary to perform their tasks. This approach minimizes the potential for accidental or malicious misuse of the Kubernetes system.\nRole-based access control (RBAC) is a method of regulating access to computer or network resources based on the roles of individual users within your organization.\nCore Components of Kubernetes RBAC Role Roles define a set of permissions within a specific namespace. They define which actions (verbs) can be performed on which resources (nouns) within that namespace. A role, for example, might enable a user to list and retrieve pods in a specific namespace. ClusterRole, by contrast, is a non-namespaced resource. The resources have different names (Role and ClusterRole) because a Kubernetes object always has to be either namespaced or not namespaced; it can’t be both. ClusterRoles It has several uses. You can use a ClusterRole to define permissions on namespaced resources and be granted access within individual namespace(s) define permissions on namespaced resources and be granted access across all namespaces define permissions on cluster-scoped resources RoleBinding and ClusterRoleBinding A role binding grants the permissions defined in a role to a user or set of users. It holds a list of subjects (users, groups, or service accounts). A RoleBinding grants permissions within a specific namespace whereas a ClusterRoleBinding grants that access cluster-wide. Documentation and Websites Referred Service Account A service account provides an identity for processes that run in a Pod, and maps to a ServiceAccount object. Token associated to a SA is stored at /var/run/secrets/kubernetes.io/serviceaccount/token inside a pod We can configure a SA not to mount a secret in pod using kubectl apply -f - \u003c\u003cEOF apiVersion: v1 kind: ServiceAccount metadata: name: build-robot automountServiceAccountToken: false EOF\rkubectl apply -f - \u003c\u003cEOF apiVersion: v1 kind: Pod metadata: name: my-pod spec: serviceAccountName: build-robot automountServiceAccountToken: false EOF Step 1: Create a new Service Account: kubectl create serviceaccount external\rStep 2: Create a new POD with External Service Account kubectl run external-pod --image=nginx --dry-run=client -o yaml\rAdd following contents under .spec\nserviceAccountName: external\rFinal File will look like this:\nkubectl apply -f - \u003c\u003cEOF apiVersion: v1 kind: Pod metadata: creationTimestamp: null labels: run: external-pod name: external-pod spec: serviceAccountName: external containers: - image: nginx name: external-pod resources: {} dnsPolicy: ClusterFirst restartPolicy: Always status: {} EOF\rkubectl apply -f pod-sa.yaml\rStep 3: Verification kubectl get pods kubectl get pod external -o yaml\rStep 4: Fetch the Token of a POD kubectl exec -it external-pod -- bash cat /var/run/secrets/kubernetes.io/serviceaccount/token\rStep 5: Make a Request to Kubernetes using Token curl -k \u003cK8S-SERVER-URL\u003e curl -k \u003cK8S-SERVER-URL\u003e/api/v1 --header \"Authorization: Bearer \u003cTOKEN-HERE\u003e\"\rCreate user with certificate Step 1: Create a new private key and CSR mkdir /root/certificates cd /root/certificates\ropenssl genrsa -out john.key 2048 openssl req -new -key john.key -out john.csr -subj \"/CN=john/O=finance\"\rStep 2: Decode the csr cat john.csr | base64 | tr -d '\\n'\rStep 3: Generate the Kubernetes Signing Request nano csr-requests.yaml\rkubectl apply -f - \u003c\u003cEOF apiVersion: certificates.k8s.io/v1 kind: CertificateSigningRequest metadata: name: john-csr spec: groups: - system:authenticated request: ADD-YOUR-CSR-HERE signerName: kubernetes.io/kube-apiserver-client usages: - digital signature - key encipherment - client auth EOF\rStep 4: Apply the Signing Requests: kubectl apply -f csr-requests.yaml\rYou can optionally verify with kubectl get csr\nStep 5: Approve the csr kubectl certificate approve john-csr\rStep 6: Download the Certificate from csr kubectl get csr john-csr -o jsonpath='{.status.certificate}' | base64 -d \u003e john.crt\rStep 7: Create a new context kubectl config set-credentials john --client-certificate=john.crt --client-key=john.key\rStep 8: Set new Context kubectl config set-context john-context --cluster [YOUR-CLUSTER-HERE] --user=john\rStep 9: Use Context to Verify kubectl --context=john-context get pods\rStep 10: Create RBAC Role Allowing List PODS Operation kubectl apply -f - \u003c\u003cEOF apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: default name: pod-reader rules: - apiGroups: [\"\"] resources: [\"pods\"] verbs: [\"list\"] EOF\rStep 11: Create a New Role Binding kubectl apply -f - \u003c\u003cEOF apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: read-pods namespace: default subjects: - kind: User name: john apiGroup: rbac.authorization.k8s.io roleRef: kind: Role name: pod-reader apiGroup: rbac.authorization.k8s.io EOF\rStep 12: Verify Permissions kubectl --context=john-context get pods\rStep 13: Delete Resources Created in this Lab NameSpace Access Control kubectl apply -f - \u003c\u003cEOF apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: teama name: pod-reader rules: - apiGroups: [\"\"] resources: [\"pods\"] verbs: [\"get\", \"watch\", \"list\", \"create\"] EOF\rteama-rolebinding.yaml kubectl apply -f - \u003c\u003cEOF apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: read-pods namespace: teama subjects: - kind: User name: john apiGroup: rbac.authorization.k8s.io roleRef: kind: Role name: pod-reader apiGroup: rbac.authorization.k8s.io EOF\rClusterRole \u0026 ClusterRoleBinding Step 1: Delete Role and Role Binding Step 2: Create a Cluster Role kubectl apply -f - \u003c\u003cEOF apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: pod-reader rules: - apiGroups: [\"\"] resources: [\"pods\"] verbs: [\"list\"] EOF\rCheck clusterrole kubectl get clusterrole kubectl describe clusterrole pod-reader\rStep 3: Create a Cluster Role Binding for a ServiceAccount named external in default namespace (make sure to create sa in right namespace) kubectl apply -f - \u003c\u003cEOF apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: list-pods-global subjects: - kind: User name: system:serviceaccount:default:external apiGroup: rbac.authorization.k8s.io roleRef: kind: ClusterRole name: pod-reader apiGroup: rbac.authorization.k8s.io EOF\rCheck Clusterrolebindings kubectl get clusterrolebinding kubectl describe clusterrolebinding list-pods-global\rStep 4: Verify Permissions curl -k \u003cK8S-SERVER-URL\u003e/api/v1/namespaces/default/pods --header \"Authorization: Bearer \u003cTOKEN-HERE\u003e\"\rStep 5: Create a new Namespace kubectl create namespace external\rcurl -k \u003cK8S-SERVER-URL\u003e/api/v1/namespaces/external/pods --header \"Authorization: Bearer \u003cTOKEN-HERE\u003e\"\rStep 6: Delete Cluster Role Binding Verify if you receive forbidden error. curl -k \u003cK8S-SERVER-URL\u003e/api/v1/namespaces/external/pods --header \"Authorization: Bearer \u003cTOKEN-HERE\u003e\"\rStep 7: Create Role Binding with ClusterRole kubectl apply -f - \u003c\u003cEOF apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: read-pods namespace: default subjects: - kind: User name: system:serviceaccount:default:external apiGroup: rbac.authorization.k8s.io roleRef: kind: ClusterRole name: pod-reader apiGroup: rbac.authorization.k8s.io EOF\rStep 8: Verification curl -k \u003cK8S-SERVER-URL\u003e/api/v1/namespaces/external/pods --header \"Authorization: Bearer \u003cTOKEN-HERE\u003e\"\rcurl -k \u003cK8S-SERVER-URL\u003e/api/v1/namespaces/default/pods --header \"Authorization: Bearer \u003cTOKEN-HERE\u003e\"\rYou can also assign cluster-Admin role using below kubectl apply -f - \u003c\u003cEOF apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: admin-user roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: admin-user namespace: kube-system EOF\rRole with Imperative commands Create a Role named “pod-reader” that allows users to perform get, watch and list on pods: kubectl create role pod-reader --verb=get --verb=list --verb=watch --resource=pods\rCreate a Role named “pod-reader” with resourceNames specified: kubectl create role pod-reader --verb=get --resource=pods --resource-name=readablepod --resource-name=anotherpod\rCreate a Role named “foo” with apiGroups specified: kubectl create role foo --verb=get,list,watch --resource=replicasets.apps\rCreate a Role named “foo” with subresource permissions: kubectl create role foo --verb=get,list,watch --resource=pods,pods/status\rCreate a Role named “my-component-lease-holder” with permissions to get/update a resource with a specific name: kubectl create role my-component-lease-holder --verb=get,list,watch,update --resource=lease --resource-name=my-component\rCLusterROle Create a ClusterRole named “pod-reader” that allows user to perform get, watch and list on pods: kubectl create clusterrole pod-reader --verb=get,list,watch --resource=pods\rCreate a ClusterRole named “pod-reader” with resourceNames specified: kubectl create clusterrole pod-reader --verb=get --resource=pods --resource-name=readablepod --resource-name=anotherpod\rCreate a ClusterRole named “foo” with apiGroups specified: kubectl create clusterrole foo --verb=get,list,watch --resource=replicasets.apps\rCreate a ClusterRole named “foo” with subresource permissions: kubectl create clusterrole foo --verb=get,list,watch --resource=pods,pods/status\rRolebindings with Imperative commands kubectl create clusterrolebinding myapp-view-binding --clusterrole=view --serviceaccount=acme:myapp\rkubectl create rolebinding my-sa-view \\ --clusterrole=view \\ --serviceaccount=my-namespace:my-sa \\ --namespace=my-namespace\rKubernetes RBAC Setup Authentication Authorization Role Binding The rolebinding.yaml file defines a role binding named read-pods that binds the pod-reader role to the user jack in the default namespace.\nCluster Role The clusterrole.yaml file contains the configuration for creating a cluster role named secret-reader. This cluster role allows the user to perform actions like get, watch, and list on secrets resources.\nRole Binding (Namespace-level) The rolebinding.yaml file defines a role binding named read-secrets that binds the secret-reader cluster role to the user dev in the development namespace.\nCluster Role Binding The clusterrolebinding.yaml file contains the configuration for creating a cluster role binding named read-secrets-global. This cluster role binding binds the secret-reader cluster role to the user riya globally.\nSecurityContext A Kubernetes security context is a set of security settings that are applied at the pod or container level. It provides you with the ability to define privilege and access controls for pods or containers. Example 1 kubectl apply -f - \u003c\u003cEOF apiVersion: v1 kind: Pod metadata: name: pod-context spec: securityContext: runAsUser: 1000 runAsGroup: 3000 containers: - name: sec-ctx-demo image: busybox command: [ \"sh\", \"-c\", \"sleep 1h\" ] EOF\rCheck the user id assigned kubectl exec -it pod-context sh id\rExample 2 kubectl apply -f - \u003c\u003cEOF apiVersion: v1 kind: Pod metadata: name: security-context-demo spec: securityContext: fsGroup: 2000 volumes: - name: sec-ctx-vol emptyDir: {} containers: - name: sec-ctx-demo image: busybox command: [ \"sh\", \"-c\", \"sleep 1h\" ] volumeMounts: - name: sec-ctx-vol mountPath: /data/demo EOF\rCheck the permission of /data/demo kubectl exec -it security-context-demo sh cd /data ls -l",
    "description": "RBAC Kubernetes RBAC is a powerful security feature that allows administrators to control who can access the Kubernetes API and what actions they can perform. You can use it to implement the principle of “least privilege,” which means that users should have the minimum levels of access necessary to perform their tasks. This approach minimizes the potential for accidental or malicious misuse of the Kubernetes system.",
    "tags": [],
    "title": "Kubernetes RBAC",
    "uri": "/kubernetes-rbac/index.html"
  },
  {
    "breadcrumb": "/",
    "content": "Kubernetes Installation on Centos 9 Common Steps for master and worker node sudo tee /etc/sysctl.d/99-k8s-cri.conf \u003e/dev/null \u003c\u003cEOF net.bridge.bridge-nf-call-iptables=1 net.ipv4.ip_forward=1 net.bridge.bridge-nf-call-ip6tables=1 EOF\rupdate the setting without restarting the OS sysctl --system modprobe overlay modprobe br_netfilter echo -e overlay\\\\nbr_netfilter \u003e /etc/modules-load.d/k8s.conf install from EPEL\rInstall iptables legacy dnf --enablerepo=epel -y install iptables-legacy alternatives --config iptables There are 2 programs which provide 'iptables'. Selection Command ----------------------------------------------- *+ 1 /usr/sbin/iptables-nft 2 /usr/sbin/iptables-legacy switch to [iptables-legacy] Enter to keep the current selection[+], or type selection number: 2\rset Swap off setting swapoff -a vi /etc/fstab comment out the Swap line /dev/mapper/cs-swap none swap defaults 0 0\rInstall Cri and other software dnf -y install centos-release-okd-4.14 sed -i -e \"s/enabled=1/enabled=0/g\" /etc/yum.repos.d/CentOS-OKD-4.14.repo dnf --enablerepo=centos-okd-4.14 -y install cri-o systemctl enable --now crio cat \u003c\u003c'EOF' \u003e /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-$basearch enabled=0 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg EOF\rInstall kubelet and cri-tools dnf --enablerepo=kubernetes -y install kubeadm kubelet cri-tools iproute-tc container-selinux\rRestart kubelet systemctl enable kubelet On Master node: Bootstrap Kubernetes Cluster kubeadm init --apiserver-advertise-address=10.224.29.85 --pod-network-cidr=192.168.0.0/16 --cri-socket=unix:///var/run/crio/crio.sock\rRun the below command on master nodes to run kubectl command To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config Alternatively, if you are the root user, you can run: export KUBECONFIG=/etc/kubernetes/admin.conf\rExecute the below command on worker nodes\nOn worker node kubeadm join 10.224.29.85:6443 --token sfkbuw.h6oloizivsdsovbs \\ --discovery-token-ca-cert-hash sha256:664454f0d923630af2ac8e65bf14ff1e63bd1040b6d8aaf5665d016d3a6dbdf8",
    "description": "Kubernetes Installation on Centos 9 Common Steps for master and worker node sudo tee /etc/sysctl.d/99-k8s-cri.conf \u003e/dev/null \u003c\u003cEOF net.bridge.bridge-nf-call-iptables=1 net.ipv4.ip_forward=1 net.bridge.bridge-nf-call-ip6tables=1 EOF\rupdate the setting without restarting the OS sysctl --system modprobe overlay modprobe br_netfilter echo -e overlay\\\\nbr_netfilter \u003e /etc/modules-load.d/k8s.conf install from EPEL\rInstall iptables legacy dnf --enablerepo=epel -y install iptables-legacy alternatives --config iptables There are 2 programs which provide 'iptables'. Selection Command ----------------------------------------------- *+ 1 /usr/sbin/iptables-nft 2 /usr/sbin/iptables-legacy switch to [iptables-legacy] Enter to keep the current selection[+], or type selection number: 2\rset Swap off setting swapoff -a vi /etc/fstab comment out the Swap line /dev/mapper/cs-swap none swap defaults 0 0\rInstall Cri and other software dnf -y install centos-release-okd-4.14 sed -i -e \"s/enabled=1/enabled=0/g\" /etc/yum.repos.d/CentOS-OKD-4.14.repo dnf --enablerepo=centos-okd-4.14 -y install cri-o systemctl enable --now crio cat \u003c\u003c'EOF' \u003e /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-$basearch enabled=0 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg EOF\rInstall kubelet and cri-tools dnf --enablerepo=kubernetes -y install kubeadm kubelet cri-tools iproute-tc container-selinux\rRestart kubelet systemctl enable kubelet On Master node: Bootstrap Kubernetes Cluster kubeadm init --apiserver-advertise-address=10.224.29.85 --pod-network-cidr=192.168.0.0/16 --cri-socket=unix:///var/run/crio/crio.sock\rRun the below command on master nodes to run kubectl command To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config Alternatively, if you are the root user, you can run: export KUBECONFIG=/etc/kubernetes/admin.conf\rExecute the below command on worker nodes",
    "tags": [],
    "title": "Kubernetes-on-Centos",
    "uri": "/kubernetes-on-centos/index.html"
  },
  {
    "breadcrumb": "/",
    "content": "Topics Downloading Release Binaries Configuring CA Installing ETCD Configuring API Server Configuring Controller Manager Configuring Scheduler Validating Cluster Status Worker Node Configuration Configuring Networking API to Kubelet RBAC Configuring DNS Kubelet Preferred Instance Type Downloading Release Binaries Kubernetes Binaries Source Code Client Binaries Server Binaries Node Binaries Container Images Download release Binaries Download Etcd Install wget and unzip yum install wget unzip -y\rDownload Server Binaries on master node mkdir /root/binaries cd /root/binaries wget https://dl.k8s.io/v1.28.7/kubernetes-server-linux-amd64.tar.gz tar -xzvf kubernetes-server-linux-amd64.tar.gz cd /root/binaries/kubernetes/server/bin/\rCheck all master binaries ls -l /root/binaries/kubernetes/server/bin\rDownload etcd in the same folder cd /root/binaries/ wget https://github.com/etcd-io/etcd/releases/download/v3.5.12/etcd-v3.5.12-darwin-amd64.zip unzip etcd-v3.5.12-darwin-amd64.zip cd etcd-v3.5.12-darwin-amd64/\rCheck etcd relatated binaries ls -l /root/binaries/tcd-v3.5.12-darwin-amd64/\rDownload Node Binaries on Worker node mkdir /root/binaries cd /root/binaries wget https://dl.k8s.io/v1.28.7/kubernetes-node-linux-amd64.tar.gz tar -xzvf kubernetes-node-linux-amd64.tar.gz\rCheck all node binaries ls -l /root/binaries/node/bin\rConfiguring kubernetes Certificate Authority Create base directory were all the certificates and keys will be stored mkdir /root/certificates cd /root/certificates\rCreating a private key for Certificate Authority openssl genrsa -out ca.key 2048\rCreating CSR openssl req -new -key ca.key -subj \"/CN=KUBERNETES-CA\" -out ca.csr\rSelf-Sign the CSR openssl x509 -req -in ca.csr -signkey ca.key -CAcreateserial -out ca.crt -days 1000\rSee Contents of Certificate openssl x509 -in ca.crt -text -noout\rRemove CSR rm -f ca.csr\rInstalling ETCD Documentation Referred Pre-Requisite: Set SERVER-IP variable SERVER_IP=172.16.16.110 (change this to your IP) echo $SERVER_IP\rConfigure the Certificates: cd /root/certificates/ openssl genrsa -out etcd.key 2048\rNote: Replace the value associated with IP.1 in the below step. cat \u003e etcd.cnf \u003c\u003cEOF [req] req_extensions = v3_req distinguished_name = req_distinguished_name [req_distinguished_name] [ v3_req ] basicConstraints = CA:FALSE keyUsage = nonRepudiation, digitalSignature, keyEncipherment subjectAltName = @alt_names [alt_names] IP.1 = ${SERVER_IP} IP.2 = 127.0.0.1 EOF\ropenssl req -new -key etcd.key -subj \"/CN=etcd\" -out etcd.csr -config etcd.cnf openssl x509 -req -in etcd.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out etcd.crt -extensions v3_req -extfile etcd.cnf -days 1000\rCopy the Certificates and Key to /etc/etcd mkdir /etc/etcd cp etcd.crt etcd.key ca.crt /etc/etcd\rCopy the ETCD and ETCDCTL Binaries to the Path cd /root/binaries/etcd-v3.5.12-darwin-amd64/ cp etcd etcdctl /usr/local/bin/\rConfigure the systemd File Create a service file: cat \u003c\u003cEOF | sudo tee /etc/systemd/system/etcd.service [Unit] Description=etcd Documentation=https://github.com/coreos [Service] ExecStart=/usr/local/bin/etcd \\\\ --name master-1 \\\\ --cert-file=/etc/etcd/etcd.crt \\\\ --key-file=/etc/etcd/etcd.key \\\\ --peer-cert-file=/etc/etcd/etcd.crt \\\\ --peer-key-file=/etc/etcd/etcd.key \\\\ --trusted-ca-file=/etc/etcd/ca.crt \\\\ --peer-trusted-ca-file=/etc/etcd/ca.crt \\\\ --peer-client-cert-auth \\\\ --client-cert-auth \\\\ --initial-advertise-peer-urls https://${SERVER_IP}:2380 \\\\ --listen-peer-urls https://${SERVER_IP}:2380 \\\\ --listen-client-urls https://${SERVER_IP}:2379,https://127.0.0.1:2379 \\\\ --advertise-client-urls https://${SERVER_IP}:2379 \\\\ --initial-cluster-token etcd-cluster-0 \\\\ --initial-cluster master-1=https://${SERVER_IP}:2380 \\\\ --initial-cluster-state new \\\\ --data-dir=/var/lib/etcd Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target EOF\rStart Service: systemctl start etcd systemctl status etcd systemctl enable etcd\rVerification Commands: When we try with etcdctl –endpoints=https://127.0.0.1:2379 get foo, it gives unknown certificate authority.\nETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 --cacert=/etc/etcd/ca.crt --cert=/etc/etcd/etcd.crt --key=/etc/etcd/etcd.key put test \"Test\"\rETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 --cacert=/etc/etcd/ca.crt --cert=/etc/etcd/etcd.crt --key=/etc/etcd/etcd.key get test\rConfiguring API Server Reference Documentation https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/\nhttps://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/\nPreRequisites Move the kube-apiserver binary to /usr/local/bin directory. cd /root/binaries/kubernetes/server/bin/ cp kube-apiserver /usr/local/bin/\rEnsure that the SERVER_IP variable is still set.\nStep 1. Generate Configuration File for CSR Creation. cd /root/certificates\rcat \u003c\u003cEOF | sudo tee api.conf [req] req_extensions = v3_req distinguished_name = req_distinguished_name [req_distinguished_name] [ v3_req ] basicConstraints = CA:FALSE keyUsage = nonRepudiation, digitalSignature, keyEncipherment subjectAltName = @alt_names [alt_names] DNS.1 = kubernetes DNS.2 = kubernetes.default DNS.3 = kubernetes.default.svc DNS.4 = kubernetes.default.svc.cluster.local IP.1 = 127.0.0.1 IP.2 = ${SERVER_IP} IP.3 = 10.0.2.15 EOF\rGenerate Certificates for API Server openssl genrsa -out kube-api.key 2048 openssl req -new -key kube-api.key -subj \"/CN=kube-apiserver\" -out kube-api.csr -config api.conf openssl x509 -req -in kube-api.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out kube-api.crt -extensions v3_req -extfile api.conf -days 1000\rGenerate Certificate for Service Account: openssl genrsa -out service-account.key 2048 openssl req -new -key service-account.key -subj \"/CN=service-accounts\" -out service-account.csr openssl x509 -req -in service-account.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out service-account.crt -days 100\rCopy the certificate files to /var/lib/kubernetes directory mkdir /var/lib/kubernetes cp etcd.crt etcd.key ca.crt kube-api.key kube-api.crt service-account.crt service-account.key /var/lib/kubernetes ls /var/lib/kubernetes\rCreating Encryption key and Configuration ENCRYPTION_KEY=$(head -c 32 /dev/urandom | base64)\rcat \u003e encryption-at-rest.yaml \u003c\u003cEOF kind: EncryptionConfig apiVersion: v1 resources: - resources: - secrets providers: - aescbc: keys: - name: key1 secret: ${ENCRYPTION_KEY} - identity: {} EOF\rcp encryption-at-rest.yaml /var/lib/kubernetes/encryption-at-rest.yaml\rCreating Systemd service file: Systemd file:\ncat \u003c\u003cEOF | sudo tee /etc/systemd/system/kube-apiserver.service [Unit] Description=Kubernetes API Server Documentation=https://github.com/kubernetes/kubernetes [Service] ExecStart=/usr/local/bin/kube-apiserver \\ --advertise-address=${SERVER_IP} \\ --allow-privileged=true \\ --authorization-mode=Node,RBAC \\ --client-ca-file=/var/lib/kubernetes/ca.crt \\ --enable-admission-plugins=NamespaceLifecycle,NodeRestriction,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota \\ --enable-bootstrap-token-auth=true \\ --etcd-cafile=/var/lib/kubernetes/ca.crt \\ --etcd-certfile=/var/lib/kubernetes/etcd.crt \\ --etcd-keyfile=/var/lib/kubernetes/etcd.key \\ --etcd-servers=https://127.0.0.1:2379 \\ --kubelet-client-certificate=/var/lib/kubernetes/kube-api.crt \\ --kubelet-client-key=/var/lib/kubernetes/kube-api.key \\ --service-account-key-file=/var/lib/kubernetes/service-account.crt \\ --service-cluster-ip-range=10.32.0.0/24 \\ --tls-cert-file=/var/lib/kubernetes/kube-api.crt \\ --tls-private-key-file=/var/lib/kubernetes/kube-api.key \\ --requestheader-client-ca-file=/var/lib/kubernetes/ca.crt \\ --service-node-port-range=30000-32767 \\ --audit-log-maxage=30 \\ --audit-log-maxbackup=3 \\ --audit-log-maxsize=100 \\ --audit-log-path=/var/log/kube-api-audit.log \\ --bind-address=0.0.0.0 \\ --event-ttl=1h \\ --service-account-key-file=/var/lib/kubernetes/service-account.crt \\ --service-account-signing-key-file=/var/lib/kubernetes/service-account.key \\ --service-account-issuer=https://${SERVER_IP}:6443 \\ --encryption-provider-config=/var/lib/kubernetes/encryption-at-rest.yaml \\ --v=2 Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target EOF\rStart the kube-api service: systemctl start kube-apiserver systemctl status kube-apiserver systemctl enable kube-apiserver\rConfiguring Controller Manager Generate Certificates: cd /root/certificates\ropenssl genrsa -out kube-controller-manager.key 2048 openssl req -new -key kube-controller-manager.key -subj \"/CN=system:kube-controller-manager\" -out kube-controller-manager.csr openssl x509 -req -in kube-controller-manager.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out kube-controller-manager.crt -days 1000\rGenerating KubeConfig cp /root/binaries/kubernetes/server/bin/kubectl /usr/local/bin\rkubectl config set-cluster kubernetes-from-scratch \\ --certificate-authority=ca.crt \\ --embed-certs=true \\ --server=https://127.0.0.1:6443 \\ --kubeconfig=kube-controller-manager.kubeconfig\rkubectl config set-cluster kubernetes-from-scratch \\ --certificate-authority=ca.crt \\ --embed-certs=true \\ --server=https://127.0.0.1:6443 \\ --kubeconfig=kube-controller-manager.kubeconfig kubectl config set-credentials system:kube-controller-manager \\ --client-certificate=kube-controller-manager.crt \\ --client-key=kube-controller-manager.key \\ --embed-certs=true \\ --kubeconfig=kube-controller-manager.kubeconfig kubectl config set-context default \\ --cluster=kubernetes-from-scratch \\ --user=system:kube-controller-manager \\ --kubeconfig=kube-controller-manager.kubeconfig\rkubectl config use-context default --kubeconfig=kube-controller-manager.kubeconfig\rCopying the files to kubernetes directory cp kube-controller-manager.crt kube-controller-manager.key kube-controller-manager.kubeconfig ca.key /var/lib/kubernetes/\rConfiguring SystemD service file: cat \u003c\u003cEOF | sudo tee /etc/systemd/system/kube-controller-manager.service [Unit] Description=Kubernetes Controller Manager Documentation=https://github.com/kubernetes/kubernetes [Service] ExecStart=/usr/local/bin/kube-controller-manager \\\\ --bind-address=0.0.0.0 \\\\ --service-cluster-ip-range=10.32.0.0/24 \\\\ --cluster-cidr=10.200.0.0/16 \\\\ --kubeconfig=/var/lib/kubernetes/kube-controller-manager.kubeconfig \\\\ --authentication-kubeconfig=/var/lib/kubernetes/kube-controller-manager.kubeconfig \\\\ --authorization-kubeconfig=/var/lib/kubernetes/kube-controller-manager.kubeconfig \\\\ --leader-elect=true \\\\ --cluster-signing-cert-file=/var/lib/kubernetes/ca.crt \\\\ --cluster-signing-key-file=/var/lib/kubernetes/ca.key \\\\ --root-ca-file=/var/lib/kubernetes/ca.crt \\\\ --service-account-private-key-file=/var/lib/kubernetes/service-account.key \\\\ --use-service-account-credentials=true \\\\ --v=2 Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target EOF\rStart Controller Manager: cp /root/binaries/kubernetes/server/bin/kube-controller-manager /usr/local/bin systemctl start kube-controller-manager systemctl status kube-controller-manager systemctl enable kube-controller-manager\rConfiguring Scheduler Generate Certificates: cd /root/certificates openssl genrsa -out kube-scheduler.key 2048 openssl req -new -key kube-scheduler.key -subj \"/CN=system:kube-scheduler\" -out kube-scheduler.csr openssl x509 -req -in kube-scheduler.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out kube-scheduler.crt -days 1000\rGenerate Kubeconfig file: { kubectl config set-cluster kubernetes-from-scratch \\ --certificate-authority=ca.crt \\ --embed-certs=true \\ --server=https://127.0.0.1:6443 \\ --kubeconfig=kube-scheduler.kubeconfig kubectl config set-credentials system:kube-scheduler \\ --client-certificate=kube-scheduler.crt \\ --client-key=kube-scheduler.key \\ --embed-certs=true \\ --kubeconfig=kube-scheduler.kubeconfig kubectl config set-context default \\ --cluster=kubernetes-from-scratch \\ --user=system:kube-scheduler \\ --kubeconfig=kube-scheduler.kubeconfig kubectl config use-context default --kubeconfig=kube-scheduler.kubeconfig }\rCopy the scheduler kubeconfig cp kube-scheduler.kubeconfig /var/lib/kubernetes/\rConfiguring systemd service: cat \u003c\u003cEOF | sudo tee /etc/systemd/system/kube-scheduler.service [Unit] Description=Kubernetes Scheduler Documentation=https://github.com/kubernetes/kubernetes [Service] ExecStart=/usr/local/bin/kube-scheduler \\\\ --kubeconfig=/var/lib/kubernetes/kube-scheduler.kubeconfig \\\\ --authentication-kubeconfig=/var/lib/kubernetes/kube-scheduler.kubeconfig \\\\ --authorization-kubeconfig=/var/lib/kubernetes/kube-scheduler.kubeconfig \\\\ --bind-address=127.0.0.1 \\\\ --leader-elect=true Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target EOF\rVerification: cp /root/binaries/kubernetes/server/bin/kube-scheduler /usr/local/bin systemctl start kube-scheduler systemctl status kube-scheduler systemctl enable kube-scheduler\rValidating Cluster Status Generate Certificate for Administrator User cd /root/certificates openssl genrsa -out admin.key 2048 openssl req -new -key admin.key -subj \"/CN=admin/O=system:masters\" -out admin.csr openssl x509 -req -in admin.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out admin.crt -days 1000\rCreate KubeConfig file: Note: Replace the IP address from the below snippet in line 5 with your IP address.\n{ kubectl config set-cluster kubernetes-from-scratch \\ --certificate-authority=ca.crt \\ --embed-certs=true \\ --server=https://${SERVER_IP}:6443 \\ --kubeconfig=admin.kubeconfig kubectl config set-credentials admin \\ --client-certificate=admin.crt \\ --client-key=admin.key \\ --embed-certs=true \\ --kubeconfig=admin.kubeconfig kubectl config set-context default \\ --cluster=kubernetes-from-scratch \\ --user=admin \\ --kubeconfig=admin.kubeconfig kubectl config use-context default --kubeconfig=admin.kubeconfig }\rVerify Cluster Status kubectl get componentstatuses --kubeconfig=admin.kubeconfig cp /root/certificates/admin.kubeconfig ~/.kube/config kubectl get componentstatuses\rVerify Kubernetes Objects Creation kubectl create namespace test kubectl get namespace test -o yaml kubectl create secret generic prod-secret --from-literal=username=admin --from-literal=password=password123 kubectl get secret\rWorker Node Configuration Configure Container Runtime. (WORKER NODE) cat \u003c\u003cEOF | sudo tee /etc/modules-load.d/containerd.conf overlay br_netfilter EOF\rmodprobe overlay modprobe br_netfilter\rcat \u003c\u003cEOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward = 1 net.bridge.bridge-nf-call-ip6tables = 1 EOF\rsysctl --system\rsudo dnf config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo dnf install -y containerd.io mkdir -p /etc/containerd containerd config default \u003e /etc/containerd/config.toml\rnano /etc/containerd/config.toml\r–\u003e SystemdCgroup = true\nsystemctl restart containerd\rcat \u003c\u003cEOF | sudo tee /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF\rsudo sysctl --system\rPre-Requisites - 2: (WORKER NODE) dnf install socat conntrack ipset sysctl -w net.ipv4.conf.all.forwarding=1 cd /root/binaries/kubernetes/node/bin/ cp kube-proxy kubectl kubelet /usr/local/bin\rGenerate Kubelet Certificate for Worker Node. (MASTER NODE) Note:\nReplace the IP Address and Hostname field in the below configurations according to your enviornement. Run this in the Kubernetes Master Node cd /root/certificates\rcat \u003e openssl-worker.example.com.cnf \u003c\u003cEOF [req] req_extensions = v3_req distinguished_name = req_distinguished_name [req_distinguished_name] [ v3_req ] basicConstraints = CA:FALSE keyUsage = nonRepudiation, digitalSignature, keyEncipherment subjectAltName = @alt_names [alt_names] DNS.1 = worker.example.com IP.1 = 172.16.16.111 EOF\ropenssl genrsa -out worker.example.com.key 2048\ropenssl req -new -key worker.example.com.key -subj \"/CN=system:node:worker.example.com/O=system:nodes\" -out worker.example.com.csr -config openssl-worker.example.com.cnf openssl x509 -req -in worker.example.com.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out worker.example.com.crt -extensions v3_req -extfile openssl-worker.example.com.cnf -days 1000\rStep 2: Generate kube-proxy certificate: (MASTER NODE) openssl genrsa -out kube-proxy.key 2048 openssl req -new -key kube-proxy.key -subj \"/CN=system:kube-proxy\" -out kube-proxy.csr openssl x509 -req -in kube-proxy.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out kube-proxy.crt -days 1000\rStep 3: Copy Certificates to Worker Node: This can either be manual approach or via SCP. Certificates: kubelet, kube-proxy and CA certificate.\nIn-case you want to automate it, then following configuration can be used. In the demo, we had made used of manual way.\nIn-case, you want to transfer file from master to worker node, then you can make use of the following approach:\n- Worker Node: - Master Node: Copy certificate from master node to worker nodes scp kube-proxy.crt kube-proxy.key worker.example.com.key worker.example.com.crt ca.crt vagrant@172.16.16.111:/tmp\r- Worker Node: mkdir /root/certificates cd /tmp mv kube-proxy.crt kube-proxy.key worker.example.com.crt worker.example.com.key ca.crt /root/certificates\rStep 4: Move Certificates to Specific Location. (WORKER NODE) mkdir /var/lib/kubernetes cd /root/certificates cp ca.crt /var/lib/kubernetes mkdir /var/lib/kubelet mv worker.example.com.crt worker.example.com.key kube-proxy.crt kube-proxy.key /var/lib/kubelet/\rStep 5: Generate Kubelet Configuration YAML File: (WORKER NODE) cat \u003c\u003cEOF | sudo tee /var/lib/kubelet/kubelet-config.yaml kind: KubeletConfiguration apiVersion: kubelet.config.k8s.io/v1beta1 authentication: anonymous: enabled: false webhook: enabled: true x509: clientCAFile: \"/var/lib/kubernetes/ca.crt\" authorization: mode: Webhook clusterDomain: \"cluster.local\" clusterDNS: - \"10.32.0.10\" runtimeRequestTimeout: \"15m\" cgroupDriver: systemd EOF\rStep 6: Generate Systemd service file for kubelet: (WORKER NODE) cat \u003c\u003cEOF | sudo tee /etc/systemd/system/kubelet.service [Unit] Description=Kubernetes Kubelet Documentation=https://github.com/kubernetes/kubernetes After=containerd.service Requires=containerd.service [Service] ExecStart=/usr/local/bin/kubelet \\ --config=/var/lib/kubelet/kubelet-config.yaml \\ --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock \\ --kubeconfig=/var/lib/kubelet/kubeconfig \\ --v=2 Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target EOF\rStep 7: Generate the Kubeconfig file for Kubelet (WORKER NODE) cd /var/lib/kubelet cp /var/lib/kubernetes/ca.crt . SERVER_IP=IP-OF-API-SERVER\r{ kubectl config set-cluster kubernetes-from-scratch \\ --certificate-authority=ca.crt \\ --embed-certs=true \\ --server=https://${SERVER_IP}:6443 \\ --kubeconfig=worker.example.com.kubeconfig kubectl config set-credentials system:node:worker.example.com \\ --client-certificate=worker.example.com.crt \\ --client-key=worker.example.com.key \\ --embed-certs=true \\ --kubeconfig=worker.example.com.kubeconfig kubectl config set-context default \\ --cluster=kubernetes-from-scratch \\ --user=system:node:worker.example.com \\ --kubeconfig=worker.example.com.kubeconfig kubectl config use-context default --kubeconfig=worker.example.com.kubeconfig }\rmv worker.example.com.kubeconfig kubeconfig\rPart 2 - Kube-Proxy Step 1: Copy Kube Proxy Certificate to Directory: (WORKER NODE) mkdir /var/lib/kube-proxy\rStep 2: Generate KubeConfig file: { kubectl config set-cluster kubernetes-from-scratch \\ --certificate-authority=ca.crt \\ --embed-certs=true \\ --server=https://${SERVER_IP}:6443 \\ --kubeconfig=kube-proxy.kubeconfig kubectl config set-credentials system:kube-proxy \\ --client-certificate=kube-proxy.crt \\ --client-key=kube-proxy.key \\ --embed-certs=true \\ --kubeconfig=kube-proxy.kubeconfig kubectl config set-context default \\ --cluster=kubernetes-from-scratch \\ --user=system:kube-proxy \\ --kubeconfig=kube-proxy.kubeconfig kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig }\rmv kube-proxy.kubeconfig /var/lib/kube-proxy/kubeconfig\rStep 3: Generate kube-proxy configuration file: (WORKER NODE) cd /var/lib/kube-proxy\rcat \u003c\u003cEOF | sudo tee /var/lib/kube-proxy/kube-proxy-config.yaml kind: KubeProxyConfiguration apiVersion: kubeproxy.config.k8s.io/v1alpha1 clientConnection: kubeconfig: \"/var/lib/kube-proxy/kubeconfig\" mode: \"iptables\" clusterCIDR: \"10.200.0.0/16\" EOF\rStep 4: Create kube-proxy service file: (WORKER NODE) cat \u003c\u003cEOF | sudo tee /etc/systemd/system/kube-proxy.service [Unit] Description=Kubernetes Kube Proxy Documentation=https://github.com/kubernetes/kubernetes [Service] ExecStart=/usr/local/bin/kube-proxy \\\\ --config=/var/lib/kube-proxy/kube-proxy-config.yaml Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target EOF\rWORKER NODE systemctl start kubelet systemctl start kube-proxy systemctl enable kubelet systemctl enable kube-proxy\rStep 6: (MASTER NODE) kubectl get nodes\rCheck the log of kubelet journalctl -u kubelet.service",
    "description": "Topics Downloading Release Binaries Configuring CA Installing ETCD Configuring API Server Configuring Controller Manager Configuring Scheduler Validating Cluster Status Worker Node Configuration Configuring Networking API to Kubelet RBAC Configuring DNS Kubelet Preferred Instance Type Downloading Release Binaries Kubernetes Binaries Source Code Client Binaries Server Binaries Node Binaries Container Images Download release Binaries Download Etcd Install wget and unzip yum install wget unzip -y\rDownload Server Binaries on master node mkdir /root/binaries cd /root/binaries wget https://dl.k8s.io/v1.28.7/kubernetes-server-linux-amd64.tar.gz tar -xzvf kubernetes-server-linux-amd64.tar.gz cd /root/binaries/kubernetes/server/bin/\rCheck all master binaries ls -l /root/binaries/kubernetes/server/bin\rDownload etcd in the same folder cd /root/binaries/ wget https://github.com/etcd-io/etcd/releases/download/v3.5.12/etcd-v3.5.12-darwin-amd64.zip unzip etcd-v3.5.12-darwin-amd64.zip cd etcd-v3.5.12-darwin-amd64/\rCheck etcd relatated binaries ls -l /root/binaries/tcd-v3.5.12-darwin-amd64/\rDownload Node Binaries on Worker node mkdir /root/binaries cd /root/binaries wget https://dl.k8s.io/v1.28.7/kubernetes-node-linux-amd64.tar.gz tar -xzvf kubernetes-node-linux-amd64.tar.gz\rCheck all node binaries ls -l /root/binaries/node/bin\rConfiguring kubernetes Certificate Authority Create base directory were all the certificates and keys will be stored mkdir /root/certificates cd /root/certificates\rCreating a private key for Certificate Authority openssl genrsa -out ca.key 2048\rCreating CSR openssl req -new -key ca.key -subj \"/CN=KUBERNETES-CA\" -out ca.csr\rSelf-Sign the CSR openssl x509 -req -in ca.csr -signkey ca.key -CAcreateserial -out ca.crt -days 1000\rSee Contents of Certificate openssl x509 -in ca.crt -text -noout\rRemove CSR rm -f ca.csr\rInstalling ETCD Documentation Referred Pre-Requisite: Set SERVER-IP variable SERVER_IP=172.16.16.110 (change this to your IP) echo $SERVER_IP\rConfigure the Certificates: cd /root/certificates/ openssl genrsa -out etcd.key 2048\rNote: Replace the value associated with IP.1 in the below step. cat \u003e etcd.cnf \u003c\u003cEOF [req] req_extensions = v3_req distinguished_name = req_distinguished_name [req_distinguished_name] [ v3_req ] basicConstraints = CA:FALSE keyUsage = nonRepudiation, digitalSignature, keyEncipherment subjectAltName = @alt_names [alt_names] IP.1 = ${SERVER_IP} IP.2 = 127.0.0.1 EOF\ropenssl req -new -key etcd.key -subj \"/CN=etcd\" -out etcd.csr -config etcd.cnf openssl x509 -req -in etcd.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out etcd.crt -extensions v3_req -extfile etcd.cnf -days 1000\rCopy the Certificates and Key to /etc/etcd mkdir /etc/etcd cp etcd.crt etcd.key ca.crt /etc/etcd\rCopy the ETCD and ETCDCTL Binaries to the Path cd /root/binaries/etcd-v3.5.12-darwin-amd64/ cp etcd etcdctl /usr/local/bin/\rConfigure the systemd File Create a service file: cat \u003c\u003cEOF | sudo tee /etc/systemd/system/etcd.service [Unit] Description=etcd Documentation=https://github.com/coreos [Service] ExecStart=/usr/local/bin/etcd \\\\ --name master-1 \\\\ --cert-file=/etc/etcd/etcd.crt \\\\ --key-file=/etc/etcd/etcd.key \\\\ --peer-cert-file=/etc/etcd/etcd.crt \\\\ --peer-key-file=/etc/etcd/etcd.key \\\\ --trusted-ca-file=/etc/etcd/ca.crt \\\\ --peer-trusted-ca-file=/etc/etcd/ca.crt \\\\ --peer-client-cert-auth \\\\ --client-cert-auth \\\\ --initial-advertise-peer-urls https://${SERVER_IP}:2380 \\\\ --listen-peer-urls https://${SERVER_IP}:2380 \\\\ --listen-client-urls https://${SERVER_IP}:2379,https://127.0.0.1:2379 \\\\ --advertise-client-urls https://${SERVER_IP}:2379 \\\\ --initial-cluster-token etcd-cluster-0 \\\\ --initial-cluster master-1=https://${SERVER_IP}:2380 \\\\ --initial-cluster-state new \\\\ --data-dir=/var/lib/etcd Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target EOF\rStart Service: systemctl start etcd systemctl status etcd systemctl enable etcd\rVerification Commands: When we try with etcdctl –endpoints=https://127.0.0.1:2379 get foo, it gives unknown certificate authority.",
    "tags": [],
    "title": "Kubernets Installation From Scatch on rocky 9",
    "uri": "/kuberneteshardway/index.html"
  },
  {
    "breadcrumb": "/",
    "content": "Topics Set up a Highly Available Kubernetes Cluster using kubeadm Follow this documentation to set up a highly available Kubernetes cluster using Ubuntu 24.04 LTS.\nThis documentation guides you in setting up a cluster with two master nodes, one worker node and a load balancer node using HAProxy.\nVagrant Environment Role FQDN IP OS RAM CPU Load Balancer loadbalancer.example.com 172.16.16.100 Ubuntu 24.04 1G 1 Master kmaster1.example.com 172.16.16.101 Ubuntu 24.04 2G 2 Master kmaster2.example.com 172.16.16.102 Ubuntu 24.04 2G 2 Worker kworker1.example.com 172.16.16.201 Ubuntu 24.04 1G 1 Password for the root account on all these virtual machines is kubeadmin Perform all the commands as root user unless otherwise specified Pre-requisites If you want to try this in a virtualized environment on your workstation\nVirtualbox installed Vagrant installed Host machine has atleast 8 cores Host machine has atleast 8G memory Bring up all the virtual machines vagrant up\rSet up load balancer node Install Haproxy Login to LoadBalancer node ssh vagrant@172.16.16.100\rMake yourself root sudo -s\rUpdate OS apt update \u0026\u0026 apt install -y haproxy\rConfigure haproxy Append the below lines to /etc/haproxy/haproxy.cfg\nfrontend kubernetes-frontend bind 172.16.16.100:6443 mode tcp option tcplog default_backend kubernetes-backend backend kubernetes-backend mode tcp option tcp-check balance roundrobin server kmaster1 172.16.16.101:6443 check fall 3 rise 2 server kmaster2 172.16.16.102:6443 check fall 3 rise 2\rRestart haproxy service systemctl restart haproxy\rOn all kubernetes nodes (kmaster1, kmaster2, kworker1) Disable Firewall of enabled ufw disable\rDisable swap swapoff -a; sed -i 's/SystemdCgroup \\= false/SystemdCgroup \\= true/g' /etc/containerd/config.toml\rUpdate sysctl settings for Kubernetes networking cat \u003e\u003e/etc/sysctl.d/kubernetes.conf\u003c\u003cEOF net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF sysctl --system\rAdd kernel Parameters cat \u003c\u003cEOF | sudo tee /etc/modules-load.d/k8s.conf overlay br_netfilter EOF sudo modprobe overlay sudo modprobe br_netfilter\rInstall Containerd engine apt update -y apt-get install -y containerd mkdir -p /etc/containerd containerd config default \u003e /etc/containerd/config.toml sed -i 's/SystemdCgroup \\= false/SystemdCgroup \\= true/g' /etc/containerd/config.toml systemctl restart containerd\rKubernetes Setup Add Apt repository sudo apt-get update # apt-transport-https may be a dummy package; if so, you can skip that package sudo apt-get install -y apt-transport-https ca-certificates curl gpg curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.29/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg # This overwrites any existing configuration in /etc/apt/sources.list.d/kubernetes.list echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.29/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list sudo apt-get update sudo apt-get install -y kubelet kubeadm kubectl sudo apt-mark hold kubelet kubeadm kubectl\rInstall Kubernetes components apt update \u0026\u0026 apt install -y kubeadm kubelet kubectl\rOn any one of the Kubernetes master node (Eg: kmaster1) Initialize Kubernetes Cluster kubeadm init --control-plane-endpoint=\"172.16.16.100:6443\" --upload-certs --apiserver-advertise-address=172.16.16.101 --pod-network-cidr=192.168.0.0/16\rCopy the commands to join other master nodes and worker nodes.\nDeploy Calico network kubectl --kubeconfig=/etc/kubernetes/admin.conf create -f https://docs.projectcalico.org/v3.15/manifests/calico.yaml\rJoin other nodes to the cluster (kmaster2 \u0026 kworker1) Use the respective kubeadm join commands you copied from the output of kubeadm init command on the first master.\nIMPORTANT: You also need to pass –apiserver-advertise-address to the join command when you join the other master node.\nExample kubeadm join 172.16.16.100:6443 --apiserver-advertise-address=172.16.16.102 --token uvk94a.j4qzcty1pkj56qy6 \\ --discovery-token-ca-cert-hash sha256:c7f16013787f799fd5db13f1ddbf377a5cb1f30d22b4e34445fe9fc569fe5a0c \\ --control-plane --certificate-key f060afdc1dd0efad58f2f0e97ab881ee68fc6cb85e228b34db78673a911d2c5c\rDownloading kube config to your local machine On your host machine\nmkdir ~/.kube scp root@172.16.16.101:/etc/kubernetes/admin.conf ~/.kube/config\rHere Password for root account is kubeadmin (if you used Vagrant setup)\nVerifying the cluster kubectl cluster-info kubectl get nodes kubectl get cs\rCommands I used to verify cluster check the logs for kubelet sudo journalctl -u kubelet --since \"1 hour ago\" sudo journalctl -u kubelet -b -p err journalctl -u kubelet -f\rCheck all etcd members ETCDCTL_API=3 etcdctl \\ --endpoints=https://127.0.0.1:2379 \\ --cacert=/etc/kubernetes/pki/etcd/ca.crt \\ --cert=/etc/kubernetes/pki/etcd/server.crt \\ --key=/etc/kubernetes/pki/etcd/server.key \\ member list\rCheck the health of etcd cluster ETCDCTL_API=3 etcdctl \\ --endpoints=https://127.0.0.1:2379 \\ --cacert=/etc/kubernetes/pki/etcd/ca.crt \\ --cert=/etc/kubernetes/pki/etcd/server.crt \\ --key=/etc/kubernetes/pki/etcd/server.key \\ endpoint health",
    "description": "Topics Set up a Highly Available Kubernetes Cluster using kubeadm Follow this documentation to set up a highly available Kubernetes cluster using Ubuntu 24.04 LTS.\nThis documentation guides you in setting up a cluster with two master nodes, one worker node and a load balancer node using HAProxy.\nVagrant Environment Role FQDN IP OS RAM CPU Load Balancer loadbalancer.example.com 172.16.16.100 Ubuntu 24.04 1G 1 Master kmaster1.example.com 172.16.16.101 Ubuntu 24.04 2G 2 Master kmaster2.example.com 172.16.16.102 Ubuntu 24.04 2G 2 Worker kworker1.example.com 172.16.16.201 Ubuntu 24.04 1G 1 Password for the root account on all these virtual machines is kubeadmin Perform all the commands as root user unless otherwise specified Pre-requisites If you want to try this in a virtualized environment on your workstation",
    "tags": [],
    "title": "Multi Master kubernetes",
    "uri": "/multimaster/index.html"
  },
  {
    "breadcrumb": "/",
    "content": "Topics MinIO is a high-performance, S3 compatible object store. It is built for large scale AI/ML, data lake and database workloads. It is software-defined and runs on any cloud or on-premises infrastructure. MinIO is dual-licensed under open source GNU AGPL v3 and a commercial enterprise license.\nSimple Simplicity is the foundation for exascale data infrastructure - both technically and operationally. No other object store lets you go from download to production in less time.\nHigh Performance MinIO is the world’s fastest object store with published GETs/PUTs results that exceed 325 GiB/sec and 165 GiB/sec on 32 nodes of NVMe drives and a 100GbE network.\nKubernetes-Native With a native Kubernetes operator integration, MinIO supports all the major Kubernetes distributions on public, private and edge clouds.\nAI Ready MinIO was built for AI and works out of the box with every major AI/ML technology. From predictive models to GenAI, MinIO delivers the performance and scalability to power the AI enterprise.\nLAB Create a virtual machine git clone https://gitlab.com/container-and-kubernetes/kubernetes-2024.git cd kubernetes-2024/ vagrant up\rYou will have a vm ready within 5 mins of time Login to Linux vm ssh vagrant@172.16.16.105 Enter password: vagrant\rSwitch from vagrant user of root user and password is also vagrant sudo -s\rUse the following commands to download the latest stable MinIO RPM and install it. wget https://dl.min.io/server/minio/release/linux-amd64/archive/minio-20240606093642.0.0-1.x86_64.rpm -O minio.rpm sudo dnf install minio.rpm\rUse the following commands to download the latest stable MinIO DEB and install it: wget https://dl.min.io/server/minio/release/linux-amd64/archive/minio_20240606093642.0.0_amd64.deb -O minio.deb sudo dpkg -i minio.deb\rUse the following commands to download the latest stable MinIO binary and install it to the system $PATH: wget https://dl.min.io/server/minio/release/linux-amd64/minio chmod +x minio sudo mv minio /usr/local/bin/\rhttps://min.io/docs/minio/linux/operations/install-deploy-manage/deploy-minio-single-node-single-drive.html#minio-snsd\nhttps://medium.com/yavar/setting-up-minio-object-storage-server-a-step-by-step-guide-for-ubuntu-20-04-5270528273f5",
    "description": "Topics MinIO is a high-performance, S3 compatible object store. It is built for large scale AI/ML, data lake and database workloads. It is software-defined and runs on any cloud or on-premises infrastructure. MinIO is dual-licensed under open source GNU AGPL v3 and a commercial enterprise license.\nSimple Simplicity is the foundation for exascale data infrastructure - both technically and operationally. No other object store lets you go from download to production in less time.",
    "tags": [],
    "title": "The Object Store for AI Data Infrastructure",
    "uri": "/minio/index.html"
  },
  {
    "breadcrumb": "/",
    "content": "Topics Kubernetes Overview Kubernetes Architecture Kubernetes Components Kubernetes Installation Kubernetes SSL Certificates Kubernetes Static Pods Kubernetes Configuration files Kubernetes Upgrade Etcd backup and restore Kubernetes Node related commands CIS Benchmark Kubernetes Overview Kubernetes or k8s is an open-source orchestration and cluster management for container-based applications maintained by the Cloud Native Computing Foundation. Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications. It groups containers that make up an application into logical units for easy management and discovery\nKubernetes Architecture \u0026 Components Managed Kubernetes Cluster (We do not have on control Plane) e.g AKS on Azure Cloud EKS on AWS cloud GKE on Google Cloud Self Managed kubernetes Cluster (Installed on VMs) Kubernetes has two type of nodes Master Nodes (Control Plane) Worker Nodes Control Plane The control plane is responsible for container orchestration and maintaining the desired state of the cluster. It has the following components.\nkube-apiserver etcd kube-scheduler kube-controller-manager cloud-controller-manager Worker Node The Worker nodes are responsible for running containerized applications. The worker Node has the following components.\nkubelet kube-proxy Container runtime Control Plane Components: kube-apiserver The kube-api server is the central hub of the Kubernetes cluster that exposes the Kubernetes API. End users, and other cluster components, talk to the cluster via the API server. So when you use kubectl to manage the cluster, at the backend you are actually communicating with the API server through HTTP REST APIs. The communication between the API server and other components in the cluster happens over TLS to prevent unauthorized access to the cluster. It is the only component that communicates with etcd. etcd etcd is an open-source strongly consistent, distributed key-value store. etcd is designed to run on multiple nodes as a cluster without sacrificing consistency. etcd stores all configurations, states, and metadata of Kubernetes objects (pods, secrets, daemonsets, deployments, configmaps, statefulsets, etc). etcd it is the only Statefulset component in the control plane. Only kube-apiserver can talk to etcd. kube-scheduler: The kube-scheduler is responsible for scheduling Kubernetes pods on worker nodes. It is a controller that listens to pod creation events in the API server. The scheduler has two phases. Scheduling cycle and the Binding cycle. Together it is called the scheduling context. The scheduling cycle selects a worker node and the binding cycle applies that change to the cluster. Kube Controller Manager Controllers are programs that run infinite control loops. Meaning it runs continuously and watches the actual and desired state of objects. Kube controller manager is a component that manages all the Kubernetes controllers. Kubernetes resources/objects like pods, namespaces, jobs, replicaset are managed by respective controllers. the Kube scheduler is also a controller managed by the Kube controller manager. built-in Kubernetes controllers. Deployment controller Replicaset controller DaemonSet controller Job Controller (Kubernetes Jobs) CronJob Controller endpoints controller namespace controller service accounts controller. Node controller Cloud Controller Manager (CCM) When kubernetes is deployed in cloud environments, the cloud controller manager acts as a bridge between Cloud Platform APIs and the Kubernetes cluster.\nCloud controller integration allows Kubernetes cluster to provision cloud resources like instances (for nodes), Load Balancers (for services), and Storage Volumes (for persistent volumes).\n3 main controllers that are part of the cloud controller manager.\nNode controller: This controller updates node-related information by talking to the cloud provider API. For example, node labeling \u0026 annotation, getting hostname, CPU \u0026 memory availability, nodes health, etc.\nRoute controller: It is responsible for configuring networking routes on a cloud platform. So that pods in different nodes can talk to each other.\nService controller: It takes care of deploying load balancers for kubernetes services, assigning IP addresses, etc. Kubernetes Worker Node Components Kubelet Kubelet is an agent component that runs on every node in the cluster. It does not run as a container instead runs as a daemon, managed by systemd. It is responsible for registering worker nodes with the API server. It then brings the podSpec to the desired state by creating containers. Creating, modifying, and deleting containers for the pod. Responsible for handling liveliness, readiness, and startup probes. Responsible for Mounting volumes by reading pod configuration and creating respective directories on the host for the volume mount. Kubelet is also a controller that watches for pod changes and utilizes the node’s container runtime to pull images, run containers, etc. Static pods are controlled by kubelet, not the API servers. Static pods from podSpecs located at /etc/kubernetes/manifests Kube proxy Kube-Proxy is a network proxy that runs on each node in a Kubernetes cluster. It is responsible for maintaining network connectivity between services and pods. Kube-Proxy does this by translating service definitions into actionable networking rules. Kube-proxy uses iptables as a default mode. Container Runtime Container runtime runs on all the nodes in the Kubernetes cluster. It is responsible for pulling images from container registries, running containers, allocating and isolating resources for containers, and managing the entire lifecycle of a container on a host.\nKubernetes supports multiple container runtimes (CRI-O, Docker Engine, containerd, etc) that are compliant with Container Runtime Interface (CRI). This means, all these container runtimes implement the CRI interface and expose gRPC CRI APIs (runtime and image service endpoints). When there is a new request for a pod from the API server, the kubelet talks to CRI-O daemon to launch the required containers via Kubernetes Container Runtime Interface.\nCRI-O checks and pulls the required container image from the configured container registry using containers/image library.\nCRI-O then generates OCI runtime specification (JSON) for a container.\nCRI-O then launches an OCI-compatible runtime (runc) to start the container process as per the runtime specification.\nKubernetes Cluster Addon Components CNI Plugin (Container Network Interface) CoreDNS (For DNS server): CoreDNS acts as a DNS server within the Kubernetes cluster. By enabling this addon, you can enable DNS-based service discovery. Metrics Server (For Resource Metrics): This addon helps you collect performance data and resource usage of Nodes and pods in the cluster. Web UI (Kubernetes Dashboard): This addon enables the Kubernetes dashboard to manage the object via web UI. OCI \u0026 CRI Open Container Initiative (OCI) and Kubernetes Overview of OCI The Open Container Initiative (OCI) is an open governance structure established to create industry standards around container formats and runtimes.\nStart Date: June 2015 Purpose: To standardize container technologies, ensuring compatibility and interoperability among container tools and platforms. OCI Specifications OCI provides two main specifications:\nOCI Runtime Specification\nPurpose: Defines how to run a container, including its configuration and lifecycle. Details: Specifies how the container runtime should execute containers, manage resources, and handle process isolation. OCI Image Specification\nPurpose: Defines how container images should be structured and stored. Details: Specifies the format of container images, including how images are organized and the required metadata. OCI and Kubernetes Container Runtimes in Kubernetes: Kubernetes relies on container runtimes to run and manage containers. These runtimes must be OCI-compliant to ensure compatibility.\nModularity: Kubernetes has moved towards a more modular architecture, supporting any container runtime that adheres to the OCI specifications. This approach allows Kubernetes to integrate with a variety of OCI-compliant runtimes beyond Docker.\nBenefits:\nInteroperability: Ensures different container tools and platforms can work together. Flexibility: Supports a diverse ecosystem of container runtimes. Conclusion The Open Container Initiative plays a vital role in the container ecosystem by defining standards that promote interoperability and compatibility. Kubernetes’ support for OCI-compliant runtimes reflects its commitment to a flexible and standard-driven approach in container orchestration.\nContainer Runtime Management in Kubernetes: Before and After CRI Pre-CRI Era in Kubernetes Docker as the Default Runtime Docker as the Default: For a significant period, Docker was the default and primary container runtime used by Kubernetes. Integration: Kubernetes integrated directly with Docker through its own set of APIs for container lifecycle management, including pulling images, starting, stopping, and removing containers. Monolithic Integration Tightly Coupled: Kubernetes was tightly coupled with Docker’s API, meaning that any changes or updates in Docker would often require corresponding updates in Kubernetes. Single Point of Failure: This tight coupling created a single point of failure and limited flexibility in choosing alternative container runtimes. Limited Container Runtime Options Limited Flexibility: Kubernetes was primarily designed to work with Docker. While other container runtimes existed, Kubernetes did not have a standardized way to interface with them. Custom Solutions: Users who wanted to use different container runtimes had to rely on custom or experimental solutions, often leading to inconsistencies and additional maintenance overhead. Introduction of CRI Container Runtime Interface (CRI) Standardized API: CRI provides a standardized API that allows Kubernetes to communicate with various container runtimes in a consistent way, decoupling Kubernetes from any specific container runtime implementation. Flexibility: By using CRI, Kubernetes can support different container runtimes like Docker, containerd, and CRI-O without having to integrate directly with each one. Benefits of CRI Modularity: CRI allows Kubernetes to interact with any container runtime that implements the CRI API, making it easier to switch between different runtimes or use multiple runtimes in the same cluster. Improved Support: Enables better support for specialized container runtimes that might offer performance optimizations or features not available in Docker. Future-Proofing: Makes it easier to incorporate new technologies and container runtimes as they emerge, without requiring major changes to Kubernetes. How CRI Plugins Work CRI Plugins and Kubernetes Components Kubelet: The kubelet manages containers on each node and communicates with the container runtime through the CRI API to handle container lifecycle operations. Container Runtime: Any container runtime that implements the CRI API can be used, including containerd, CRI-O, and previously Docker (through its CRI-Dockerd component). Migration from Docker CRI-Dockerd: As Docker was phased out as the default runtime, Kubernetes introduced CRI-Dockerd, a bridge that allowed Docker to still be used as a container runtime while interacting through the CRI API. Transition: Users transitioning from Docker had to switch to CRI-compatible runtimes like containerd or CRI-O, and Kubernetes gradually phased out direct Docker support. Summary Before CRI plugins, Kubernetes relied heavily on Docker for container runtime management with a tight integration that limited flexibility. The introduction of CRI standardized container runtime management, allowing Kubernetes to interface with various container runtimes through a consistent API, enhancing modularity, flexibility, and future-proofing.\nVirual Machine setup for Kubernetes Installation. Prerequisites Virtualbox installed on Windows vm. Internet connectivity has to be there. Vscode should be installed Gitbash should also be available on Windows machine Create two VMS using vagrant on virtualbox Create a folder in Windows OS Open folder in vscode Open Terminal in vscode and run the below commands git clone https://gitlab.com/container-and-kubernetes/kubernetes-2024.git cd kubernetes-2024/two-vms vagrant up\rFollow these steps to create a folder, open it in Visual Studio Code, and set up your Kubernetes environment.\nStep 1: Create a Folder Open File Explorer. Navigate to the location where you want to create your project folder. Right-click and select New \u003e Folder. Name the folder kubernetes-setup. Step 2: Open the Folder in Visual Studio Code Open Visual Studio Code. Click on File \u003e Open Folder. Browse to the location of your newly created folder, kubernetes-setup, and select it. Step 3: Open Terminal in Visual Studio Code In Visual Studio Code, open the integrated terminal by pressing Ctrl + (backtick) or navigating to Terminal \u003e New Terminal. Step 4: Clone the Repository and Start Vagrant Run the following commands in the terminal:\ngit clone https://gitlab.com/container-and-kubernetes/kubernetes-2024.git cd kubernetes-2024/two-vms vagrant up\rCheck your virtualbox and see if you have two vms (kmaster \u0026 kworker1) with below ips kmaster: 172.16.16.100 kworker1: 172.16.16.101\rTry to login to each vms using the command below usinf vscode terminal ssh vagrant@172.16.16.100 ssh vagrant@172.16.16.101\rKubernetes Cluster Setup Documentation Documentation Link: https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/\nInstallation of kubeadm ,kubelet etc will be done on master and worker nodes. Setup containerd Prerequisites A compatible Linux host.\n2 GB or more of RAM per machine (any less will leave little room for your apps).\n2 CPUs or more.\nFull network connectivity between all machines in the cluster (public or private network is fine).\nUnique hostname, MAC address, and product_uuid for every node. See here for more details.\nCertain ports are open on your machines. See here for more details.\nSwap configuration. The default behavior of a kubelet was to fail to start if swap memory was detected on a node.\nVerify the MAC address and product_uuid are unique for every node\nip link ip addr show ifconfig -a sudo cat /sys/class/dmi/id/product_uuid\rCheck Required Ports nc 127.0.0.1 6443\rDisable swap sudo swapoff -a sudo sed -i '/\\/swap.img/ s/^\\(.*\\)$/#\\1/' /etc/fstab\rInstalling a container runtime To run containers in Pods, Kubernetes uses a container runtime.\nBy default, Kubernetes uses the Container Runtime Interface (CRI) to interface with your chosen container runtime.\nIf you don’t specify a runtime, kubeadm automatically tries to detect an installed container runtime by scanning through a list of known endpoints.\nIf multiple or no container runtimes are detected kubeadm will throw an error and will request that you specify which one you want to use.\nThe tables below include the known endpoints for supported operating systems:\nRuntime\tPath to Unix domain socket\nContainer Runtime Socket containerd unix:///var/run/containerd/containerd.sock CRI-O unix:///var/run/crio/crio.sock Docker Engine (cri-dockerd) unix:///var/run/cri-dockerd.sock Install containerd apt update -y apt-get install -y containerd mkdir -p /etc/containerd containerd config default \u003e /etc/containerd/config.toml\rUpdate SystemdCgroup Settings sed -i 's/SystemdCgroup \\= false/SystemdCgroup \\= true/g' /etc/containerd/config.toml\rRestart containerd systemctl restart containerd\rKernel Parameter Configuration Forwarding IPv4 and letting iptables see bridged traffic cat \u003c\u003cEOF | sudo tee /etc/modules-load.d/k8s.conf overlay br_netfilter EOF\rEnable setting at runtime sudo modprobe overlay sudo modprobe br_netfilter\rsysctl params required by setup, params persist across reboots cat \u003c\u003cEOF | sudo tee /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-iptables = 1 net.bridge.bridge-nf-call-ip6tables = 1 net.ipv4.ip_forward = 1 EOF\rApply sysctl params without reboot sudo sysctl --system\rInstalling kubeadm, kubelet and kubectl sudo apt-get update # apt-transport-https may be a dummy package; if so, you can skip that package sudo apt-get install -y apt-transport-https ca-certificates curl gpg curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.29/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg # This overwrites any existing configuration in /etc/apt/sources.list.d/kubernetes.list echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.29/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list sudo apt-get update sudo apt-get install -y kubelet kubeadm kubectl sudo apt-mark hold kubelet kubeadm kubectl\rInitialize Cluster with kubeadm (Only master node) kubeadm init --pod-network-cidr=10.244.0.0/16 --apiserver-advertise-address=172.16.16.100\rExecute the below command to run kubectl commands as you will require kubeconfig file to connect to kubernetes cluster mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config\rClick Here to know more about kubeconfig\nCheck nodes status kubectl get nodes\rInstall Network Addon (flannel) (master node) kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml\rConnect Worker Node (Only worker node) kubeadm join 172.16.16.100:6443 --token qmw5dj.ljdh8r74ce3y85ad \\ --discovery-token-ca-cert-hash sha256:83374ec05088fa7efe9c31cce63326ae7037210ab049048ef08f8c961a048ddf\rVerification check node status kubectl get nodes\rrun a pod kubectl run nginx --image=nginx\rcheck running pod kubectl get pods -o wide\rKubernetes Cluster Configurations Static Pod Manifests: /etc/kubernetes/manifests /etc/kubernetes/manifests ├── etcd.yaml ├── kube-apiserver.yaml ├── kube-controller-manager.yaml └── kube-scheduler.yaml\rTLS Certificates All the components talk to each other over mTLS. Under the PKI folder, you will find all the TLS certificates and keys Kubeconfig Files\nAny components that need to authenticate to the API server need the kubeconfig file.\nAll the cluster Kubeconfig files are present in the /etc/kubernetes folder (.conf files). You will find the following files.\nadmin.conf controller-manager.conf kubelet.conf scheduler.conf It contains the API server endpoint, cluster CA certificate, cluster client certificate, and other information.\nThe admin.conf, file, which is the admin kubeconfig file used by end users to access the API server to manage the clusters.\nThe Kubeconfig for the Controller manager controller-manager.conf, scheduler, and Kubelet is used for API server authentication and authorization.\nKubelet Configurations kubelet kubeconfig file: /etc/kubernetes/kubelet.conf kubelet config file: /var/lib/kubelet/config.yaml EnvironmentFile=-/var/lib/kubelet/kubeadm-flags.env\rThe /var/lib/kubelet/config.yaml contains all the kubelet-related configurations. The static pod manifest location is added as part of the staticPodPath parameter. /var/lib/kubelet/kubeadm-flags.env file contains the container runtime environment Linux socket and the infra container (pause container) image. CoreDNS Configurations kubectl get configmap --namespace=kube-system\rLAB 1 Create a virtual machine Login to vm and chnage the user to root Install kubernetes using kubeadm Add a worker node to kubernetes Cluster Node Related commands Check all nodes which are the part of kubernetes cluster kubectl get nodes\rCordon a node (prevent new pods from being scheduled) kubectl cordon \u003cnode-name\u003e\rUncordon a node (allow new pods to be scheduled): kubectl uncordon \u003cnode-name\u003e\rDrain a node (evacuate all pods, reschedule them elsewhere) kubectl drain \u003cnode-name\u003e\rSort Nodes by CPU Capacity kubectl get nodes --sort-by=.status.capacity.cpu\rSort by memory kubectl get nodes --sort-by=.status.capacity.memory\rHow to label a node kubectl label node \u003cnodename\u003e key=value\rcheck nodes with label kubectl get nodes -l key=value\rcheck the nodes which does not label key=value kubectl get nodes key!=value\rCheck detailed information about a node kubectl get nodes -o wide\rUpgrade Kubeadm Cluster Before upgrade kubernetes cluster Do not forgot to study kubernetes release Check the release notes before you upgrade your kubernetes cluster Also drain a node and uncordon after the upgrade is successful Before upgrading Kubernetes to version 1.30, make sure to add the corresponding repository for version 1.30 Run the below command sudo apt-get update\rapt-transport-https may be a dummy package; if so, you can skip that package sudo apt-get install -y apt-transport-https ca-certificates curl gpg\rIf the directory /etc/apt/keyrings does not exist, it should be created before the curl command, read the note below. sudo mkdir -p -m 755 /etc/apt/keyrings curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring01.gpg\rThis overwrites any existing configuration in /etc/apt/sources.list.d/kubernetes.list echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes01.list\rsudo apt-get update\rCheck the upgrade plan using below commands kubeadm upgrade plan kubectl -n kube-system get cm kubeadm-config -o yaml\rCheck the kubeadm version available apt-cache madison kubeadm\rUnhold the packages to be upgraded apt-mark unhold kubelet kubectl kubeadm\rUpgrade the available packages apt-get update \u0026\u0026 apt-get install -y kubelet=1.30.0-1.1 kubectl=1.30.0-1.1 kubeadm=1.30.0-1.1\rLock again the below packages apt-mark hold kubelet kubectl\rCheck new kubeadm version kubeadm version\rUpgrade the kubernetes cluster (Run only on Master node) kubeadm upgrade apply 1.30.0\rRestart the services systemctl daemon-reload systemctl restart kubelet\rCheck the kubernetes version kubectl version\rKubens \u0026 Kubectx Login to master node and run the command there Install Kubens to switch from one namespace to another wget https://raw.githubusercontent.com/ahmetb/kubectx/master/kubens chmod +x kubens cp kubens /bin\rInstall kubectx to switch from one Kubernetes cluster to another wget https://raw.githubusercontent.com/ahmetb/kubectx/master/kubectx chmod +x kubectx cp kubectx /bin\rKubernetes Initial commands for cluster verification How to check nodes in cluster kubectl get nodes\rCluster api info kubectl api-resources\rCluster info kubect cluster-info\rKubernetes nodes information kubectl describe nodes \u003cnodename\u003e\rUpdate worker node role as a worker kubectl label node \u003cnode-name\u003e kubernetes.io/role=worker\rMake a test after creating one pod kubectl run mypod --image=nginx\rCheck newly created pod kubectl get pods\rLAB 2 Perform basic operation on newly created kuberntes cluster Check no of nodes Create a pod check cluster info describe the node Install kubens and kubectx Upgrade kubenetes master and worker nodes Install kubernetes Cluster Automatically git clone https://gitlab.com/container-and-kubernetes/kubernetes-2024.git cd kubernetes-2024/ cd kubernetes-with-vagrant/ vagrant up\rCommon Kubeadm Commands kubeadm init (initialize kubernetes) kubeadm init --pod-network-cidr=192.168.0.0/16\rkubeadm join (To join a worker node) kubeadm join \u003cmaster-node-ip\u003e:\u003cmaster-node-port\u003e --token \u003ctoken\u003e --discovery-token-ca-cert-hash \u003chash\u003e\rkubeadm reset( removing all installed Kubernetes components and returning it to its pre-Kubernetes state. ) kubeadm reset --force\rkubeadm upgrade (check available upgrades) kubeadm upgrade plan\rkubeadm token (Token management) kubeadm token create\rPrint Token along with command kubeadm token create --print-join-command\rkubeadm token list (Check all token along with expiration date) kubeadm token list\rkubeadm token delete kubeadm token delete \u003ctoken_value\u003e\rkubeadm config migrate kubeadm config migrate --old-config kubeadm.conf --new-config kubeadm.yaml\rkubeadm config (Print default config files) kubeadm config print init-defaults\rkubeadm config images(List all required images) kubeadm config images list\rkubeadm certs(Check certificate status) kubeadm certs check-expiration kubeadm certs certificate-key\rkubeadm upgrade node(upgrades the kubelet and kube-proxy on a worker node to match the control plane’s version.) kubeadm upgrade node\rLAB 3 Test kubeadm command How to generate the woker node join command How to reset the kubeadm config How to check certificate expirty date How to delete token Create kubernetes cluster automatically What is a Kubeconfig file? A Kubeconfig is a YAML file with all the Kubernetes cluster details, certificates, and secret tokens to authenticate the cluster. You might get this config file directly from the cluster administrator or from a cloud platform if you are using a managed Kubernetes cluster. When you use kubectl, it uses the information in the kubeconfig file to connect to the kubernetes cluster API. The default location of the Kubeconfig file is $HOME/.kube/config Example Kubeconfig File certificate-authority-data: Cluster CA server: Cluster endpoint (IP/DNS of the master node) name: Cluster name user: name of the user/service account. token: Secret token of the user/service account. apiVersion: v1 clusters: - cluster: certificate-authority-data: \u003cca-data-here\u003e server: https://your-k8s-cluster.com name: \u003ccluster-name\u003e contexts: - context: cluster: \u003ccluster-name\u003e user: \u003ccluster-name-user\u003e name: \u003ccluster-name\u003e current-context: \u003ccluster-name\u003e kind: Config preferences: {} users: - name: \u003ccluster-name-user\u003e user: token: \u003csecret-token-here\u003e\rkubeconfig Related Commands The kubeconfig file is used to configure access to Kubernetes clusters. It contains information about clusters, users, and contexts. Here are some common commands and their usage:\n1. View Current Context To view the currently active context in your kubeconfig file: kubectl config current-context\rList all cluster contexts kubectl config get-contexts -o=name\rSet the current context kubectl config use-context \u003ccluster-name\u003e Using Environment variable (KUBECONFIG) export KUBECONFIG=\u003ckubeconfig filename\u003e\rUsing Kubeconfig File With Kubectl kubectl get nodes --kubeconfig=\u003cfilepath\u003e\rHow to merge multiple kubeconfig into one KUBECONFIG=config:dev_config:test_config kubectl config view --merge --flatten \u003e config.new\rAdd or Update Cluster kubectl config set-cluster \u003ccluster_name\u003e --server=\u003cserver_url\u003e --certificate-authority=\u003cca_file\u003e --embed-certs=true\rAdd or Update User kubectl config set-credentials \u003cuser_name\u003e --client-certificate=\u003ccert_file\u003e --client-key=\u003ckey_file\u003e --embed-certs=true\rAdd or Update Context kubectl config set-context \u003ccontext_name\u003e --cluster=\u003ccluster_name\u003e --user=\u003cuser_name\u003e\rDelete a Context kubectl config delete-context \u003ccontext_name\u003e\rDelete a Cluster kubectl config delete-cluster \u003ccluster_name\u003e\rDelete a User kubectl config delete-credentials \u003cuser_name\u003e\rView Config File Location echo $KUBECONFIG\rCheck kubeconfig file kubectl config view\rLAB 4 Download a kubeconfig file Try to use kubeconfig file with environment variable or passing to command line Check etcd docs Check how we can set up multi master kubernetes cluster.",
    "description": "Topics Kubernetes Overview Kubernetes Architecture Kubernetes Components Kubernetes Installation Kubernetes SSL Certificates Kubernetes Static Pods Kubernetes Configuration files Kubernetes Upgrade Etcd backup and restore Kubernetes Node related commands CIS Benchmark Kubernetes Overview Kubernetes or k8s is an open-source orchestration and cluster management for container-based applications maintained by the Cloud Native Computing Foundation. Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications. It groups containers that make up an application into logical units for easy management and discovery",
    "tags": [],
    "title": "CKS",
    "uri": "/cks/index.html"
  },
  {
    "breadcrumb": "/",
    "content": "",
    "description": "",
    "tags": [],
    "title": "EKS",
    "uri": "/eks/index.html"
  },
  {
    "breadcrumb": "/",
    "content": "",
    "description": "",
    "tags": [],
    "title": "GKE",
    "uri": "/gke/index.html"
  },
  {
    "breadcrumb": "/",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Kubernetes Data Protection",
    "uri": "/kuberntes-backup/index.html"
  },
  {
    "breadcrumb": "/",
    "content": "Topics Kubernetes Overview Kubernetes Architecture Kubernetes Components Kubernetes Installation Kubernetes SSL Certificates Kubernetes Static Pods Kubernetes Configuration files Kubernetes Upgrade Etcd backup and restore Kubernetes Node related commands CIS Benchmark Kubernetes Overview Kubernetes or k8s is an open-source orchestration and cluster management for container-based applications maintained by the Cloud Native Computing Foundation. Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications. It groups containers that make up an application into logical units for easy management and discovery\nKubernetes Architecture \u0026 Components Managed Kubernetes Cluster (We do not have on control Plane) e.g AKS on Azure Cloud EKS on AWS cloud GKE on Google Cloud Self Managed kubernetes Cluster (Installed on VMs) Kubernetes has two type of nodes Master Nodes (Control Plane) Worker Nodes Control Plane The control plane is responsible for container orchestration and maintaining the desired state of the cluster. It has the following components.\nkube-apiserver etcd kube-scheduler kube-controller-manager cloud-controller-manager Worker Node The Worker nodes are responsible for running containerized applications. The worker Node has the following components.\nkubelet kube-proxy Container runtime Control Plane Components: kube-apiserver The kube-api server is the central hub of the Kubernetes cluster that exposes the Kubernetes API. End users, and other cluster components, talk to the cluster via the API server. So when you use kubectl to manage the cluster, at the backend you are actually communicating with the API server through HTTP REST APIs. The communication between the API server and other components in the cluster happens over TLS to prevent unauthorized access to the cluster. It is the only component that communicates with etcd. etcd etcd is an open-source strongly consistent, distributed key-value store. etcd is designed to run on multiple nodes as a cluster without sacrificing consistency. etcd stores all configurations, states, and metadata of Kubernetes objects (pods, secrets, daemonsets, deployments, configmaps, statefulsets, etc). etcd it is the only Statefulset component in the control plane. Only kube-apiserver can talk to etcd. kube-scheduler: The kube-scheduler is responsible for scheduling Kubernetes pods on worker nodes. It is a controller that listens to pod creation events in the API server. The scheduler has two phases. Scheduling cycle and the Binding cycle. Together it is called the scheduling context. The scheduling cycle selects a worker node and the binding cycle applies that change to the cluster. Kube Controller Manager Controllers are programs that run infinite control loops. Meaning it runs continuously and watches the actual and desired state of objects. Kube controller manager is a component that manages all the Kubernetes controllers. Kubernetes resources/objects like pods, namespaces, jobs, replicaset are managed by respective controllers. the Kube scheduler is also a controller managed by the Kube controller manager. built-in Kubernetes controllers. Deployment controller Replicaset controller DaemonSet controller Job Controller (Kubernetes Jobs) CronJob Controller endpoints controller namespace controller service accounts controller. Node controller Cloud Controller Manager (CCM) When kubernetes is deployed in cloud environments, the cloud controller manager acts as a bridge between Cloud Platform APIs and the Kubernetes cluster.\nCloud controller integration allows Kubernetes cluster to provision cloud resources like instances (for nodes), Load Balancers (for services), and Storage Volumes (for persistent volumes).\n3 main controllers that are part of the cloud controller manager.\nNode controller: This controller updates node-related information by talking to the cloud provider API. For example, node labeling \u0026 annotation, getting hostname, CPU \u0026 memory availability, nodes health, etc.\nRoute controller: It is responsible for configuring networking routes on a cloud platform. So that pods in different nodes can talk to each other.\nService controller: It takes care of deploying load balancers for kubernetes services, assigning IP addresses, etc. Kubernetes Worker Node Components Kubelet Kubelet is an agent component that runs on every node in the cluster. It does not run as a container instead runs as a daemon, managed by systemd. It is responsible for registering worker nodes with the API server. It then brings the podSpec to the desired state by creating containers. Creating, modifying, and deleting containers for the pod. Responsible for handling liveliness, readiness, and startup probes. Responsible for Mounting volumes by reading pod configuration and creating respective directories on the host for the volume mount. Kubelet is also a controller that watches for pod changes and utilizes the node’s container runtime to pull images, run containers, etc. Static pods are controlled by kubelet, not the API servers. Static pods from podSpecs located at /etc/kubernetes/manifests Kube proxy Kube-Proxy is a network proxy that runs on each node in a Kubernetes cluster. It is responsible for maintaining network connectivity between services and pods. Kube-Proxy does this by translating service definitions into actionable networking rules. Kube-proxy uses iptables as a default mode. Container Runtime Container runtime runs on all the nodes in the Kubernetes cluster. It is responsible for pulling images from container registries, running containers, allocating and isolating resources for containers, and managing the entire lifecycle of a container on a host.\nKubernetes supports multiple container runtimes (CRI-O, Docker Engine, containerd, etc) that are compliant with Container Runtime Interface (CRI). This means, all these container runtimes implement the CRI interface and expose gRPC CRI APIs (runtime and image service endpoints). When there is a new request for a pod from the API server, the kubelet talks to CRI-O daemon to launch the required containers via Kubernetes Container Runtime Interface.\nCRI-O checks and pulls the required container image from the configured container registry using containers/image library.\nCRI-O then generates OCI runtime specification (JSON) for a container.\nCRI-O then launches an OCI-compatible runtime (runc) to start the container process as per the runtime specification.\nKubernetes Cluster Addon Components CNI Plugin (Container Network Interface) CoreDNS (For DNS server): CoreDNS acts as a DNS server within the Kubernetes cluster. By enabling this addon, you can enable DNS-based service discovery. Metrics Server (For Resource Metrics): This addon helps you collect performance data and resource usage of Nodes and pods in the cluster. Web UI (Kubernetes Dashboard): This addon enables the Kubernetes dashboard to manage the object via web UI. OCI \u0026 CRI Open Container Initiative (OCI) and Kubernetes Overview of OCI The Open Container Initiative (OCI) is an open governance structure established to create industry standards around container formats and runtimes.\nStart Date: June 2015 Purpose: To standardize container technologies, ensuring compatibility and interoperability among container tools and platforms. OCI Specifications OCI provides two main specifications:\nOCI Runtime Specification\nPurpose: Defines how to run a container, including its configuration and lifecycle. Details: Specifies how the container runtime should execute containers, manage resources, and handle process isolation. OCI Image Specification\nPurpose: Defines how container images should be structured and stored. Details: Specifies the format of container images, including how images are organized and the required metadata. OCI and Kubernetes Container Runtimes in Kubernetes: Kubernetes relies on container runtimes to run and manage containers. These runtimes must be OCI-compliant to ensure compatibility.\nModularity: Kubernetes has moved towards a more modular architecture, supporting any container runtime that adheres to the OCI specifications. This approach allows Kubernetes to integrate with a variety of OCI-compliant runtimes beyond Docker.\nBenefits:\nInteroperability: Ensures different container tools and platforms can work together. Flexibility: Supports a diverse ecosystem of container runtimes. Conclusion The Open Container Initiative plays a vital role in the container ecosystem by defining standards that promote interoperability and compatibility. Kubernetes’ support for OCI-compliant runtimes reflects its commitment to a flexible and standard-driven approach in container orchestration.\nContainer Runtime Management in Kubernetes: Before and After CRI Pre-CRI Era in Kubernetes Docker as the Default Runtime Docker as the Default: For a significant period, Docker was the default and primary container runtime used by Kubernetes. Integration: Kubernetes integrated directly with Docker through its own set of APIs for container lifecycle management, including pulling images, starting, stopping, and removing containers. Monolithic Integration Tightly Coupled: Kubernetes was tightly coupled with Docker’s API, meaning that any changes or updates in Docker would often require corresponding updates in Kubernetes. Single Point of Failure: This tight coupling created a single point of failure and limited flexibility in choosing alternative container runtimes. Limited Container Runtime Options Limited Flexibility: Kubernetes was primarily designed to work with Docker. While other container runtimes existed, Kubernetes did not have a standardized way to interface with them. Custom Solutions: Users who wanted to use different container runtimes had to rely on custom or experimental solutions, often leading to inconsistencies and additional maintenance overhead. Introduction of CRI Container Runtime Interface (CRI) Standardized API: CRI provides a standardized API that allows Kubernetes to communicate with various container runtimes in a consistent way, decoupling Kubernetes from any specific container runtime implementation. Flexibility: By using CRI, Kubernetes can support different container runtimes like Docker, containerd, and CRI-O without having to integrate directly with each one. Benefits of CRI Modularity: CRI allows Kubernetes to interact with any container runtime that implements the CRI API, making it easier to switch between different runtimes or use multiple runtimes in the same cluster. Improved Support: Enables better support for specialized container runtimes that might offer performance optimizations or features not available in Docker. Future-Proofing: Makes it easier to incorporate new technologies and container runtimes as they emerge, without requiring major changes to Kubernetes. How CRI Plugins Work CRI Plugins and Kubernetes Components Kubelet: The kubelet manages containers on each node and communicates with the container runtime through the CRI API to handle container lifecycle operations. Container Runtime: Any container runtime that implements the CRI API can be used, including containerd, CRI-O, and previously Docker (through its CRI-Dockerd component). Migration from Docker CRI-Dockerd: As Docker was phased out as the default runtime, Kubernetes introduced CRI-Dockerd, a bridge that allowed Docker to still be used as a container runtime while interacting through the CRI API. Transition: Users transitioning from Docker had to switch to CRI-compatible runtimes like containerd or CRI-O, and Kubernetes gradually phased out direct Docker support. Summary Before CRI plugins, Kubernetes relied heavily on Docker for container runtime management with a tight integration that limited flexibility. The introduction of CRI standardized container runtime management, allowing Kubernetes to interface with various container runtimes through a consistent API, enhancing modularity, flexibility, and future-proofing.\nVirual Machine setup for Kubernetes Installation. Prerequisites Virtualbox installed on Windows vm. Internet connectivity has to be there. Vscode should be installed Gitbash should also be available on Windows machine Create two VMS using vagrant on virtualbox Create a folder in Windows OS Open folder in vscode Open Terminal in vscode and run the below commands git clone https://gitlab.com/container-and-kubernetes/kubernetes-2024.git cd kubernetes-2024/two-vms vagrant up\rFollow these steps to create a folder, open it in Visual Studio Code, and set up your Kubernetes environment.\nStep 1: Create a Folder Open File Explorer. Navigate to the location where you want to create your project folder. Right-click and select New \u003e Folder. Name the folder kubernetes-setup. Step 2: Open the Folder in Visual Studio Code Open Visual Studio Code. Click on File \u003e Open Folder. Browse to the location of your newly created folder, kubernetes-setup, and select it. Step 3: Open Terminal in Visual Studio Code In Visual Studio Code, open the integrated terminal by pressing Ctrl + (backtick) or navigating to Terminal \u003e New Terminal. Step 4: Clone the Repository and Start Vagrant Run the following commands in the terminal:\ngit clone https://gitlab.com/container-and-kubernetes/kubernetes-2024.git cd kubernetes-2024/two-vms vagrant up\rCheck your virtualbox and see if you have two vms (kmaster \u0026 kworker1) with below ips kmaster: 172.16.16.100 kworker1: 172.16.16.101\rTry to login to each vms using the command below usinf vscode terminal ssh vagrant@172.16.16.100 ssh vagrant@172.16.16.101\rKubernetes Cluster Setup Documentation Documentation Link: https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/\nInstallation of kubeadm ,kubelet etc will be done on master and worker nodes. Setup containerd Prerequisites A compatible Linux host.\n2 GB or more of RAM per machine (any less will leave little room for your apps).\n2 CPUs or more.\nFull network connectivity between all machines in the cluster (public or private network is fine).\nUnique hostname, MAC address, and product_uuid for every node. See here for more details.\nCertain ports are open on your machines. See here for more details.\nSwap configuration. The default behavior of a kubelet was to fail to start if swap memory was detected on a node.\nVerify the MAC address and product_uuid are unique for every node\nip link ip addr show ifconfig -a sudo cat /sys/class/dmi/id/product_uuid\rCheck Required Ports nc 127.0.0.1 6443\rDisable swap sudo swapoff -a sudo sed -i '/\\/swap.img/ s/^\\(.*\\)$/#\\1/' /etc/fstab\rInstalling a container runtime To run containers in Pods, Kubernetes uses a container runtime.\nBy default, Kubernetes uses the Container Runtime Interface (CRI) to interface with your chosen container runtime.\nIf you don’t specify a runtime, kubeadm automatically tries to detect an installed container runtime by scanning through a list of known endpoints.\nIf multiple or no container runtimes are detected kubeadm will throw an error and will request that you specify which one you want to use.\nThe tables below include the known endpoints for supported operating systems:\nRuntime\tPath to Unix domain socket\nContainer Runtime Socket containerd unix:///var/run/containerd/containerd.sock CRI-O unix:///var/run/crio/crio.sock Docker Engine (cri-dockerd) unix:///var/run/cri-dockerd.sock Install containerd apt update -y apt-get install -y containerd mkdir -p /etc/containerd containerd config default \u003e /etc/containerd/config.toml\rUpdate SystemdCgroup Settings sed -i 's/SystemdCgroup \\= false/SystemdCgroup \\= true/g' /etc/containerd/config.toml\rRestart containerd systemctl restart containerd\rKernel Parameter Configuration Forwarding IPv4 and letting iptables see bridged traffic cat \u003c\u003cEOF | sudo tee /etc/modules-load.d/k8s.conf overlay br_netfilter EOF\rEnable setting at runtime sudo modprobe overlay sudo modprobe br_netfilter\rsysctl params required by setup, params persist across reboots cat \u003c\u003cEOF | sudo tee /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-iptables = 1 net.bridge.bridge-nf-call-ip6tables = 1 net.ipv4.ip_forward = 1 EOF\rApply sysctl params without reboot sudo sysctl --system\rInstalling kubeadm, kubelet and kubectl sudo apt-get update # apt-transport-https may be a dummy package; if so, you can skip that package sudo apt-get install -y apt-transport-https ca-certificates curl gpg curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.29/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg # This overwrites any existing configuration in /etc/apt/sources.list.d/kubernetes.list echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.29/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list sudo apt-get update sudo apt-get install -y kubelet kubeadm kubectl sudo apt-mark hold kubelet kubeadm kubectl\rInitialize Cluster with kubeadm (Only master node) kubeadm init --pod-network-cidr=10.244.0.0/16 --apiserver-advertise-address=172.16.16.100\rExecute the below command to run kubectl commands as you will require kubeconfig file to connect to kubernetes cluster mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config\rClick Here to know more about kubeconfig\nCheck nodes status kubectl get nodes\rInstall Network Addon (flannel) (master node) kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml\rConnect Worker Node (Only worker node) kubeadm join 172.16.16.100:6443 --token qmw5dj.ljdh8r74ce3y85ad \\ --discovery-token-ca-cert-hash sha256:83374ec05088fa7efe9c31cce63326ae7037210ab049048ef08f8c961a048ddf\rVerification check node status kubectl get nodes\rrun a pod kubectl run nginx --image=nginx\rcheck running pod kubectl get pods -o wide\rKubernetes Cluster Configurations Static Pod Manifests: /etc/kubernetes/manifests /etc/kubernetes/manifests ├── etcd.yaml ├── kube-apiserver.yaml ├── kube-controller-manager.yaml └── kube-scheduler.yaml\rTLS Certificates All the components talk to each other over mTLS. Under the PKI folder, you will find all the TLS certificates and keys Kubeconfig Files\nAny components that need to authenticate to the API server need the kubeconfig file.\nAll the cluster Kubeconfig files are present in the /etc/kubernetes folder (.conf files). You will find the following files.\nadmin.conf controller-manager.conf kubelet.conf scheduler.conf It contains the API server endpoint, cluster CA certificate, cluster client certificate, and other information.\nThe admin.conf, file, which is the admin kubeconfig file used by end users to access the API server to manage the clusters.\nThe Kubeconfig for the Controller manager controller-manager.conf, scheduler, and Kubelet is used for API server authentication and authorization.\nKubelet Configurations kubelet kubeconfig file: /etc/kubernetes/kubelet.conf kubelet config file: /var/lib/kubelet/config.yaml EnvironmentFile=-/var/lib/kubelet/kubeadm-flags.env\rThe /var/lib/kubelet/config.yaml contains all the kubelet-related configurations. The static pod manifest location is added as part of the staticPodPath parameter. /var/lib/kubelet/kubeadm-flags.env file contains the container runtime environment Linux socket and the infra container (pause container) image. CoreDNS Configurations kubectl get configmap --namespace=kube-system\rLAB 1 Create a virtual machine Login to vm and chnage the user to root Install kubernetes using kubeadm Add a worker node to kubernetes Cluster Node Related commands Check all nodes which are the part of kubernetes cluster kubectl get nodes\rCordon a node (prevent new pods from being scheduled) kubectl cordon \u003cnode-name\u003e\rUncordon a node (allow new pods to be scheduled): kubectl uncordon \u003cnode-name\u003e\rDrain a node (evacuate all pods, reschedule them elsewhere) kubectl drain \u003cnode-name\u003e\rSort Nodes by CPU Capacity kubectl get nodes --sort-by=.status.capacity.cpu\rSort by memory kubectl get nodes --sort-by=.status.capacity.memory\rHow to label a node kubectl label node \u003cnodename\u003e key=value\rcheck nodes with label kubectl get nodes -l key=value\rcheck the nodes which does not label key=value kubectl get nodes key!=value\rCheck detailed information about a node kubectl get nodes -o wide\rUpgrade Kubeadm Cluster Before upgrade kubernetes cluster Do not forgot to study kubernetes release Check the release notes before you upgrade your kubernetes cluster Also drain a node and uncordon after the upgrade is successful Before upgrading Kubernetes to version 1.30, make sure to add the corresponding repository for version 1.30 Run the below command sudo apt-get update\rapt-transport-https may be a dummy package; if so, you can skip that package sudo apt-get install -y apt-transport-https ca-certificates curl gpg\rIf the directory /etc/apt/keyrings does not exist, it should be created before the curl command, read the note below. sudo mkdir -p -m 755 /etc/apt/keyrings curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring01.gpg\rThis overwrites any existing configuration in /etc/apt/sources.list.d/kubernetes.list echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes01.list\rsudo apt-get update\rCheck the upgrade plan using below commands kubeadm upgrade plan kubectl -n kube-system get cm kubeadm-config -o yaml\rCheck the kubeadm version available apt-cache madison kubeadm\rUnhold the packages to be upgraded apt-mark unhold kubelet kubectl kubeadm\rUpgrade the available packages apt-get update \u0026\u0026 apt-get install -y kubelet=1.30.0-1.1 kubectl=1.30.0-1.1 kubeadm=1.30.0-1.1\rLock again the below packages apt-mark hold kubelet kubectl\rCheck new kubeadm version kubeadm version\rUpgrade the kubernetes cluster (Run only on Master node) kubeadm upgrade apply 1.30.0\rRestart the services systemctl daemon-reload systemctl restart kubelet\rCheck the kubernetes version kubectl version\rKubens \u0026 Kubectx Login to master node and run the command there Install Kubens to switch from one namespace to another wget https://raw.githubusercontent.com/ahmetb/kubectx/master/kubens chmod +x kubens cp kubens /bin\rInstall kubectx to switch from one Kubernetes cluster to another wget https://raw.githubusercontent.com/ahmetb/kubectx/master/kubectx chmod +x kubectx cp kubectx /bin\rKubernetes Initial commands for cluster verification How to check nodes in cluster kubectl get nodes\rCluster api info kubectl api-resources\rCluster info kubect cluster-info\rKubernetes nodes information kubectl describe nodes \u003cnodename\u003e\rUpdate worker node role as a worker kubectl label node \u003cnode-name\u003e kubernetes.io/role=worker\rMake a test after creating one pod kubectl run mypod --image=nginx\rCheck newly created pod kubectl get pods\rLAB 2 Perform basic operation on newly created kuberntes cluster Check no of nodes Create a pod check cluster info describe the node Install kubens and kubectx Upgrade kubenetes master and worker nodes Install kubernetes Cluster Automatically git clone https://gitlab.com/container-and-kubernetes/kubernetes-2024.git cd kubernetes-2024/ cd kubernetes-with-vagrant/ vagrant up\rCommon Kubeadm Commands kubeadm init (initialize kubernetes) kubeadm init --pod-network-cidr=192.168.0.0/16\rkubeadm join (To join a worker node) kubeadm join \u003cmaster-node-ip\u003e:\u003cmaster-node-port\u003e --token \u003ctoken\u003e --discovery-token-ca-cert-hash \u003chash\u003e\rkubeadm reset( removing all installed Kubernetes components and returning it to its pre-Kubernetes state. ) kubeadm reset --force\rkubeadm upgrade (check available upgrades) kubeadm upgrade plan\rkubeadm token (Token management) kubeadm token create\rPrint Token along with command kubeadm token create --print-join-command\rkubeadm token list (Check all token along with expiration date) kubeadm token list\rkubeadm token delete kubeadm token delete \u003ctoken_value\u003e\rkubeadm config migrate kubeadm config migrate --old-config kubeadm.conf --new-config kubeadm.yaml\rkubeadm config (Print default config files) kubeadm config print init-defaults\rkubeadm config images(List all required images) kubeadm config images list\rkubeadm certs(Check certificate status) kubeadm certs check-expiration kubeadm certs certificate-key\rkubeadm upgrade node(upgrades the kubelet and kube-proxy on a worker node to match the control plane’s version.) kubeadm upgrade node\rLAB 3 Test kubeadm command How to generate the woker node join command How to reset the kubeadm config How to check certificate expirty date How to delete token Create kubernetes cluster automatically What is a Kubeconfig file? A Kubeconfig is a YAML file with all the Kubernetes cluster details, certificates, and secret tokens to authenticate the cluster. You might get this config file directly from the cluster administrator or from a cloud platform if you are using a managed Kubernetes cluster. When you use kubectl, it uses the information in the kubeconfig file to connect to the kubernetes cluster API. The default location of the Kubeconfig file is $HOME/.kube/config Example Kubeconfig File certificate-authority-data: Cluster CA server: Cluster endpoint (IP/DNS of the master node) name: Cluster name user: name of the user/service account. token: Secret token of the user/service account. apiVersion: v1 clusters: - cluster: certificate-authority-data: \u003cca-data-here\u003e server: https://your-k8s-cluster.com name: \u003ccluster-name\u003e contexts: - context: cluster: \u003ccluster-name\u003e user: \u003ccluster-name-user\u003e name: \u003ccluster-name\u003e current-context: \u003ccluster-name\u003e kind: Config preferences: {} users: - name: \u003ccluster-name-user\u003e user: token: \u003csecret-token-here\u003e\rkubeconfig Related Commands The kubeconfig file is used to configure access to Kubernetes clusters. It contains information about clusters, users, and contexts. Here are some common commands and their usage:\n1. View Current Context To view the currently active context in your kubeconfig file: kubectl config current-context\rList all cluster contexts kubectl config get-contexts -o=name\rSet the current context kubectl config use-context \u003ccluster-name\u003e Using Environment variable (KUBECONFIG) export KUBECONFIG=\u003ckubeconfig filename\u003e\rUsing Kubeconfig File With Kubectl kubectl get nodes --kubeconfig=\u003cfilepath\u003e\rHow to merge multiple kubeconfig into one KUBECONFIG=config:dev_config:test_config kubectl config view --merge --flatten \u003e config.new\rAdd or Update Cluster kubectl config set-cluster \u003ccluster_name\u003e --server=\u003cserver_url\u003e --certificate-authority=\u003cca_file\u003e --embed-certs=true\rAdd or Update User kubectl config set-credentials \u003cuser_name\u003e --client-certificate=\u003ccert_file\u003e --client-key=\u003ckey_file\u003e --embed-certs=true\rAdd or Update Context kubectl config set-context \u003ccontext_name\u003e --cluster=\u003ccluster_name\u003e --user=\u003cuser_name\u003e\rDelete a Context kubectl config delete-context \u003ccontext_name\u003e\rDelete a Cluster kubectl config delete-cluster \u003ccluster_name\u003e\rDelete a User kubectl config delete-credentials \u003cuser_name\u003e\rView Config File Location echo $KUBECONFIG\rCheck kubeconfig file kubectl config view\rLAB 4 Download a kubeconfig file Try to use kubeconfig file with environment variable or passing to command line Check etcd docs Check how we can set up multi master kubernetes cluster.",
    "description": "Topics Kubernetes Overview Kubernetes Architecture Kubernetes Components Kubernetes Installation Kubernetes SSL Certificates Kubernetes Static Pods Kubernetes Configuration files Kubernetes Upgrade Etcd backup and restore Kubernetes Node related commands CIS Benchmark Kubernetes Overview Kubernetes or k8s is an open-source orchestration and cluster management for container-based applications maintained by the Cloud Native Computing Foundation. Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications. It groups containers that make up an application into logical units for easy management and discovery",
    "tags": [],
    "title": "KUBERNETES LOGGING AND MONITORING",
    "uri": "/kubernetesloggin-monitoring/index.html"
  },
  {
    "breadcrumb": "/",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Kubernetes Security",
    "uri": "/neuvector/index.html"
  },
  {
    "breadcrumb": "",
    "content": "",
    "description": "",
    "tags": [],
    "title": "/",
    "uri": "/index.html"
  },
  {
    "breadcrumb": "/",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Categories",
    "uri": "/categories/index.html"
  },
  {
    "breadcrumb": "/",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tags",
    "uri": "/tags/index.html"
  }
]
